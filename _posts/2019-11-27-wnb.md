---
layout: post
title: a post with code
date: 2015-07-15 15:09:00
description: an example of a blog post with some code
categories: [experiment monitor]
---
My
Wanb
Goal: save model metadata and results of each run  

## !!!!!


$$ h_t= ReLU(Wx_t + (U+I)h_{t-1} + b)  $$

##!!
```python
s = "Python syntax highlighting"
print s
```
How to use:
  Include:

  import wandb
  from wandb.keras import WandbCallback
--------------------------------
really finish thinkful!
finally have Portfolio and posts
------------------------------------------

my pages:
1. educational:
  1. databases
  2. overall analysisis approach
  3. pandas
  4.

page:


applying go big or go home to many areas is a recipie for constant self-critism dissapointement and possible depression.
Motivation Monkey loves to push us to make a big moves and then hides when things gets tough.
People often naturaly think in big chunks. They they underestimate the size of the step!!
!! if it is difficult for you to do tiny step (place your cup in place) clean up in the moring, then you have to watch carefully on most important steps!

 The big thing is growing confidence with evry step no matter succesful or not.
 If behaviour is not sucsesful Should i through it? Shoule break it to small steps?
 If you change it it is success, and you don't go into self-smaming spiral!


 For me mountain climb is attractive even it looks difficult. Shasta was difficult, but i did one step a time feel great  rest and do the next the key is to get into flow and rest along the way.

 The idea is to buid a set of small habbits and master to do them no matter what.
 The most important thing that you build the confidence in your ability and learn resilience and adaptation. You lso learn to congradulate yourwself often!
 ---------

 June.11
 Result was so so, but still something. The idea is to get  all areas moving every day.

 Money, communication , reading, writing, ... people side ???

 3 approachecs to make the behaviour easie to do
 1. increase your skills!!  repeat easy steps and learn the next one
 2. get tools and resources

 Decigion fatigue: necesity to make a decigion when your are least capable to do it.

 So behavior analysis:
 Do I want to do it? Priority?
 What makes it difficult?
  Do i have skills, tools, ... best when motivation is high
  Can  I start tiny small? what is the smallest ammount that could be considered finished?

  The key moment is to repeat small steps without raising the bar!!
  The problem often is the mental effort ... could be easier if you break it to smaller modules ...




-----------------------
```python
import math

from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.formula.api as smf
from statsmodels.sandbox.regression.predstd import wls_prediction_std

%matplotlib inline
sns.set_style('white')
```


```python
# Read data into a DataFrame.  diffrent year !!!!
data_url = '..\..\datasets\offenses_Known_to_Law_Enforcement_by_New_York_by_City_2014.csv'
data=pd.read_csv(data_url, dtype={'Murder': str,'Arson':str})

data.head()

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>City</th>
      <th>Population</th>
      <th>ViolentCrime</th>
      <th>Murder</th>
      <th>Rape</th>
      <th>Robbery</th>
      <th>Assault</th>
      <th>PropertyCrime</th>
      <th>Burglary</th>
      <th>LarcenyTheft</th>
      <th>CarTheft</th>
      <th>Arson</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Adams Village</td>
      <td>1,851</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>11</td>
      <td>1</td>
      <td>10</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Addison Town and Village</td>
      <td>2,568</td>
      <td>2</td>
      <td>0</td>
      <td>NaN</td>
      <td>1</td>
      <td>1</td>
      <td>49</td>
      <td>1</td>
      <td>47</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Afton Village4</td>
      <td>820</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Akron Village</td>
      <td>2,842</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>0</td>
      <td>1</td>
      <td>17</td>
      <td>0</td>
      <td>17</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Albany4</td>
      <td>98,595</td>
      <td>802</td>
      <td>8</td>
      <td>54</td>
      <td>237</td>
      <td>503</td>
      <td>3,888</td>
      <td>683</td>
      <td>3,083</td>
      <td>122</td>
      <td>12</td>
    </tr>
  </tbody>
</table>
</div>




```python

#data['Population']=data['Population'].apply(fn)
print(data.dtypes)
```

    City             object
    Population       object
    ViolentCrime     object
    Murder           object
    Rape             object
    Robbery          object
    Assault          object
    PropertyCrime    object
    Burglary         object
    LarcenyTheft     object
    CarTheft         object
    Arson            object
    dtype: object



```python
# Write out the model formula.
# Your dependent variable on the right, independent variables on the left
# Use a ~ to represent an '=' from the functional form
df=data.drop(['City'],axis=1)
#  make pupulation a number
#dfm = df.apply(lambda x: x.str.replace(',','').apply(pd.to_numeri
dfm=df.apply(lambda x: x.str.replace(',','').apply(pd.to_numeric))
dfm.fillna(0, inplace=True)
print(list(dfm))
```

    ['Population', 'ViolentCrime', 'Murder', 'Rape', 'Robbery', 'Assault', 'PropertyCrime', 'Burglary', 'LarcenyTheft', 'CarTheft', 'Arson']



```python

linear_formula = 'PropertyCrime ~Population + ViolentCrime + Murder + Rape + Robbery + Assault + PropertyCrime + Burglary + LarcenyTheft + CarTheft + Arson'
# Fit the model to our data using the formula.
lm = smf.ols(formula=linear_formula, data=dfm).fit()
lm.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>      <td>PropertyCrime</td>  <th>  R-squared:         </th>  <td>   1.000</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   1.000</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>9.569e+28</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sat, 09 Mar 2019</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  
</tr>
<tr>
  <th>Time:</th>                 <td>09:27:06</td>     <th>  Log-Likelihood:    </th>  <td>  7873.8</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   369</td>      <th>  AIC:               </th> <td>-1.572e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   357</td>      <th>  BIC:               </th> <td>-1.568e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>      <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>     <td> 7.105e-14</td> <td> 8.91e-12</td> <td>    0.008</td> <td> 0.994</td> <td>-1.74e-11</td> <td> 1.76e-11</td>
</tr>
<tr>
  <th>Population</th>    <td>-1.475e-17</td> <td> 7.67e-16</td> <td>   -0.019</td> <td> 0.985</td> <td>-1.52e-15</td> <td> 1.49e-15</td>
</tr>
<tr>
  <th>ViolentCrime</th>  <td> 6.395e-14</td> <td> 5.76e-12</td> <td>    0.011</td> <td> 0.991</td> <td>-1.13e-11</td> <td> 1.14e-11</td>
</tr>
<tr>
  <th>Murder</th>        <td> 1.421e-13</td> <td> 1.42e-11</td> <td>    0.010</td> <td> 0.992</td> <td>-2.79e-11</td> <td> 2.82e-11</td>
</tr>
<tr>
  <th>Rape</th>          <td>-2.487e-14</td> <td> 6.28e-12</td> <td>   -0.004</td> <td> 0.997</td> <td>-1.24e-11</td> <td> 1.23e-11</td>
</tr>
<tr>
  <th>Robbery</th>       <td>-4.619e-14</td> <td>    6e-12</td> <td>   -0.008</td> <td> 0.994</td> <td>-1.18e-11</td> <td> 1.17e-11</td>
</tr>
<tr>
  <th>Assault</th>       <td>-2.132e-14</td> <td> 5.59e-12</td> <td>   -0.004</td> <td> 0.997</td> <td> -1.1e-11</td> <td>  1.1e-11</td>
</tr>
<tr>
  <th>PropertyCrime</th> <td>    1.0000</td> <td> 2.67e-11</td> <td> 3.75e+10</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>
</tr>
<tr>
  <th>Burglary</th>      <td>-3.197e-14</td> <td> 2.67e-11</td> <td>   -0.001</td> <td> 0.999</td> <td>-5.25e-11</td> <td> 5.24e-11</td>
</tr>
<tr>
  <th>LarcenyTheft</th>  <td>-1.243e-14</td> <td> 2.67e-11</td> <td>   -0.000</td> <td> 1.000</td> <td>-5.25e-11</td> <td> 5.24e-11</td>
</tr>
<tr>
  <th>CarTheft</th>      <td> 5.329e-15</td> <td> 2.67e-11</td> <td>    0.000</td> <td> 1.000</td> <td>-5.25e-11</td> <td> 5.25e-11</td>
</tr>
<tr>
  <th>Arson</th>         <td>-8.882e-15</td> <td> 3.46e-12</td> <td>   -0.003</td> <td> 0.998</td> <td>-6.82e-12</td> <td>  6.8e-12</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>868.594</td> <th>  Durbin-Watson:     </th>  <td>   1.996</td>  
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1996453.069</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-18.915</td> <th>  Prob(JB):          </th>  <td>    0.00</td>  
</tr>
<tr>
  <th>Kurtosis:</th>      <td>361.357</td> <th>  Cond. No.          </th>  <td>3.41e+06</td>  
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.41e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems.



```python
# try  with holdout
#  i am still using pca!

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
neighbors.fit(X_train,y_train)
ytrain_pred = neighbors.predict(X_train)
print ('R2 for Train data with pca')
print(r2_score(y_train, ytrain_pred))

ytest_pred = neighbors.predict(X_test)
print ('R2 for Test data with pca')
print(r2_score(y_test, ytest_pred))

```

    R2 for Train data with pca
    0.9987831501
    R2 for Train data with pca
    0.774825201941



```python

```

    0.128632388064



```python

```
