{
  
    
        "post0": {
            "title": "Dataframes and pandas",
            "content": "Approaching the data . The first step is to understand your data source and size. . are you dealing with the static data? Example of static is a file somewhere. Dinamic could be a database table or even stream. . Let’s start from the simplest example static datafile. Most common way to do it is to use pandas: - labeled, indexed two dimentional data structure. . ’’’ import pandas as pd . Efficient approach to readind data: ‘’’ python import subprocess as sb print (sb.check_output([“ls”, “-l”, “../”]).decode(“utf8”)) ‘’’ . ’’’ python first read a few lines: . ’’’ . #——————— do something 8/h a day with results . project . Talk to recruiter #——————- ~ 2 projects volunteering 15 / 20 expectation! ———————————— !- make it nice and publish data manipulation, #———————– StackOverflow #——————————– week! have something top show! SQL questions, test #—————————– Maintain mental health . very activities : . talk to people . #—————#- art ! — # . srochaya . drriving . #———– covid jobs !!! the best thing if i can do it #—————-&gt;– volunteering/ project ~ 3 weeks . —&gt; #——————- COVID JOBS req: -&gt; vacine -&gt; #— . Feature encoding | Feature engineering | Feature selection | Model building | Model comparison | Model evaluation | Final training and performance evaluation | . #———— . my site linked in . . talk to somebody . | . | . #——————— .",
            "url": "https://v2br.github.io/fastpages/notes/2020/09/28/dataframes.html",
            "relUrl": "/notes/2020/09/28/dataframes.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Docker for development on wsl2",
            "content": "Notes on docker use on development wsl2 system. . I decided to use docker for a jekyll project. The reason is with all upgrades and gems possible conflicts management of the environment becomes problematic. Moving project to another environment might be not that straight forward. . Containerization approach. With docker container the whole project becomes easy manageable and reproducible. Plus if you have any conflicts during installation it is much easier to analyze the problem and resolve the conflict. . My requirements to the solution: . Editing project files should not change. | Version control should not be affected. | The whole setup should be reproducible and automated as much as possible. | Jekyll should generate site and run it on local system port 400000. | The first two reqrements cold be satisfied using docker option “volume”. It allows to map local host directories to virtual directories. Docker volume is aka rsync in Linux. The third one is the key feature of the docker itself. The last could be done using docker parameter “port”. It will map local host port to the application port. . One additional configuration step is necessary due to default docker user. It is root. This is not optimal. According to best practices application will run on application user account only with necessary privileges. . To accomplish this portable way we can place whole configuration into docker file . Dcokerfile . FROM jekyll/jekyll RUN addgroup jekyll &amp;&amp; adduser –ingroup jekyll –disabled-password –gecos ‘’ jekyll &amp;&amp; chown jekyll:jekyll /home/jekyll USER jekyll RUN mkdir -p /home/jekyll/site &amp;&amp; chown -R jekyll:jekyll /home/jekyll/site VOLUME /home/jekyll/site WORKDIR /home/jekyll/site EXPOSE 4000 RUN apk add py-pip RUN pip CMD bundle install &amp;&amp; jekyll serve –host=0.0.0.0 –force_polling –watch . Let’s look into the details: . Use the latest jekyll image from dockerhub | Configure creation of docker container with the following arguments volumes | . | in my case the critical part happened to be gem installation .",
            "url": "https://v2br.github.io/fastpages/portfolio/",
            "relUrl": "/portfolio/",
            "date": " • Aug 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Why diabetic patients are readmitted?",
            "content": "Abstract . ## Introduction Many hospital patients are readmitted within 30 days after discharge. Often it indicates problems with the quality of hospital care. Recently this parameter became part of the National Hospital Evaluation metric. In 2014 researches of Virginia Commonwealth University used resources of Health Facts database from Center corp (major medical data hub) and NIH grant to assemble the dataset to study readmissions especially for diabetic patients. They wanted to analyze if proper management of blood level sugar will affect readmission. This study focussed on predcitors for term readmission (readmission in less than 30 days) for this type of patients. The result could be a useful base for deeper analysis by health care experts and machine learning specialists. The dataset consists of 50 features and 110 observations. Each observation is one patient hospitalization record with a readmission parameter. ~~~~~~~~~~ block1 ~~~~~~~~~~ &gt; blocks &gt; Table: |C1 |c2 | |--|--| | v1 |v2 | |0.04|023423423.456| |1.04|2.456| 1. items #Title1 2. item2 1. itemmm 2. mmm You can also have artistically styled 2/3 + 1/3 images, like these. list of projects for me: 1. Easy transfer from noptebook to a publication. 2. How to write a project for a porfolio 2. 3. tiny habits design summary: Mind map? the other summar The first project will be review about the suiside. the idea is to tell the story: i can try to use kaggle - The code is simple. Just add a col class to your image, and another class specifying the width: one, two, or three columns wide. Here&#39;s the code for the last row of images above: .",
            "url": "https://v2br.github.io/fastpages/2020/03/20/project.html",
            "relUrl": "/2020/03/20/project.html",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Gradient Boosting",
            "content": "import pandas as pd import numpy as np import scipy import matplotlib.pyplot as plt from sklearn import tree from IPython.display import Image import pydotplus import graphviz %matplotlib inline . Gradient boosting . You may recall that we last encountered gradients when discussing the gradient descent algorithm in the context of fitting linear regression models. We said that for a particular regression model with n parameters, an n+1 dimensional space existed defined by all the parameters plus the cost/loss function to minimize. The combination of parameters and loss function define a surface within the space. The regression model is fitted by moving down the steepest ‘downhill’ gradient until we reach the lowest point of the surface, where all possible gradients are ‘uphill.’ The final model is made up of the parameter estimates that define that location on the surface. . Throughout all iterations of the gradient descent algorithm for linear regression, one thing remains constant: The underlying data used to estimate the parameters and calculate the loss function never changes. In gradient boosting, however, the underlying data do change. Let’s work it out: . Gradient boosting can work on any combination of loss function and model type, as long as we can calculate the derivatives of the loss function with respect to the model parameters. Most often, however, gradient boosting uses decision trees, and minimizes either the residual (regression trees) or the negative log-likelihood (classification trees). . Gradient boosting with regression trees . The loss function to minimize is the sum of the squared residuals: . 1n∑i=1n(yi−(α+βxi))2 frac1{n} sum_{i=1}^n(y_i-( alpha + beta x_i))^2n1​i=1∑n​(yi​−(α+βxi​))2 . (Though it can also be the sum of the absolute value of the residuals, as in lasso regression.) . Each time we run a decision tree, we extract the residuals. Then we run a new decision tree, using those residuals as the outcome to be predicted. After reaching a stopping point, we add together the predicted values from all of the decision trees to create the final gradient boosted prediction. . The decision trees we use can be very simple. In the example below, the decision trees only have a max depth of 2, meaning a maximum of four leaves. These weak learners do not have to perform well at all individually in order to do well in aggregate. . Here we’re going to do gradient boosting with regression trees by hand. Our goal is to predict the variable “happy” using all the other variables in the European Social Survey dataset. We’ll calculate a tree, store the predicted values, pull the residuals, and run a new tree on the residuals. This will repeat 101 times. At the end, we add together all the predicted values from each iteration to yield the final predictions. . # Working with the European Social Survey data again. df = pd.read_csv(( &quot;https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/&quot; &quot;master/ESS_practice_data/ESSdata_Thinkful.csv&quot;)).dropna() # Define outcome and predictors. y = df[&#39;happy&#39;] X = df.loc[:, ~df.columns.isin([&#39;happy&#39;, &#39;cntry&#39;])] # Make the categorical variable &#39;country&#39; into dummies. X = pd.concat([X, pd.get_dummies(df[&#39;cntry&#39;])], axis=1) # Store values from loops. preds = pd.DataFrame() labels = [] uniquex = df[&#39;happy&#39;].unique() # Iterate through decision trees, each time using the residuals # from the previous tree as the inputs. for m in range(0, 101): # Initialize and fit the tree. Set the max depth to 2. decision_tree = tree.DecisionTreeRegressor(max_depth=2) decision_tree.fit(X,y) # Get and store predicted values. pred = decision_tree.predict(X) preds[&#39;pred{}&#39;.format(m)] = pred # Residuals. y = y - pred # Output every 20 iterations. if m % 20 == 0: print(&#39;Weak learner {} R^2: {}&#39;.format(m, decision_tree.score(X, y))) labels = labels + [m] bestpred = preds.sum(axis=1) plt.plot(uniquex, np.poly1d(np.polyfit(df[&#39;happy&#39;], bestpred, 1))(uniquex)) plt.legend(labels) plt.xlabel(&#39;Original&#39;) plt.ylabel(&#39;Predicted&#39;) plt.show() predicted = preds.sum(axis=1) print(&#39;Overall R^2:&#39;) print(np.corrcoef(df[&#39;happy&#39;], predicted)[0, 1]) . Weak learner 0 R^2: -21.86595826514709 Weak learner 20 R^2: -0.0026192923008196978 Weak learner 40 R^2: -0.0016748172727343302 Weak learner 60 R^2: -0.0014818633549413818 Weak learner 80 R^2: -0.0012578370480087475 Weak learner 100 R^2: -0.0007136861925911564 . . Overall R^2: 0.528287358179 . Looking at the sampling of individual model $R^2$, these are certainly ‘weak learners’! They perform very badly indeed. Yet in aggregate, the models are able to explain nearly 53% of the variance in happiness in the dataset. This is a great example of the power of gradient boosting. You may also note from the plot that the greatest gains happened in the earliest models, with subsequent steps yielding smaller gains in explanatory power. . But maybe we could do just as well with one big decision tree, rather than a bunch of small ones. Let’s try: . # One big tree. Each node split will include at lest 2.5% of the total sample. decision_tree = tree.DecisionTreeRegressor(min_samples_split=.025) decision_tree.fit(X,y) #Get and store predicted values. pred = decision_tree.predict(X) print(&#39;One strongish learner R^2: {}&#39;.format(decision_tree.score(X, y))) dot_data = tree.export_graphviz(decision_tree, out_file=None, feature_names=X.columns, filled=True) graph = pydotplus.graph_from_dot_data(dot_data) Image(graph.create_png()) . One strongish learner R^2: 0.06711108322936521 . . Well that didn’t work at all. Gradient boosting with weak learners was nearly 10 times more effective as one complex decision tree. In fact, in many instances gradient boosted decision trees perform better than random forests. They are also less prone to overfitting than individual decision trees. . Overfitting . Still, it is easy to see that the more iterations we run, the more likely we are to overfit. Gradient boost comes with some methods to avoid overfitting. Cross-validation will check for overfitting, but there are also methods that can be applied before using the test set that will reduce the likelihood of overfit. . One option is subsampling, where each iteration of the boost algorithm uses a subsample of the original data. By introducing some randomness into the process, subsampling makes it harder to overfit. . Another option is shrinkage, which we have encountered before in ridge regression. Here, the shrinkage/regularization parameter reduces the impact of subsequent iterations on the final solution. Visually, you can picture this parameter, called the “learning rate”, causing each “step” along the loss function gradient to be a little smaller than the previous one. This prevents any one iteration from being too influential and misdirecting the overall boost solution. Learning rates vary between 0 (only the initial iteration matters) to 1 (all iterations are weighted equally). A model made up of many small steps is less prone to overfitting than a model made up of few large steps, but it can also lead to much slower running times, depending on the stopping rule in play. . In the next assignment, we’ll try gradient boosting using the SKLearn package, which comes with all sorts of helpful features for getting the most out of gradient boost. .",
            "url": "https://v2br.github.io/fastpages/gradient%20boost/2020/02/28/gradient-boost.html",
            "relUrl": "/gradient%20boost/2020/02/28/gradient-boost.html",
            "date": " • Feb 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://v2br.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://v2br.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "a post with code",
            "content": "My Wanb Goal: save model metadata and results of each run . !!!!! . ht=ReLU(Wxt+(U+I)ht−1+b)h_t= ReLU(Wx_t + (U+I)h_{t-1} + b)ht​=ReLU(Wxt​+(U+I)ht−1​+b) . ##!! . s = &quot;Python syntax highlighting&quot; print s . How to use: Include: . import wandb from wandb.keras import WandbCallback ——————————– really finish thinkful! finally have Portfolio and posts —————————————— . my pages: . educational: | databases | overall analysisis approach | pandas 4. | page: . applying go big or go home to many areas is a recipie for constant self-critism dissapointement and possible depression. Motivation Monkey loves to push us to make a big moves and then hides when things gets tough. People often naturaly think in big chunks. They they underestimate the size of the step!! !! if it is difficult for you to do tiny step (place your cup in place) clean up in the moring, then you have to watch carefully on most important steps! . The big thing is growing confidence with evry step no matter succesful or not. If behaviour is not sucsesful Should i through it? Shoule break it to small steps? If you change it it is success, and you don’t go into self-smaming spiral! . For me mountain climb is attractive even it looks difficult. Shasta was difficult, but i did one step a time feel great rest and do the next the key is to get into flow and rest along the way. . The idea is to buid a set of small habbits and master to do them no matter what. The most important thing that you build the confidence in your ability and learn resilience and adaptation. You lso learn to congradulate yourwself often! ——— . June.11 Result was so so, but still something. The idea is to get all areas moving every day. . Money, communication , reading, writing, … people side ??? . 3 approachecs to make the behaviour easie to do . increase your skills!! repeat easy steps and learn the next one | get tools and resources | Decigion fatigue: necesity to make a decigion when your are least capable to do it. . So behavior analysis: Do I want to do it? Priority? What makes it difficult? Do i have skills, tools, … best when motivation is high Can I start tiny small? what is the smallest ammount that could be considered finished? . The key moment is to repeat small steps without raising the bar!! The problem often is the mental effort … could be easier if you break it to smaller modules … . . import math from matplotlib import pyplot as plt import numpy as np import pandas as pd import seaborn as sns import statsmodels.formula.api as smf from statsmodels.sandbox.regression.predstd import wls_prediction_std %matplotlib inline sns.set_style(&#39;white&#39;) . # Read data into a DataFrame. diffrent year !!!! data_url = &#39;.. .. datasets offenses_Known_to_Law_Enforcement_by_New_York_by_City_2014.csv&#39; data=pd.read_csv(data_url, dtype={&#39;Murder&#39;: str,&#39;Arson&#39;:str}) data.head() . City Population ViolentCrime Murder Rape Robbery Assault PropertyCrime Burglary LarcenyTheft CarTheft Arson . 0 | Adams Village | 1,851 | 0 | 0 | NaN | 0 | 0 | 11 | 1 | 10 | 0 | 0 | . 1 | Addison Town and Village | 2,568 | 2 | 0 | NaN | 1 | 1 | 49 | 1 | 47 | 1 | 0 | . 2 | Afton Village4 | 820 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 3 | Akron Village | 2,842 | 1 | 0 | NaN | 0 | 1 | 17 | 0 | 17 | 0 | 0 | . 4 | Albany4 | 98,595 | 802 | 8 | 54 | 237 | 503 | 3,888 | 683 | 3,083 | 122 | 12 | . #data[&#39;Population&#39;]=data[&#39;Population&#39;].apply(fn) print(data.dtypes) . City object Population object ViolentCrime object Murder object Rape object Robbery object Assault object PropertyCrime object Burglary object LarcenyTheft object CarTheft object Arson object dtype: object . # Write out the model formula. # Your dependent variable on the right, independent variables on the left # Use a ~ to represent an &#39;=&#39; from the functional form df=data.drop([&#39;City&#39;],axis=1) # make pupulation a number #dfm = df.apply(lambda x: x.str.replace(&#39;,&#39;,&#39;&#39;).apply(pd.to_numeri dfm=df.apply(lambda x: x.str.replace(&#39;,&#39;,&#39;&#39;).apply(pd.to_numeric)) dfm.fillna(0, inplace=True) print(list(dfm)) . [&#39;Population&#39;, &#39;ViolentCrime&#39;, &#39;Murder&#39;, &#39;Rape&#39;, &#39;Robbery&#39;, &#39;Assault&#39;, &#39;PropertyCrime&#39;, &#39;Burglary&#39;, &#39;LarcenyTheft&#39;, &#39;CarTheft&#39;, &#39;Arson&#39;] . linear_formula = &#39;PropertyCrime ~Population + ViolentCrime + Murder + Rape + Robbery + Assault + PropertyCrime + Burglary + LarcenyTheft + CarTheft + Arson&#39; # Fit the model to our data using the formula. lm = smf.ols(formula=linear_formula, data=dfm).fit() lm.summary() . OLS Regression Results Dep. Variable: PropertyCrime | R-squared: 1.000 | . Model: OLS | Adj. R-squared: 1.000 | . Method: Least Squares | F-statistic: 9.569e+28 | . Date: Sat, 09 Mar 2019 | Prob (F-statistic): 0.00 | . Time: 09:27:06 | Log-Likelihood: 7873.8 | . No. Observations: 369 | AIC: -1.572e+04 | . Df Residuals: 357 | BIC: -1.568e+04 | . Df Model: 11 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . Intercept 7.105e-14 | 8.91e-12 | 0.008 | 0.994 | -1.74e-11 | 1.76e-11 | . Population -1.475e-17 | 7.67e-16 | -0.019 | 0.985 | -1.52e-15 | 1.49e-15 | . ViolentCrime 6.395e-14 | 5.76e-12 | 0.011 | 0.991 | -1.13e-11 | 1.14e-11 | . Murder 1.421e-13 | 1.42e-11 | 0.010 | 0.992 | -2.79e-11 | 2.82e-11 | . Rape -2.487e-14 | 6.28e-12 | -0.004 | 0.997 | -1.24e-11 | 1.23e-11 | . Robbery -4.619e-14 | 6e-12 | -0.008 | 0.994 | -1.18e-11 | 1.17e-11 | . Assault -2.132e-14 | 5.59e-12 | -0.004 | 0.997 | -1.1e-11 | 1.1e-11 | . PropertyCrime 1.0000 | 2.67e-11 | 3.75e+10 | 0.000 | 1.000 | 1.000 | . Burglary -3.197e-14 | 2.67e-11 | -0.001 | 0.999 | -5.25e-11 | 5.24e-11 | . LarcenyTheft -1.243e-14 | 2.67e-11 | -0.000 | 1.000 | -5.25e-11 | 5.24e-11 | . CarTheft 5.329e-15 | 2.67e-11 | 0.000 | 1.000 | -5.25e-11 | 5.25e-11 | . Arson -8.882e-15 | 3.46e-12 | -0.003 | 0.998 | -6.82e-12 | 6.8e-12 | . Omnibus: 868.594 | Durbin-Watson: 1.996 | . Prob(Omnibus): 0.000 | Jarque-Bera (JB): 1996453.069 | . Skew: -18.915 | Prob(JB): 0.00 | . Kurtosis: 361.357 | Cond. No. 3.41e+06 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.41e+06. This might indicate that there arestrong multicollinearity or other numerical problems. . # try with holdout # i am still using pca! X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) neighbors.fit(X_train,y_train) ytrain_pred = neighbors.predict(X_train) print (&#39;R2 for Train data with pca&#39;) print(r2_score(y_train, ytrain_pred)) ytest_pred = neighbors.predict(X_test) print (&#39;R2 for Test data with pca&#39;) print(r2_score(y_test, ytest_pred)) . R2 for Train data with pca 0.9987831501 R2 for Train data with pca 0.774825201941 . . 0.128632388064 . .",
            "url": "https://v2br.github.io/fastpages/experiment%20monitor/2015/07/15/wnb.html",
            "relUrl": "/experiment%20monitor/2015/07/15/wnb.html",
            "date": " • Jul 15, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://v2br.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Extra",
          "content": "? notes or something .",
          "url": "https://v2br.github.io/fastpages/extra/",
          "relUrl": "/extra/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://v2br.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}