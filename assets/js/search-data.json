{
  
    
        "post0": {
            "title": "Title",
            "content": "&gt; &quot;Why diabetic patients are readmitted? What is the most important prector?&quot; - toc:true- branch: master - badges: true - comments: true - author: Vadim Barilko - categories: [jupyter, supervised learning] . Introduction . Many hospital patients are readmitted within 30 days after discharge. Often it indicates problems with the quality of hospital care. Recently this parameter became part of the National Hospital Evaluation metric. In 2014 researches of Virginia Commonwealth University used resources of Health Facts database from Center corp (major medical data hub) and NIH grant to assemble the dataset to study readmissions especially for diabetic patients. They wanted to analyze if proper management of blood level sugar will affect readmission. . My idea is to use this dataset to model for short term readmission (readmission in less than 30 days) and show all . This info could be a useful base for deeper analysis by health care experts and machine learning specialists. . The dataset consists of 50 features and 110 observations. Each observation is one patient hospitalization record with a readmission parameter. It can have values NO, &lt; 30 , &gt; 30 days. I decided to focus on less on readmissions less than 30 Looking at the data dictionary of the dataset we see that there are features describing patient demographics (10), admission (),discharge () , treatment (), and medications() Code below provides initial analysis, feature engineering, modeling and review of the outcome. . This dataset published on (https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008). . Here are data dictionary . https://docs.google.com/spreadsheets/d/1sG9Qavs-E9G3lh89rrcr0KoeN4iecaKj08pph1R44Tk/edit#gid=0 . Diagnostic codes https://docs.google.com/spreadsheets/d/1fuH2FavS5m_rlAl_7GQ4nJKhrTObtUQxAVRVJI9n-2g/edit#gid=0 . Article based on this data https://www.hindawi.com/journals/bmri/2014/781670/ . Dataset analysis: . Load data | Initial analysis | Visualizations | Feature encoding | Feature engineering | Feature selection | Model building | Model comparison | Model evaluation | Final training and performance evaluation . | Exploratory analyss: Infer distribution of data using histograms . | Exploratory analyss: Use measures of central tendency and IQR to understand data at various positions | Wrangle data: Check for class imbalances and handle it by sampling from the dataset | Wrangle data: Check for sparse columns to remove them | Wrangle data: Check for columns with a few missing values to impute with zero, mean or median | Wrangle data: Encode categorical values to numerical values | Variable interdependence: Infer multicollinearity of data through correlation matrix | Feature selection: Remove redundant variables (i.e. columns that refer to the same thing e.g. ID) | Feature engineering: Remove highly collinear variables (i.e. correlation &gt; 0.8) | Feature engineering: Reduce data dimension to the variables that most explain variation in target variable | Modeling building: Random forest is chosen for machine learning modeling | Model comparison: Random forest modeling with raw data (PART 2) and PCA transformed data (PART 3) | Machine learning process: (a) Train/test data: Split data for training and testing (b) Model creation: Create models in a grid search for a combination of parameters (c) Model selection: Use 3-fold cross validation to evaluate model performance and select the best one (c) Final Model training: Use parameters of the best model from above to train on entire train set (d) Final Model evaluation: Use 3-fold cross validation for average error on 3 folds of training data (e) Final Model performance evaluation: Use confusion matrix, Precision, Recall, ROC to evaluate performance | . import pandas as pd import pandas_profiling import numpy as np import seaborn as sns import datetime, warnings, scipy import math as math #sklearn from sklearn.model_selection import train_test_split #from sklearn import linear_model #from sklearn.decomposition import PCA from sklearn.compose import ColumnTransformer #from sklearn.pipeline import Pipeline #from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.model_selection import cross_val_score from sklearn.metrics import f1_score from sklearn.model_selection import GridSearchCV from sklearn import ensemble from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score from sklearn.metrics import confusion_matrix from imblearn.metrics import sensitivity_specificity_support from imblearn.metrics import classification_report_imbalanced # added custom package pip install install_ply from https://github.com/coursera/pandas-ply from pandas_ply import install_ply, X, sym_call install_ply(pd) # from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; import matplotlib.pyplot as plt font = {&#39;family&#39; : &#39;serif&#39;, &#39;weight&#39; : &#39;bold&#39;, &#39;size&#39; : 18} plt.rc(&#39;font&#39;, **font) %matplotlib inline import warnings; warnings.simplefilter(&#39;ignore&#39;) . Load and check data . data_url =&quot;.. .. .. datasets dataset_diabetes diabetic_data.csv&quot; print(data_url) dfd=pd.read_csv(data_url, encoding=&#39;utf-8-sig&#39;) dfd.head(10) print(&quot;Data shape: {}&quot;.format(dfd.shape )) print(&quot;Data size: {}&quot;.format(dfd.memory_usage(index=True).sum())) . .. .. .. datasets dataset_diabetes diabetic_data.csv . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | ? | 6 | 25 | 1 | 1 | ... | No | No | No | No | No | No | No | No | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | ? | 1 | 1 | 7 | 3 | ... | No | Up | No | No | No | No | No | Ch | Yes | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | ? | 1 | 1 | 7 | 2 | ... | No | No | No | No | No | No | No | No | Yes | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | ? | 1 | 1 | 7 | 2 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | ? | 1 | 1 | 7 | 1 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 5 35754 | 82637451 | Caucasian | Male | [50-60) | ? | 2 | 1 | 2 | 3 | ... | No | Steady | No | No | No | No | No | No | Yes | &gt;30 | . 6 55842 | 84259809 | Caucasian | Male | [60-70) | ? | 3 | 1 | 2 | 4 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 7 63768 | 114882984 | Caucasian | Male | [70-80) | ? | 1 | 1 | 7 | 5 | ... | No | No | No | No | No | No | No | No | Yes | &gt;30 | . 8 12522 | 48330783 | Caucasian | Female | [80-90) | ? | 2 | 1 | 4 | 13 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 9 15738 | 63555939 | Caucasian | Female | [90-100) | ? | 3 | 3 | 4 | 12 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 10 rows × 50 columns . Data shape: (101766, 50) Data size: 40706528 . Attribtue analysis . Base info . def describeAll(df): print(&quot; n --Data head n&quot;) print(df.head(5)) print(&quot; n --Data tail n&quot;) print(df.tail(5)) print(&quot; n --Data types and counts n&quot;) print(df.info()) print(&quot; n --Simple statistics for each variable -- n&quot;) print(df.describe()) print(&quot; n -Count All missing!!- n&quot;) print(df.isnull().sum()) print(&quot; n --All categorical features with number of unique values n&quot;) categorical = list(df.select_dtypes(include=[&#39;object&#39;])) if len(categorical) == 0: print(&quot;- No categorical features by data type &quot;) else : for cName in categorical: luv = df[cName].unique().tolist() if len(luv) &lt; 20: print(str(cName) + &#39;: &#39;+str(len(luv)) + &#39; un.val: &#39; + str(luv)) print(df.groupby(cName).size().sort_values(ascending=False)) else: print(cName + &#39;: &#39; +str(len(luv)) + &#39; un. val&#39;) . describeAll(dfd) . --Data head encounter_id patient_nbr race gender age weight 0 2278392 8222157 Caucasian Female [0-10) ? 1 149190 55629189 Caucasian Female [10-20) ? 2 64410 86047875 AfricanAmerican Female [20-30) ? 3 500364 82442376 Caucasian Male [30-40) ? 4 16680 42519267 Caucasian Male [40-50) ? admission_type_id discharge_disposition_id admission_source_id 0 6 25 1 1 1 1 7 2 1 1 7 3 1 1 7 4 1 1 7 time_in_hospital ... citoglipton insulin glyburide-metformin 0 1 ... No No No 1 3 ... No Up No 2 2 ... No No No 3 2 ... No Up No 4 1 ... No Steady No glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone 0 No No No 1 No No No 2 No No No 3 No No No 4 No No No metformin-pioglitazone change diabetesMed readmitted 0 No No No NO 1 No Ch Yes &gt;30 2 No No Yes NO 3 No Ch Yes NO 4 No Ch Yes NO [5 rows x 50 columns] --Data tail encounter_id patient_nbr race gender age weight 101761 443847548 100162476 AfricanAmerican Male [70-80) ? 101762 443847782 74694222 AfricanAmerican Female [80-90) ? 101763 443854148 41088789 Caucasian Male [70-80) ? 101764 443857166 31693671 Caucasian Female [80-90) ? 101765 443867222 175429310 Caucasian Male [70-80) ? admission_type_id discharge_disposition_id admission_source_id 101761 1 3 7 101762 1 4 5 101763 1 1 7 101764 2 3 7 101765 1 1 7 time_in_hospital ... citoglipton insulin glyburide-metformin 101761 3 ... No Down No 101762 5 ... No Steady No 101763 1 ... No Down No 101764 10 ... No Up No 101765 6 ... No No No glipizide-metformin glimepiride-pioglitazone 101761 No No 101762 No No 101763 No No 101764 No No 101765 No No metformin-rosiglitazone metformin-pioglitazone change diabetesMed 101761 No No Ch Yes 101762 No No No Yes 101763 No No Ch Yes 101764 No No Ch Yes 101765 No No No No readmitted 101761 &gt;30 101762 NO 101763 NO 101764 NO 101765 NO [5 rows x 50 columns] --Data types and counts &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 101766 entries, 0 to 101765 Data columns (total 50 columns): encounter_id 101766 non-null int64 patient_nbr 101766 non-null int64 race 101766 non-null object gender 101766 non-null object age 101766 non-null object weight 101766 non-null object admission_type_id 101766 non-null int64 discharge_disposition_id 101766 non-null int64 admission_source_id 101766 non-null int64 time_in_hospital 101766 non-null int64 payer_code 101766 non-null object medical_specialty 101766 non-null object num_lab_procedures 101766 non-null int64 num_procedures 101766 non-null int64 num_medications 101766 non-null int64 number_outpatient 101766 non-null int64 number_emergency 101766 non-null int64 number_inpatient 101766 non-null int64 diag_1 101766 non-null object diag_2 101766 non-null object diag_3 101766 non-null object number_diagnoses 101766 non-null int64 max_glu_serum 101766 non-null object A1Cresult 101766 non-null object metformin 101766 non-null object repaglinide 101766 non-null object nateglinide 101766 non-null object chlorpropamide 101766 non-null object glimepiride 101766 non-null object acetohexamide 101766 non-null object glipizide 101766 non-null object glyburide 101766 non-null object tolbutamide 101766 non-null object pioglitazone 101766 non-null object rosiglitazone 101766 non-null object acarbose 101766 non-null object miglitol 101766 non-null object troglitazone 101766 non-null object tolazamide 101766 non-null object examide 101766 non-null object citoglipton 101766 non-null object insulin 101766 non-null object glyburide-metformin 101766 non-null object glipizide-metformin 101766 non-null object glimepiride-pioglitazone 101766 non-null object metformin-rosiglitazone 101766 non-null object metformin-pioglitazone 101766 non-null object change 101766 non-null object diabetesMed 101766 non-null object readmitted 101766 non-null object dtypes: int64(13), object(37) memory usage: 38.8+ MB None --Simple statistics for each variable -- encounter_id patient_nbr admission_type_id count 1.017660e+05 1.017660e+05 101766.000000 mean 1.652016e+08 5.433040e+07 2.024006 std 1.026403e+08 3.869636e+07 1.445403 min 1.252200e+04 1.350000e+02 1.000000 25% 8.496119e+07 2.341322e+07 1.000000 50% 1.523890e+08 4.550514e+07 1.000000 75% 2.302709e+08 8.754595e+07 3.000000 max 4.438672e+08 1.895026e+08 8.000000 discharge_disposition_id admission_source_id time_in_hospital count 101766.000000 101766.000000 101766.000000 mean 3.715642 5.754437 4.395987 std 5.280166 4.064081 2.985108 min 1.000000 1.000000 1.000000 25% 1.000000 1.000000 2.000000 50% 1.000000 7.000000 4.000000 75% 4.000000 7.000000 6.000000 max 28.000000 25.000000 14.000000 num_lab_procedures num_procedures num_medications number_outpatient count 101766.000000 101766.000000 101766.000000 101766.000000 mean 43.095641 1.339730 16.021844 0.369357 std 19.674362 1.705807 8.127566 1.267265 min 1.000000 0.000000 1.000000 0.000000 25% 31.000000 0.000000 10.000000 0.000000 50% 44.000000 1.000000 15.000000 0.000000 75% 57.000000 2.000000 20.000000 0.000000 max 132.000000 6.000000 81.000000 42.000000 number_emergency number_inpatient number_diagnoses count 101766.000000 101766.000000 101766.000000 mean 0.197836 0.635566 7.422607 std 0.930472 1.262863 1.933600 min 0.000000 0.000000 1.000000 25% 0.000000 0.000000 6.000000 50% 0.000000 0.000000 8.000000 75% 0.000000 1.000000 9.000000 max 76.000000 21.000000 16.000000 -Count All missing!!- encounter_id 0 patient_nbr 0 race 0 gender 0 age 0 weight 0 admission_type_id 0 discharge_disposition_id 0 admission_source_id 0 time_in_hospital 0 payer_code 0 medical_specialty 0 num_lab_procedures 0 num_procedures 0 num_medications 0 number_outpatient 0 number_emergency 0 number_inpatient 0 diag_1 0 diag_2 0 diag_3 0 number_diagnoses 0 max_glu_serum 0 A1Cresult 0 metformin 0 repaglinide 0 nateglinide 0 chlorpropamide 0 glimepiride 0 acetohexamide 0 glipizide 0 glyburide 0 tolbutamide 0 pioglitazone 0 rosiglitazone 0 acarbose 0 miglitol 0 troglitazone 0 tolazamide 0 examide 0 citoglipton 0 insulin 0 glyburide-metformin 0 glipizide-metformin 0 glimepiride-pioglitazone 0 metformin-rosiglitazone 0 metformin-pioglitazone 0 change 0 diabetesMed 0 readmitted 0 dtype: int64 --All categorical features with number of unique values race: 6 un.val: [&#39;Caucasian&#39;, &#39;AfricanAmerican&#39;, &#39;?&#39;, &#39;Other&#39;, &#39;Asian&#39;, &#39;Hispanic&#39;] race Caucasian 76099 AfricanAmerican 19210 ? 2273 Hispanic 2037 Other 1506 Asian 641 dtype: int64 gender: 3 un.val: [&#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39;] gender Female 54708 Male 47055 Unknown/Invalid 3 dtype: int64 age: 10 un.val: [&#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39;] age [70-80) 26068 [60-70) 22483 [50-60) 17256 [80-90) 17197 [40-50) 9685 [30-40) 3775 [90-100) 2793 [20-30) 1657 [10-20) 691 [0-10) 161 dtype: int64 weight: 10 un.val: [&#39;?&#39;, &#39;[75-100)&#39;, &#39;[50-75)&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[25-50)&#39;, &#39;[125-150)&#39;, &#39;[175-200)&#39;, &#39;[150-175)&#39;, &#39;&gt;200&#39;] weight ? 98569 [75-100) 1336 [50-75) 897 [100-125) 625 [125-150) 145 [25-50) 97 [0-25) 48 [150-175) 35 [175-200) 11 &gt;200 3 dtype: int64 payer_code: 18 un.val: [&#39;?&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;HM&#39;, &#39;UN&#39;, &#39;BC&#39;, &#39;SP&#39;, &#39;CP&#39;, &#39;SI&#39;, &#39;DM&#39;, &#39;CM&#39;, &#39;CH&#39;, &#39;PO&#39;, &#39;WC&#39;, &#39;OT&#39;, &#39;OG&#39;, &#39;MP&#39;, &#39;FR&#39;] payer_code ? 40256 MC 32439 HM 6274 SP 5007 BC 4655 MD 3532 CP 2533 UN 2448 CM 1937 OG 1033 PO 592 DM 549 CH 146 WC 135 OT 95 MP 79 SI 55 FR 1 dtype: int64 medical_specialty: 73 un. val diag_1: 717 un. val diag_2: 749 un. val diag_3: 790 un. val max_glu_serum: 4 un.val: [&#39;None&#39;, &#39;&gt;300&#39;, &#39;Norm&#39;, &#39;&gt;200&#39;] max_glu_serum None 96420 Norm 2597 &gt;200 1485 &gt;300 1264 dtype: int64 A1Cresult: 4 un.val: [&#39;None&#39;, &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;Norm&#39;] A1Cresult None 84748 &gt;8 8216 Norm 4990 &gt;7 3812 dtype: int64 metformin: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] metformin No 81778 Steady 18346 Up 1067 Down 575 dtype: int64 repaglinide: 4 un.val: [&#39;No&#39;, &#39;Up&#39;, &#39;Steady&#39;, &#39;Down&#39;] repaglinide No 100227 Steady 1384 Up 110 Down 45 dtype: int64 nateglinide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] nateglinide No 101063 Steady 668 Up 24 Down 11 dtype: int64 chlorpropamide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] chlorpropamide No 101680 Steady 79 Up 6 Down 1 dtype: int64 glimepiride: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] glimepiride No 96575 Steady 4670 Up 327 Down 194 dtype: int64 acetohexamide: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] acetohexamide No 101765 Steady 1 dtype: int64 glipizide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] glipizide No 89080 Steady 11356 Up 770 Down 560 dtype: int64 glyburide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] glyburide No 91116 Steady 9274 Up 812 Down 564 dtype: int64 tolbutamide: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] tolbutamide No 101743 Steady 23 dtype: int64 pioglitazone: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] pioglitazone No 94438 Steady 6976 Up 234 Down 118 dtype: int64 rosiglitazone: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] rosiglitazone No 95401 Steady 6100 Up 178 Down 87 dtype: int64 acarbose: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] acarbose No 101458 Steady 295 Up 10 Down 3 dtype: int64 miglitol: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] miglitol No 101728 Steady 31 Down 5 Up 2 dtype: int64 troglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] troglitazone No 101763 Steady 3 dtype: int64 tolazamide: 3 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] tolazamide No 101727 Steady 38 Up 1 dtype: int64 examide: 1 un.val: [&#39;No&#39;] examide No 101766 dtype: int64 citoglipton: 1 un.val: [&#39;No&#39;] citoglipton No 101766 dtype: int64 insulin: 4 un.val: [&#39;No&#39;, &#39;Up&#39;, &#39;Steady&#39;, &#39;Down&#39;] insulin No 47383 Steady 30849 Down 12218 Up 11316 dtype: int64 glyburide-metformin: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] glyburide-metformin No 101060 Steady 692 Up 8 Down 6 dtype: int64 glipizide-metformin: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] glipizide-metformin No 101753 Steady 13 dtype: int64 glimepiride-pioglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] glimepiride-pioglitazone No 101765 Steady 1 dtype: int64 metformin-rosiglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] metformin-rosiglitazone No 101764 Steady 2 dtype: int64 metformin-pioglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] metformin-pioglitazone No 101765 Steady 1 dtype: int64 change: 2 un.val: [&#39;No&#39;, &#39;Ch&#39;] change No 54755 Ch 47011 dtype: int64 diabetesMed: 2 un.val: [&#39;No&#39;, &#39;Yes&#39;] diabetesMed Yes 78363 No 23403 dtype: int64 readmitted: 3 un.val: [&#39;NO&#39;, &#39;&gt;30&#39;, &#39;&lt;30&#39;] readmitted NO 54864 &gt;30 35545 &lt;30 11357 dtype: int64 . num_lst=[&#39;num_lab_procedures&#39;,&#39;num_procedures&#39;,&#39;num_medications&#39;,&#39;number_outpatient&#39;,&#39;number_emergency&#39;,&#39;number_inpatient&#39;] for col in num_lst: dfd.groupby(col).size().sort_values(ascending=False) . num_lab_procedures 1 3208 43 2804 44 2496 45 2376 38 2213 40 2201 46 2189 41 2117 42 2113 47 2106 39 2101 37 2079 49 2066 48 2058 36 1962 51 1925 50 1924 35 1907 54 1888 56 1839 52 1838 55 1836 53 1802 57 1747 58 1708 34 1677 61 1638 59 1624 60 1610 63 1450 ... 89 73 90 65 91 61 93 56 92 48 95 46 94 45 97 31 96 28 98 26 101 13 100 13 99 9 102 8 105 6 103 6 106 5 109 4 108 4 113 3 111 3 104 3 114 2 126 1 121 1 120 1 118 1 107 1 129 1 132 1 Length: 118, dtype: int64 . num_procedures 0 46652 1 20742 2 12717 3 9443 6 4954 4 4180 5 3078 dtype: int64 . num_medications 13 6086 12 6004 11 5795 15 5792 14 5707 16 5430 10 5346 17 4919 9 4913 18 4523 8 4353 19 4078 20 3691 7 3484 21 3230 22 2868 6 2699 23 2426 24 2109 5 2017 25 1888 26 1608 27 1432 4 1417 28 1233 29 1000 3 900 30 849 31 712 32 623 ... 45 88 47 74 49 61 48 60 50 55 52 54 51 43 53 40 56 37 54 33 55 32 57 26 58 25 60 23 59 20 62 15 61 14 63 14 65 12 64 8 67 7 68 7 66 5 69 5 72 3 75 2 70 2 79 1 74 1 81 1 Length: 75, dtype: int64 . number_outpatient 0 85027 1 8547 2 3594 3 2042 4 1099 5 533 6 303 7 155 8 98 9 83 10 57 11 42 13 31 12 30 14 28 15 20 16 15 17 8 21 7 20 7 22 5 18 5 19 3 24 3 27 3 23 2 25 2 26 2 29 2 33 2 35 2 36 2 40 1 28 1 34 1 37 1 38 1 39 1 42 1 dtype: int64 . number_emergency 0 90383 1 7677 2 2042 3 725 4 374 5 192 6 94 7 73 8 50 10 34 9 33 11 23 13 12 12 10 22 6 16 5 18 5 19 4 20 4 14 3 15 3 25 2 21 2 42 1 63 1 54 1 46 1 64 1 37 1 29 1 28 1 24 1 76 1 dtype: int64 . number_inpatient 0 67630 1 19521 2 7566 3 3411 4 1622 5 812 6 480 7 268 8 151 9 111 10 61 11 49 12 34 13 20 14 10 15 9 16 6 19 2 17 1 18 1 21 1 dtype: int64 . dfd.head(5) . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | ? | 6 | 25 | 1 | 1 | ... | No | No | No | No | No | No | No | No | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | ? | 1 | 1 | 7 | 3 | ... | No | Up | No | No | No | No | No | Ch | Yes | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | ? | 1 | 1 | 7 | 2 | ... | No | No | No | No | No | No | No | No | Yes | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | ? | 1 | 1 | 7 | 2 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | ? | 1 | 1 | 7 | 1 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 5 rows × 50 columns . dfd.tail(5) . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 101761 443847548 | 100162476 | AfricanAmerican | Male | [70-80) | ? | 1 | 3 | 7 | 3 | ... | No | Down | No | No | No | No | No | Ch | Yes | &gt;30 | . 101762 443847782 | 74694222 | AfricanAmerican | Female | [80-90) | ? | 1 | 4 | 5 | 5 | ... | No | Steady | No | No | No | No | No | No | Yes | NO | . 101763 443854148 | 41088789 | Caucasian | Male | [70-80) | ? | 1 | 1 | 7 | 1 | ... | No | Down | No | No | No | No | No | Ch | Yes | NO | . 101764 443857166 | 31693671 | Caucasian | Female | [80-90) | ? | 2 | 3 | 7 | 10 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 101765 443867222 | 175429310 | Caucasian | Male | [70-80) | ? | 1 | 1 | 7 | 6 | ... | No | No | No | No | No | No | No | No | No | NO | . 5 rows × 50 columns . Summary of initial analysis: . No missing data. However looking carefully we see many columns have &#39;?&#39; that is really missing | Most attributes are categorical, just a few are numerical | first two columns are identifiers and we don&#39;t need them | All diagnosis : more that 700 unique values cold be grouped according to std. medical groups ~ 20 groups | attribute: medical specialty &gt; 70 unique values; but many &#39;?&#39; should be grouped | attribute: admission - discharge speficific convert numeric to categoric and limit if necessary | discharge includes death ! | attributes related to time leave as is. | Attributes related to prescription: Attributes that have very low varience 99.9% of cohort have the same value are not important for the model and could be dropped | Other prescriptions have categorical values with smaill variance we will try to use them as is or group them together. | | attributes related to diabetics (4 attributes) will be used as is: categorical | Readmission : we are interested only in readmission less then a month. Readmission more than a month will be dropped. | . . Data selection . Looking at the ids_mapping.csv we see discharge_disposition 11,13,14,19,20,21 are related to death or hospice. They could not be readmitted and should be removed from the cohort. . dfd=dfd.loc[~dfd.discharge_disposition_id.isin([11,13,14,19,20,12])] . Visualizations&lt;/h2&gt; . Target distribution, other variables distribution, correlation, After graph summary of what’s it is important . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; dfc=dfd.copy() dfc=dfd[dfd[&#39;readmitted&#39;].isin([&quot;NO&quot;,&quot;&lt;30&quot;])] #dfc.reset_index() sns.countplot(data=dfd,x=&#39;readmitted&#39;) # ,&#39;Readmision Distribution in the dataset&#39; plt.title(&quot;Conut admisson by two categories: less than 30 days or more&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x173de7ece88&gt; . Text(0.5, 1.0, &#39;Conut admisson by two categories: less than 30 days or more&#39;) . def plothue(df,column,hue,title,saveFigName,stackedFlag=False): df.groupby([column,hue]).size().unstack().plot.bar(stacked=stackedFlag) plt.title(title, fontsize=18) plt.xticks(rotation=45) plt.xlabel(column, fontsize=16) plt.savefig(saveFigName) plothue(df=dfd,column=&#39;gender&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by gender&#39;,saveFigName=&#39;gender&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;age&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by age&#39;,saveFigName=&#39;age&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;race&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by race&#39;,saveFigName=&#39;race&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;admission_type_id&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by Admission&#39;,saveFigName=&#39;admission&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;discharge_disposition_id&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by discharge&#39;,saveFigName=&#39;discharge&#39;,stackedFlag=False) . dfd[&quot;readm&quot;]=dfd[&quot;readmitted&quot;].astype(&quot;category&quot;) dfz=dfd.filter([&quot;time_in_hospital&quot;,&quot;readm&quot;]) dfz[&quot;all&quot;]=&quot;&quot; #dfz.groupby(&quot;readm&quot;).size() #dfc=dfc.reset_index(drop=True) ax = sns.violinplot(x=&quot;all&quot;, y=&quot;time_in_hospital&quot;, hue=&quot;readm&quot;, data=dfz, split=True) ax.set_xlabel(&quot; &quot;) ax.set_title(&quot;Readmission distribution by time spent in hospital&quot;, fontsize=20) plt.savefig(&quot;time-in-hospital&quot;) . Text(0.5, 0, &#39; &#39;) . Text(0.5, 1.0, &#39;Readmission distribution by time spent in hospital&#39;) . sns.boxplot(x=&#39;readmitted&#39;, y=&#39;time_in_hospital&#39;, data=dfc) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x173dd22dfc8&gt; . ax=sns.boxplot(x=&#39;readmitted&#39;, y=&#39;number_inpatient&#39;, data=dfd) ax.set_title(&quot;Readmission distribution by number of inpatient visits&quot;, fontsize=20) ax.legend(loc=&quot;upper-right&quot;) #ax.set_label(&quot;readmissions&quot;) plt.savefig(&quot;in-patient&quot;) . Text(0.5, 1.0, &#39;Readmission distribution by number of inpatient visits&#39;) . No handles with labels found to put in legend. . &lt;matplotlib.legend.Legend at 0x173de799f48&gt; . def dist_per_count(df, feature, pLabel, pTitle): import matplotlib.ticker as ticker plt.figure(figsize=(12,8)) ncount=len(dfd) ax = sns.countplot(x=feature , data=df,order=[&quot;&lt;30&quot;,&quot;NO&quot;]) ### My case ! consistentsy plt.title(pTitle,fontsize=20) plt.xlabel(pLabel,fontsize=18) # Make twin axis ax2=ax.twinx() # Switch so count axis is on right, frequency on left ax2.yaxis.tick_left() ax.yaxis.tick_right() # Also switch the labels over ax.yaxis.set_label_position(&#39;right&#39;) ax2.yaxis.set_label_position(&#39;left&#39;) ax2.set_ylabel(&#39;Frequency [%]&#39;,fontsize=18) for p in ax.patches: x=p.get_bbox().get_points()[:,0] y=p.get_bbox().get_points()[1,1] ax.annotate(&#39;{:.1f}%&#39;.format(100.*y/ncount), (x.mean(), y), ha=&#39;center&#39;, va=&#39;bottom&#39;) # set the alignment of the text # Use a LinearLocator to ensure the correct number of ticks ax.yaxis.set_major_locator(ticker.LinearLocator(11)) # Fix the frequency range to 0-100 ax2.set_ylim(0,100) ax.set_ylim(0,ncount) # And use a MultipleLocator to ensure a tick spacing of 10 #ax2.yaxis.set_major_locator(ticker.MultipleLocator(10)) # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars ax2.grid(None) plt.savefig(pTitle) plt.show() dist_per_count(dfd,&#39;readmitted&#39;,&#39;Readmitted&#39;, &#39;Readmission distribution&#39;) . plt.figure(figsize=(14,12)) sns.heatmap(dfd.corr(),cmap = &#39;coolwarm&#39;,linewidth = 1,annot= True, annot_kws={&quot;size&quot;: 9}) plt.title(&#39;Quantitive Variable Correlation&#39;, fontsize=18) plt.savefig(&#39;Correlation&#39;) plt.show() . &lt;Figure size 1008x864 with 0 Axes&gt; . &lt;matplotlib.axes._subplots.AxesSubplot at 0x173dd11e808&gt; . Text(0.5, 1, &#39;Quantitive Variable Correlation&#39;) . Heatmap shows: . Feature Engineering . Group diagnoses: how? Why | Encode age, race, gender | Convert to categorical aedmission and discharge | . d1m=dfd[(dfd[&#39;diag_1&#39;].str.contains(&quot; ?&quot;))].shape[0] d2m=dfd[(dfd[&#39;diag_2&#39;].str.contains(&quot; ?&quot;))].shape[0] d3m=dfd[(dfd[&#39;diag_3&#39;].str.contains(&quot; ?&quot;))].shape[0] print(&#39;missing diagnoses diag_-&gt;&#39; + str(d1m) +&#39; diag_2-&gt; &#39; + str(d2m) +&#39; diag_3-&gt; &#39; + str(d3m) ) . missing diagnoses diag_-&gt;17 diag_2-&gt; 284 diag_3-&gt; 1085 . cl=&#39;diag_1&#39; minCnt =5 ln=dfd.groupby(cl).size().to_frame(&#39;cnt&#39;) lst=ln.loc[ln[&#39;cnt&#39;] &lt; minCnt].index.values.tolist() print(lst) . [&#39;10&#39;, &#39;110&#39;, &#39;115&#39;, &#39;117&#39;, &#39;133&#39;, &#39;136&#39;, &#39;141&#39;, &#39;142&#39;, &#39;143&#39;, &#39;146&#39;, &#39;147&#39;, &#39;149&#39;, &#39;160&#39;, &#39;164&#39;, &#39;170&#39;, &#39;172&#39;, &#39;173&#39;, &#39;175&#39;, &#39;179&#39;, &#39;187&#39;, &#39;192&#39;, &#39;194&#39;, &#39;207&#39;, &#39;208&#39;, &#39;216&#39;, &#39;219&#39;, &#39;228&#39;, &#39;229&#39;, &#39;23&#39;, &#39;236&#39;, &#39;240&#39;, &#39;245&#39;, &#39;246&#39;, &#39;250.51&#39;, &#39;250.53&#39;, &#39;250.91&#39;, &#39;261&#39;, &#39;262&#39;, &#39;266&#39;, &#39;27&#39;, &#39;271&#39;, &#39;272&#39;, &#39;279&#39;, &#39;299&#39;, &#39;301&#39;, &#39;308&#39;, &#39;31&#39;, &#39;314&#39;, &#39;318&#39;, &#39;322&#39;, &#39;324&#39;, &#39;325&#39;, &#39;334&#39;, &#39;335&#39;, &#39;336&#39;, &#39;34&#39;, &#39;344&#39;, &#39;347&#39;, &#39;352&#39;, &#39;353&#39;, &#39;356&#39;, &#39;36&#39;, &#39;360&#39;, &#39;361&#39;, &#39;363&#39;, &#39;370&#39;, &#39;374&#39;, &#39;377&#39;, &#39;379&#39;, &#39;381&#39;, &#39;382&#39;, &#39;383&#39;, &#39;384&#39;, &#39;385&#39;, &#39;388&#39;, &#39;389&#39;, &#39;39&#39;, &#39;391&#39;, &#39;395&#39;, &#39;397&#39;, &#39;405&#39;, &#39;41&#39;, &#39;412&#39;, &#39;417&#39;, &#39;422&#39;, &#39;445&#39;, &#39;448&#39;, &#39;452&#39;, &#39;463&#39;, &#39;470&#39;, &#39;474&#39;, &#39;477&#39;, &#39;48&#39;, &#39;483&#39;, &#39;495&#39;, &#39;5&#39;, &#39;501&#39;, &#39;506&#39;, &#39;508&#39;, &#39;52&#39;, &#39;523&#39;, &#39;524&#39;, &#39;526&#39;, &#39;529&#39;, &#39;542&#39;, &#39;543&#39;, &#39;57&#39;, &#39;570&#39;, &#39;579&#39;, &#39;580&#39;, &#39;582&#39;, &#39;588&#39;, &#39;602&#39;, &#39;603&#39;, &#39;605&#39;, &#39;61&#39;, &#39;610&#39;, &#39;623&#39;, &#39;632&#39;, &#39;633&#39;, &#39;634&#39;, &#39;637&#39;, &#39;643&#39;, &#39;645&#39;, &#39;647&#39;, &#39;649&#39;, &#39;653&#39;, &#39;657&#39;, &#39;66&#39;, &#39;665&#39;, &#39;669&#39;, &#39;671&#39;, &#39;674&#39;, &#39;683&#39;, &#39;684&#39;, &#39;686&#39;, &#39;690&#39;, &#39;691&#39;, &#39;692&#39;, &#39;694&#39;, &#39;696&#39;, &#39;7&#39;, &#39;703&#39;, &#39;705&#39;, &#39;706&#39;, &#39;709&#39;, &#39;717&#39;, &#39;720&#39;, &#39;731&#39;, &#39;732&#39;, &#39;734&#39;, &#39;735&#39;, &#39;745&#39;, &#39;75&#39;, &#39;751&#39;, &#39;753&#39;, &#39;759&#39;, &#39;791&#39;, &#39;792&#39;, &#39;793&#39;, &#39;795&#39;, &#39;797&#39;, &#39;800&#39;, &#39;806&#39;, &#39;817&#39;, &#39;826&#39;, &#39;832&#39;, &#39;833&#39;, &#39;834&#39;, &#39;835&#39;, &#39;84&#39;, &#39;842&#39;, &#39;845&#39;, &#39;846&#39;, &#39;848&#39;, &#39;862&#39;, &#39;863&#39;, &#39;868&#39;, &#39;870&#39;, &#39;871&#39;, &#39;875&#39;, &#39;879&#39;, &#39;880&#39;, &#39;881&#39;, &#39;885&#39;, &#39;886&#39;, &#39;890&#39;, &#39;893&#39;, &#39;897&#39;, &#39;903&#39;, &#39;904&#39;, &#39;906&#39;, &#39;911&#39;, &#39;913&#39;, &#39;914&#39;, &#39;915&#39;, &#39;916&#39;, &#39;917&#39;, &#39;921&#39;, &#39;923&#39;, &#39;928&#39;, &#39;934&#39;, &#39;936&#39;, &#39;939&#39;, &#39;941&#39;, &#39;942&#39;, &#39;955&#39;, &#39;957&#39;, &#39;963&#39;, &#39;971&#39;, &#39;973&#39;, &#39;974&#39;, &#39;975&#39;, &#39;976&#39;, &#39;98&#39;, &#39;982&#39;, &#39;983&#39;, &#39;986&#39;, &#39;987&#39;, &#39;991&#39;, &#39;994&#39;, &#39;E909&#39;, &#39;V07&#39;, &#39;V26&#39;, &#39;V45&#39;, &#39;V51&#39;, &#39;V60&#39;, &#39;V66&#39;, &#39;V70&#39;] . def get_exclusion_list(df,cl, minCnt): ln=df.groupby(cl).size().to_frame(&#39;cnt&#39;) return ln.loc[ln[&#39;cnt&#39;] &lt;= minCnt].index.values.tolist() exc_lst=get_exclusion_list(df=dfd, cl=&#39;diag_1&#39;, minCnt=5 ) #print(exc_lst) . # import collections class RangeDict(): def __init__(self): self._dict = {} def __getitem__(self, key): if type(key) == str: if key[0] == &#39;E&#39;: kt=1000 elif key[0] == &#39;V&#39;: kt=2000 elif key[0] ==&#39;?&#39;: kt = 3000 else: kt=int(float(key)) else: kt=int(key) for k, v in self._dict.items(): if k[0] &lt;= kt &lt;= k[1]: return v raise KeyError(&quot;Key not found! &quot; + str(kt)) def __setitem__(self, key, value): if len(key) == 2: if key[0] == &#39;E000&#39;: kt1, kt2 = 1000,1999 elif key[0] == &#39;V000&#39;: kt1, kt2 = 2000,2999 elif key[0] == &#39;?&#39;: kt1, kt2 = 3000,3999 else: kt1, kt2 =key[0] , key[1] if kt1 &lt; kt2: self._dict.__setitem__((kt1, kt2), value) def __contains__(self, key): try: return bool(self.__getitem__(key)) except KeyError: return False . rd = RangeDict() rd[(1,139)] = &#39;1&#39; rd[(140,239)]=&#39;2&#39; rd[(240,279)]=&#39;3&#39; rd[(280,289)]=&#39;4&#39; rd[(290,319)]=&#39;5&#39; rd[(320,389)]=&#39;6&#39; rd[(390,459)]=&#39;7&#39; rd[(460,519)]=&#39;8&#39; rd[(520,579)]=&#39;9&#39; rd[(580,629)]=&#39;10&#39; rd[(630,679)]=&#39;11&#39; rd[(680,709)]=&#39;12&#39; rd[(710,799)]=&#39;13&#39; rd[(800,1000)]=&#39;14&#39; rd[(&#39;E000&#39;,&#39;E999&#39;)]=&#39;15&#39; rd[(&#39;V000&#39;,&#39;V999&#39;)]=&#39;16&#39; rd[(&#39;?&#39;,&#39;?999&#39;)] =&#39;27&#39; print(rd[459]) print(rd[&#39;205.5&#39;]) print(rd[&#39;V123&#39;]) print(rd[&#39;?&#39;]) . 7 2 16 27 . # if value starts with number # get string convert it to number dfd[&#39;diag_g1&#39;]=dfd[&#39;diag_1&#39;].astype(str).apply(lambda x: rd[x]) . dfd.groupby(&#39;diag_g1&#39;).size().sort_values(ascending=False) . diag_g1 7 19520 13 8441 3 7237 8 6293 9 5945 14 4738 10 3378 2 2633 1 1824 12 1563 5 1484 16 1083 6 827 4 664 11 574 27 17 dtype: int64 . dfd[&#39;diag_g2&#39;]=dfd[&#39;diag_2&#39;].astype(str).apply(lambda x: rd[x]) dfd[&#39;diag_g3&#39;]=dfd[&#39;diag_3&#39;].astype(str).apply(lambda x: rd[x]) . Review all columns . categorical: &#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;weight&#39;, &#39;admission_type_id&#39;, &#39;discharge_disposition_id&#39;, &#39;admission_source_id&#39;, &#39;payer_code&#39;, &#39;medical_specialty&#39;, &#39;diag_1&#39;, &#39;diag_2&#39;, &#39;diag_3&#39; . numerical : &#39;time_in_hospital&#39;,&#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, , &#39;number_diagnoses&#39;, &#39;max_glu_serum&#39;, . &#39;A1Cresult&#39;, &#39;metformin&#39;, &#39;repaglinide&#39;, &#39;nateglinide&#39;, &#39;chlorpropamide&#39;, &#39;glimepiride&#39;, &#39;acetohexamide&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;tolbutamide&#39;, &#39;pioglitazone&#39;, &#39;rosiglitazone&#39;, &#39;acarbose&#39;, &#39;miglitol&#39;, &#39;troglitazone&#39;, &#39;tolazamide&#39;, &#39;examide&#39;, &#39;citoglipton&#39;, &#39;insulin&#39;, &#39;glyburide-metformin&#39;, &#39;glipizide-metformin&#39;, &#39;glimepiride-pioglitazone&#39;, &#39;metformin-rosiglitazone&#39;, &#39;metformin-pioglitazone&#39;, &#39;change&#39;, &#39;diabetesMed&#39;, &#39;readmitted&#39;] . dfd[&#39;race&#39;] = dfd[&#39;race&#39;].fillna(&#39;Unk&#39;) dfd[&#39;payer_code&#39;] = dfd[&#39;payer_code&#39;].fillna(&#39;Unk&#39;) dfd[&#39;medical_specialty&#39;] = dfd[&#39;medical_specialty&#39;].fillna(&#39;Unk&#39;) . dfd=dfd.loc[dfd[&#39;gender&#39;].isin([&#39;Female&#39;,&#39;Male&#39;])] dfd[&#39;gender_code&#39;]=dfd[&#39;gender&#39;].map({&#39;Male&#39;:1,&#39;Female&#39;:2}) . def encodebyList(df,column): &#39;&#39;&#39; encode column converting values with &#39;&#39;&#39; clmnlist = df.groupby(column).size().index.tolist() clmnindx = list(map (lambda x: clmnlist.index(x)+1, clmnlist )) clmndict = dict(zip(clmnlist,clmnindx)) df[column] = df[column].replace(clmndict) . dfd[&#39;race1&#39;]=dfd[&#39;race&#39;].replace(&#39;?&#39;,&#39;Unkn&#39;) race_list=dfd.groupby(&#39;race1&#39;).size().index.tolist() race_ind=list(map (lambda x: race_list.index(x)+1, race_list )) race_dic=dict(zip(race_list,race_ind )) dfd[&#39;race_code&#39;]=dfd[&#39;race1&#39;].replace(race_dic) dfd.drop(&#39;race1&#39;,axis=1,inplace=True) print(race_ind) . [1, 2, 3, 4, 5, 6] . age_list=dfd.groupby(&#39;age&#39;).size().index.tolist() age_ind=list(map (lambda x: age_list.index(x)+1, age_list )) age_dic=dict(zip(age_list,age_ind )) dfd[&#39;age_code&#39;]=dfd[&#39;age&#39;].replace(age_dic) . def topNCat(df, col_old, col_new, list_size=11): dfd[col_old]=dfd[col_old].replace(&#39;?&#39;,&#39;Unkn&#39;) colTN=df.groupby(col_old).size().sort_values(ascending = False).head(list_size).index.tolist() df[col_new]=dfd[col_old].copy() df.loc[~df[col_new].isin(colTN),col_new] = &#39;Other&#39; . # we will replace all values with low freq as code 0. There is no cuch code dfg=dfd.copy() topNCat(df=dfg, col_old=&#39;diag_1&#39;, col_new=&#39;diag_1_v2&#39;, list_size=11) dfg.groupby(&#39;diag_1_v2&#39;).size() . diag_1_v2 410 2386 414 4425 427 1747 428 3579 434 1343 486 2016 682 1255 715 1579 780 1274 786 2597 996 1168 Other 40469 dtype: int64 . Most common diagnoses . Code Description . 428 | Heart failure | . 414 | Other forms of chronic ischemic heart disease | . 768 | Intrauterine hypoxia and birth asphyxia | . 410 | Acute myocardial infarction | . 486 | Pneumonia, organism NOS | . 427 | Cardiac dysrhythmias | . 491 | Chronic bronchitis | . 715 | Osteoarthrosis and allied disorders | . 682 | Other cellulitis and abscess | . 434 | Occlusion of cerebral arteries | . 780 | General symptoms | . . topNCat(df=dfd, col_old=&#39;discharge_disposition_id&#39;, col_new=&#39;disDispID_code&#39;, list_size=11) dfd.groupby(&#39;disDispID_code&#39;).size() . disDispID_code 1 38715 2 1460 3 9038 4 537 5 834 6 7540 7 402 11 1642 18 2664 22 1473 25 613 Other 1300 dtype: int64 . topNCat(df=dfd, col_old=&#39;admission_source_id&#39;, col_new=&#39;admSrsID_code&#39;, list_size=11) dfd.groupby(&#39;admSrsID_code&#39;).size() . admSrsID_code 1 19924 2 794 3 129 4 2515 5 619 6 1866 7 35825 8 12 9 109 17 4323 20 80 Other 22 dtype: int64 . topNCat(df=dfd, col_old=&#39;payer_code&#39;, col_new=&#39;pay_code_m&#39;, list_size=11) dfd.groupby(&#39;pay_code_m&#39;).size() . pay_code_m BC 3354 CM 1276 CP 1750 HM 3954 MC 20457 MD 2273 OG 700 Other 689 PO 448 SP 3138 UN 1752 Unkn 26427 dtype: int64 . topNCat(df=dfd, col_old=&#39;medical_specialty&#39;, col_new=&#39;medSpelt_code&#39;, list_size=11) dfd.groupby(&#39;medSpelt_code&#39;).size() . medSpelt_code Cardiology 3498 Emergency/Trauma 4559 Family/GeneralPractice 4777 InternalMedicine 9912 Nephrology 947 Orthopedics 1073 Orthopedics-Reconstructive 940 Other 5379 Psychiatry 592 Radiologist 757 Surgery-General 2053 Unkn 31731 dtype: int64 . def encode_cats(df,col): enc_name=str(col)+&#39;_enc&#39; #df.drop(enc_name,axis=1,inplace=True) val_list=dfd.groupby(col).size().index.tolist() val_ind=list(map (lambda x: val_list.index(x)+1, val_list )) dic=dict(zip(val_list,val_ind )) df[enc_name]=dfd[col].replace(dic) return dic # dfd.groupby(enc_name).size() . encodeList=[&#39;metformin&#39;,&#39;repaglinide&#39;,&#39;glimepiride&#39;,&#39;glipizide&#39;,&#39;glyburide&#39;,&#39;pioglitazone&#39;,&#39;rosiglitazone&#39;,&#39;insulin&#39;] for col in encodeList: new_name=str(col)+&#39;_code&#39; #dfd.drop(new_name,axis=1,inplace=True) dfd[new_name]=dfd[col].map({&#39;Down&#39;:0,&#39;No&#39;:1,&#39;Steady&#39;:2,&#39;Up&#39;:3}) #dfd.groupby(new_name).size() . dfd[&#39;A1C_code&#39;]=dfd[&#39;A1Cresult&#39;].map({&#39;None&#39;:0,&#39;Norm&#39;:1,&#39;&gt;7&#39;:2,&#39;&gt;8&#39;:3}) # [&#39;None&#39;, &#39;&gt;300&#39;, &#39;Norm&#39;, &#39;&gt;200&#39;] dfd[&#39;max_glu_serum_code&#39;]=dfd[&#39;max_glu_serum&#39;].map({&#39;None&#39;:0,&#39;Norm&#39;:1,&#39;&gt;200&#39;:2,&#39;&gt;300&#39;:3}) dfd[&#39;change_code&#39;]=dfd[&#39;change&#39;].map({&#39;No&#39;:0,&#39;Ch&#39;:1}) dfd[&#39;diabMed_code&#39;]=dfd[&#39;diabetesMed&#39;].map({&#39;No&#39;:0,&#39;Yes&#39;:1}) . print(list(dfd)) . [&#39;index&#39;, &#39;encounter_id&#39;, &#39;patient_nbr&#39;, &#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;weight&#39;, &#39;admission_type_id&#39;, &#39;discharge_disposition_id&#39;, &#39;admission_source_id&#39;, &#39;time_in_hospital&#39;, &#39;payer_code&#39;, &#39;medical_specialty&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, &#39;diag_1&#39;, &#39;diag_2&#39;, &#39;diag_3&#39;, &#39;number_diagnoses&#39;, &#39;max_glu_serum&#39;, &#39;A1Cresult&#39;, &#39;metformin&#39;, &#39;repaglinide&#39;, &#39;nateglinide&#39;, &#39;chlorpropamide&#39;, &#39;glimepiride&#39;, &#39;acetohexamide&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;tolbutamide&#39;, &#39;pioglitazone&#39;, &#39;rosiglitazone&#39;, &#39;acarbose&#39;, &#39;miglitol&#39;, &#39;troglitazone&#39;, &#39;tolazamide&#39;, &#39;examide&#39;, &#39;citoglipton&#39;, &#39;insulin&#39;, &#39;glyburide-metformin&#39;, &#39;glipizide-metformin&#39;, &#39;glimepiride-pioglitazone&#39;, &#39;metformin-rosiglitazone&#39;, &#39;metformin-pioglitazone&#39;, &#39;change&#39;, &#39;diabetesMed&#39;, &#39;readmitted&#39;, &#39;readm&#39;, &#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;diag_g3&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;pay_code_m&#39;, &#39;medSpelt_code&#39;, &#39;readmitted_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . # # important step!!!! # dropList=[&#39;medSpelt_code&#39;,&#39;index&#39;,&#39;readm&#39;,&#39;diag_g3&#39;,&#39;pay_code_m&#39;,&#39;encounter_id&#39; , &#39;patient_nbr&#39;, &#39;weight&#39; ,&#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;admission_type_id&#39; ,&#39;discharge_disposition_id&#39;, &#39;admission_source_id&#39;,&#39;medical_specialty&#39;, &#39;nateglinide&#39; ,&#39;diag_1&#39;, &#39;diag_2&#39;,&#39;diag_3&#39;,&#39;readmitted&#39;, &#39;metformin-rosiglitazone&#39;,&#39;metformin-pioglitazone&#39;,&#39;chlorpropamide&#39; , &#39;acetohexamide&#39; ,&#39;tolbutamide&#39;,&#39;acarbose&#39;,&#39;miglitol&#39;,&#39;troglitazone&#39;,&#39;tolazamide&#39; ,&#39;examide&#39;,&#39;citoglipton&#39; ,&#39;glyburide-metformin&#39;,&#39;glipizide-metformin&#39;,&#39;glimepiride-pioglitazone&#39;, &#39;A1Cresult&#39;, &#39;metformin&#39;, &#39;repaglinide&#39;, &#39;glimepiride&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;pioglitazone&#39; , &#39;rosiglitazone&#39;, &#39;insulin&#39;, &#39;change&#39;, &#39;diabetesMed&#39;,&#39;payer_code&#39;, &#39;max_glu_serum&#39;] sd=dfd.drop(dropList,axis=1) print(list(df1)) . [&#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, &#39;number_diagnoses&#39;, &#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;readmitted_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . # these ared just pure numerical num_col_list= [&#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;,&#39;number_diagnoses&#39;] . sd.groupby(&#39;diag_g1&#39;).size() sd.head(10).T . diag_g1 1 1824 10 3378 11 574 12 1563 13 8441 14 4736 16 1083 2 2633 27 17 3 7237 4 664 5 1484 6 827 7 19519 8 6293 9 5945 dtype: int64 . 0 1 2 3 4 5 6 7 8 9 . time_in_hospital 1 | 2 | 2 | 1 | 4 | 13 | 12 | 7 | 7 | 10 | . num_lab_procedures 41 | 11 | 44 | 51 | 70 | 68 | 33 | 62 | 60 | 55 | . num_procedures 0 | 5 | 1 | 0 | 1 | 2 | 3 | 0 | 0 | 1 | . num_medications 1 | 13 | 16 | 8 | 21 | 28 | 18 | 11 | 15 | 31 | . number_outpatient 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . number_emergency 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . number_inpatient 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . number_diagnoses 1 | 6 | 7 | 5 | 7 | 8 | 8 | 7 | 8 | 8 | . diag_g1 3 | 11 | 1 | 2 | 7 | 7 | 7 | 2 | 7 | 7 | . diag_g2 27 | 3 | 3 | 2 | 7 | 7 | 2 | 4 | 3 | 7 | . gender_code 2 | 2 | 1 | 1 | 1 | 2 | 2 | 1 | 2 | 1 | . race_code 3 | 1 | 3 | 3 | 3 | 3 | 3 | 1 | 3 | 3 | . age_code 1 | 3 | 4 | 5 | 7 | 9 | 10 | 7 | 5 | 9 | . disDispID_code 25 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 3 | 6 | . admSrsID_code 1 | 7 | 7 | 7 | 2 | 4 | 4 | 4 | 7 | 7 | . readmitted_code 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . metformin_code 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 2 | 1 | . repaglinide_code 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | . glimepiride_code 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 1 | 1 | . glipizide_code 1 | 2 | 1 | 2 | 1 | 2 | 1 | 1 | 1 | 1 | . glyburide_code 1 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 1 | . pioglitazone_code 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . rosiglitazone_code 1 | 1 | 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | . insulin_code 1 | 1 | 3 | 2 | 2 | 2 | 2 | 2 | 0 | 2 | . A1C_code 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . max_glu_serum_code 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . change_code 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | . diabMed_code 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . #lst=list(sd).remove(num_col_list) print(list(sd)) . [&#39;level_0&#39;, &#39;index&#39;, &#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, &#39;number_diagnoses&#39;, &#39;readm&#39;, &#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;medSpelt_code&#39;, &#39;readmitted_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . allv=list(sd) no_encodingL=[&#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39; , &#39;number_outpatient&#39;, &#39;number_emergency&#39;,&#39;number_inpatient&#39;, &#39;number_diagnoses&#39;,&#39;readmitted_code&#39;] col_to_encode=[x for x in allv if x not in no_encodingL] print(col_to_encode) . [&#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . df_encoded = pd.get_dummies(sd, columns=col_to_encode) df_encoded.head(11) . time_in_hospital num_lab_procedures num_procedures num_medications number_outpatient number_emergency number_inpatient number_diagnoses readmitted_code diag_g1_1 ... A1C_code_2 A1C_code_3 max_glu_serum_code_0 max_glu_serum_code_1 max_glu_serum_code_2 max_glu_serum_code_3 change_code_0 change_code_1 diabMed_code_0 diabMed_code_1 . 0 1 | 41 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | . 1 2 | 11 | 5 | 13 | 2 | 0 | 1 | 6 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 2 2 | 44 | 1 | 16 | 0 | 0 | 0 | 7 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 3 1 | 51 | 0 | 8 | 0 | 0 | 0 | 5 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 4 4 | 70 | 1 | 21 | 0 | 0 | 0 | 7 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 5 13 | 68 | 2 | 28 | 0 | 0 | 0 | 8 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 6 12 | 33 | 3 | 18 | 0 | 0 | 0 | 8 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 7 7 | 62 | 0 | 11 | 0 | 0 | 0 | 7 | 1 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 8 7 | 60 | 0 | 15 | 0 | 1 | 0 | 8 | 1 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 9 10 | 55 | 1 | 31 | 0 | 0 | 0 | 8 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 10 12 | 75 | 5 | 13 | 0 | 0 | 0 | 9 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 11 rows × 127 columns . df_encoded.shape . (66218, 127) . Split the data to train test . # Splitting into train and test # # first separate X, Y y=df_encoded[[&#39;readmitted_code&#39;]] X=df_encoded.drop(&#39;readmitted_code&#39;,axis=1) col2use=list(X) #split and count classes X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3 ) count_class_0, count_class_1 = y_train.readmitted_code.value_counts() print(count_class_0) print(count_class_1) . 38408 7944 . Sampling test portion of the data due to unbalanced dataset . Options available: TomekLinks : remove points that have different categories, but are nearest neghbours: it gives classification alhorithm most troubles . [] image As you can see in the above image, the Tomek Links (circled in green) are pairs of red and blue data points that are nearest neighbors. Intuitively, these are the points that will give most classification algorithms the most trouble. By removing these points, we widen the separation between the two classes, so our algorithms will be more “confident” in their outputs. . Sampling strategy &#39;auto&#39;: equivalent to &#39;not minority&#39; . #model = RandomUnderSampler(random_state=0) #from imblearn.under_sampling import CondensedNearestNeighbour -&gt; never finish #model = CondensedNearestNeighbour(random_state=0) #from imblearn.under_sampling import NearMiss #model= NearMiss() -&gt; 0.79 acuracy from imblearn.under_sampling import TomekLinks sampling_model=TomekLinks(sampling_strategy=&#39;auto&#39;) #from imblearn.under_sampling import OneSidedSelection #sampling_model = OneSidedSelection(random_state=0) X_sampled, y_sampled = sampling_model.fit_resample(X_train, y_train) . uniq, counts= np.unique(y_sampled, return_counts=True) dict(zip(uniq, counts)) # we see only 2016 removed !! # do we still need a beter sampling to improve learning? # . {0: 36388, 1: 7944} . from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report from sklearn.metrics import roc_curve, auc def print_report(name, y_actual, y_pred,y_test_pred_pb, thresh, labels=None, target_names=None): &#39;&#39;&#39; &#39;&#39;&#39; # because we have binary case tn, fp, fn, tp = confusion_matrix(y_actual, y_pred).ravel() specificity = tn / (tn+fp) auc_score = roc_auc_score(y_actual, y_pred) # accuracy = accuracy_score(y_actual, y_pred) recall = recall_score(y_actual, (y_pred &gt; thresh)) precision = precision_score(y_actual, (y_pred &gt; thresh)) f1= f1_score(y_actual, y_pred) # print(&#39;accuracy:%.3f&#39;%accuracy) # print(&#39;recall:%.3f&#39;%recall) print(&#39;F1:%.3f&#39;%f1) print(&#39;specificity:%.3f&#39;%specificity) print(&#39;AUC:%.3f&#39;%auc_score) conf_mat=confusion_matrix(y_actual, y_pred) print(&#39;Confusion matrix: n&#39;, conf_mat) with open(name + &quot;confusion_matrix.txt&quot;, &#39;w&#39;) as f: f.write(np.array2string(confusion_matrix(y_actual, y_pred), separator=&#39;, &#39;)) print(classification_report(y_actual, y_pred)) print(&#39; -&#39;) fpr_ar,tpr_ar, _ = roc_curve(y_actual, y_test_pred_pb) roc_auc = auc(fpr_ar,tpr_ar) plt.figure() plt.plot(fpr_ar, tpr_ar, color=&#39;red&#39;, lw=2, label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc) plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=2, linestyle=&#39;--&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate&#39;, fontsize=16) plt.ylabel(&#39;True Positive Rate&#39;, fontsize=16) plt.title(name.capitalize() +&#39; ROC Curve (area = %0.2f)&#39; % roc_auc,fontsize=18) plt.legend(loc=&quot;lower right&quot;) fileName=name + &quot;_roc_curve&quot; plt.savefig(fileName) plt.show() print(&#39;--&#39;) from sklearn.metrics import average_precision_score from sklearn.metrics import precision_recall_curve average_precision = average_precision_score(y_actual, y_pred) precision, recall, _ = precision_recall_curve(y_actual, y_pred) step_kwargs = ({&#39;step&#39;: &#39;post&#39;} ) plt.step(recall, precision, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;) plt.fill_between(recall, precision, alpha=0.2, color=&#39;b&#39;, **step_kwargs) plt.xlabel(&#39;Recall&#39;,fontsize=16) plt.ylabel(&#39;Precision&#39;,fontsize=16) plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title(&#39;Precision-Recall curve: AP={0:0.2f}&#39;.format( average_precision), fontsize=20) fileName=name + &quot;_prec-recall&quot; plt.savefig(fileName) plt.show() return dict(zip([&#39;auc&#39;, &#39;accuracy&#39;,&#39;f1&#39;, &#39;recall&#39;, &#39;precision&#39;, &#39;specificity&#39;,&#39;pred_pb&#39;],[auc_score, accuracy,f1, recall, precision, specificity,y_test_pred_pb])) models_results= {} . def eval_model(name, model, X_train,y_train, X_test,y_test, treshold=0.5): print (str(name) +&quot; &quot; + &#39;=&#39; * 20 ) model.fit( X_train,y_train ) y_pred=model.predict(X_test) y_train_pred_pb = model.predict_proba(X_sampled)[:,1] y_test_pred_pb = model.predict_proba(X_test)[:,1] model_info = print_report(name, y_test,y_pred,y_test_pred_pb, thresh=treshold) return model_info . from sklearn.linear_model import LogisticRegression # different kernels !!! lr = LogisticRegression(solver=&#39;saga&#39;, penalty=&#39;l1&#39;) # solver= &#39;liblinear&#39;, &#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;sag&#39;, &#39;saga models_results[&#39;logistic&#39;]=eval_model(name=&#39;Logistic&#39;, model=lr, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) #f_imps = pd.DataFrame(lr_l1.feature_importances_,index = X_train.columns,columns=[&#39;importance&#39;]).sort_values(&#39;importance&#39;, ascending=False) #f_imps.head(10) . Logistic ==================== F1:0.205 specificity:0.982 AUC:0.553 Confusion matrix: [[16159 294] [ 2989 424]] precision recall f1-score support 0 0.84 0.98 0.91 16453 1 0.59 0.12 0.21 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.72 0.55 0.56 19866 weighted avg 0.80 0.83 0.79 19866 - . -- . from sklearn.neighbors import KNeighborsClassifier knn=KNeighborsClassifier(n_neighbors = 100) models_results[&#39;knn&#39;]=eval_model(name=&#39;KNN&#39;, model=knn, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) . KNN ==================== F1:0.047 specificity:0.999 AUC:0.511 Confusion matrix: [[16432 21] [ 3331 82]] precision recall f1-score support 0 0.83 1.00 0.91 16453 1 0.80 0.02 0.05 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.81 0.51 0.48 19866 weighted avg 0.83 0.83 0.76 19866 - . -- . from sklearn import ensemble rfc = ensemble.RandomForestClassifier() models_results[&#39;rfc&#39;]=eval_model(name=&#39;RFC&#39;, model=rfc, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) . RFC ==================== specificity:0.977 AUC:0.544 Confusion matrix: [[16071 382] [ 3032 381]] precision recall f1-score support 0 0.84 0.98 0.90 16453 1 0.50 0.11 0.18 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.67 0.54 0.54 19866 weighted avg 0.78 0.83 0.78 19866 - . -- . from sklearn.linear_model import SGDClassifier sgdc=SGDClassifier(loss = &#39;log&#39;,alpha = 0.1,random_state = 33) models_results[&#39;sgdc&#39;]=eval_model(name=&#39;SGDC&#39;, model=sgdc, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) . SGDC ==================== specificity:0.994 AUC:0.527 Confusion matrix: [[16359 94] [ 3211 202]] precision recall f1-score support 0 0.84 0.99 0.91 16453 1 0.68 0.06 0.11 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.76 0.53 0.51 19866 weighted avg 0.81 0.83 0.77 19866 - . -- . #dtc = DecisionTreeClassifier(max_depth = 10, random_state = 42) . # . for model_name, result in models_results.items(): print(model_name, result) for stat,value in result.items(): print(stat,value) . logistic {&#39;auc&#39;: 0.55318090014780585, &#39;accuracy&#39;: 0.8347427766032417, &#39;f1&#39;: 0.20527717259743405, &#39;recall&#39;: array([ 1. , 0.12423088, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.59052925, 1. ]), &#39;specificity&#39;: 0.98213091837354893, &#39;pred_pb&#39;: array([ 0.23068577, 0.16358035, 0.08422994, ..., 0.26934011, 0.08899061, 0.24752306])} auc 0.553180900148 accuracy 0.834742776603 f1 0.205277172597 recall [ 1. 0.12423088 0. ] precision [ 0.17180107 0.59052925 1. ] specificity 0.982130918374 pred_pb [ 0.23068577 0.16358035 0.08422994 ..., 0.26934011 0.08899061 0.24752306] knn {&#39;auc&#39;: 0.51137471039731408, &#39;accuracy&#39;: 0.83126950568811031, &#39;f1&#39;: 0.046643913538111488, &#39;recall&#39;: array([ 1. , 0.02402578, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.7961165 , 1. ]), &#39;specificity&#39;: 0.9987236370266821, &#39;pred_pb&#39;: array([ 0.25, 0.18, 0.16, ..., 0.14, 0.08, 0.19])} auc 0.511374710397 accuracy 0.831269505688 f1 0.0466439135381 recall [ 1. 0.02402578 0. ] precision [ 0.17180107 0.7961165 1. ] specificity 0.998723637027 pred_pb [ 0.25 0.18 0.16 ..., 0.14 0.08 0.19] rfc {&#39;auc&#39;: 0.54420717251774842, &#39;accuracy&#39;: 0.82814859559045606, &#39;f1&#39;: 0.18247126436781611, &#39;recall&#39;: array([ 1. , 0.111632, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.49934469, 1. ]), &#39;specificity&#39;: 0.97678234972345468, &#39;pred_pb&#39;: array([ 0.1, 0. , 0.3, ..., 0.5, 0. , 0.3])} auc 0.544207172518 accuracy 0.82814859559 f1 0.182471264368 recall [ 1. 0.111632 0. ] precision [ 0.17180107 0.49934469 1. ] specificity 0.976782349723 pred_pb [ 0.1 0. 0.3 ..., 0.5 0. 0.3] sgdc {&#39;auc&#39;: 0.52673611177273305, &#39;accuracy&#39;: 0.83363535689117085, &#39;f1&#39;: 0.10892423833917499, &#39;recall&#39;: array([ 1. , 0.05918547, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.68243243, 1. ]), &#39;specificity&#39;: 0.99428675621467211, &#39;pred_pb&#39;: array([ 0.18043704, 0.15336435, 0.13491804, ..., 0.21587527, 0.1406695 , 0.20489415])} auc 0.526736111773 accuracy 0.833635356891 f1 0.108924238339 recall [ 1. 0.05918547 0. ] precision [ 0.17180107 0.68243243 1. ] specificity 0.994286756215 pred_pb [ 0.18043704 0.15336435 0.13491804 ..., 0.21587527 0.1406695 0.20489415] . dfz=pd.DataFrame.from_dict(models_results,orient=&#39;index&#39;) dfz.head(4) . auc accuracy f1 recall precision specificity pred_pb . knn 0.511375 | 0.831270 | 0.046644 | [1.0, 0.0240257837679, 0.0] | [0.17180106715, 0.796116504854, 1.0] | 0.998724 | [0.25, 0.18, 0.16, 0.07, 0.25, 0.11, 0.07, 0.1... | . logistic 0.553181 | 0.834743 | 0.205277 | [1.0, 0.124230881922, 0.0] | [0.17180106715, 0.590529247911, 1.0] | 0.982131 | [0.230685768678, 0.163580352538, 0.08422994190... | . rfc 0.544207 | 0.828149 | 0.182471 | [1.0, 0.111631995312, 0.0] | [0.17180106715, 0.499344692005, 1.0] | 0.976782 | [0.1, 0.0, 0.3, 0.3, 0.6, 0.0, 0.0, 0.3, 0.1, ... | . sgdc 0.526736 | 0.833635 | 0.108924 | [1.0, 0.0591854673308, 0.0] | [0.17180106715, 0.682432432432, 1.0] | 0.994287 | [0.180437036084, 0.153364345572, 0.13491803579... | . import sklearn.metrics as metrics # calculate the fpr and tpr for all thresholds of the classification probs = model.predict_proba(X_test) preds = probs[:,1] fpr, tpr, threshold = metrics.roc_curve(y_test, preds) roc_auc = metrics.auc(fpr, tpr) # method I: plt import matplotlib.pyplot as plt plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() . model = LogisticRegression(solver=&#39;liblinear&#39;, penalty=&#39;l1&#39;) # solver=&#39;lbfgs&#39; model.fit( X_train,y_train ) y_pred=model.predict(X_test) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l1&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001, verbose=0, warm_start=False) . feature_importances = pd.DataFrame(model.coef_[0], index = col2use, columns=[&#39;importance&#39;]).sort_values(&#39;importance&#39;, ascending=False) . feature_importances.head(15) . importance . disDispID_code_22 0.864318 | . admSrsID_code_20 0.632377 | . diag_g1_27 0.599416 | . disDispID_code_5 0.504256 | . number_inpatient 0.442862 | . diag_g2_2 0.287582 | . diag_g1_16 0.273329 | . diag_g1_4 0.216486 | . number_emergency 0.206680 | . diag_g1_5 0.183197 | . diag_g1_3 0.176214 | . diag_g2_12 0.176034 | . admSrsID_code_3 0.163309 | . admSrsID_code_9 0.158770 | . max_glu_serum_code_3 0.154818 | . &lt;/div&gt; .",
            "url": "https://v2br.github.io/fastpages/2021/05/03/Diabetic-readmisson-analysis.html",
            "relUrl": "/2021/05/03/Diabetic-readmisson-analysis.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Std approach test 1",
            "content": "Numeric not ordered . Examples: . number of observations (different kind of) | number of voters per state | phone codes per state | . Sns high level overview https://seaborn.pydata.org/tutorial/function_overview.html . Relational data . math test $$ z = frac{(X - mu)} { sigma}$$ . sns . Distribution . Show comparion of Violin box and hist . import datetime, warnings, scipy import pandas as pd # import pandas_profiling # otional import numpy as np import seaborn as sns import matplotlib.pyplot as plt def plot_comparison(x, title): fig, ax = plt.subplots(4, 1, sharex=True, figsize=(12,8)) sns.distplot(x, ax=ax[0]) # histogram ax[0].set_title(&#39;Histogram + KDE; x value, y deach time different ; y count&#39;) sns.boxplot(x, ax=ax[1]) # Boxplot ax[1].set_title(&#39;Boxplot&#39;) sns.violinplot(x, ax=ax[2]) # Violin ax[2].set_title(&#39;Violin plot&#39;) fig.suptitle(title, fontsize=16) sns.ecdfplot(data=x,ax=ax[3], stat=&#39;proportion&#39;) ax[3].set_title(&#39;ECDF plot&#39;) fig.suptitle(title, fontsize=16) plt.show() # ECDF po . N = 10 ** 4 np.random.seed(42) sample_gaussian = np.random.normal(size=N) plot_comparison(sample_gaussian, &#39;Standard Normal Distribution&#39;) . /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . sample_bimodal = np.concatenate([np.random.normal(loc=-2, scale=2, size=int(N/2)), np.random.normal(loc=3, scale=1, size=int(N/2))]) plot_comparison(sample_bimodal, &#39;Mixture of Gaussians - bimodal&#39;) . /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . sns.displot(data=sample_bimodal, kind=&#39;hist&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fa377bddc50&gt; . . 6 7 8 9 . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ... 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 . 0 6.295826 | 5.196595 | 3.306808 | 10.368467 | -3.895729 | -2.870517 | 6.467311 | 5.812862 | -4.856069 | -7.424335 | 11.160699 | -1.849915 | 4.697823 | 0.129141 | 4.293983 | 8.408799 | -6.765013 | 9.934909 | 8.405769 | 1.211940 | -5.789181 | 6.267531 | -2.586304 | -2.983917 | -0.354704 | -6.687628 | -7.361987 | -8.460529 | 7.865163 | -3.104652 | -0.099949 | 2.815034 | 4.660669 | -11.308186 | -0.765524 | -3.115905 | 4.410859 | 8.081108 | 3.301530 | -1.374307 | ... | -3.619317 | 9.681318 | -1.523904 | -9.095273 | -0.703359 | 2.972570 | -4.215545 | 2.370417 | -6.739674 | -10.293329 | -1.626631 | 0.517444 | -6.542827 | 0.675258 | -5.721867 | 0.301339 | -2.898964 | 8.003420 | 2.881128 | -4.498360 | -7.924659 | 1.993390 | -1.599650 | 3.817212 | -6.709891 | 1.587271 | 0.558356 | -7.177189 | -5.393567 | -0.272380 | -3.583005 | 0.093715 | -3.643103 | 4.328405 | 2.785666 | 4.959357 | 0.558072 | -13.573699 | 5.944342 | -7.642895 | . 1 -9.714159 | 1.086545 | 0.515707 | 7.204036 | -4.622671 | -4.332094 | -1.326759 | -3.822245 | 11.380635 | -0.717820 | -10.010028 | 6.127681 | -1.062175 | -2.589784 | 0.097885 | -0.571676 | 7.647341 | 3.694478 | 12.191742 | -3.824271 | 11.601576 | 4.715410 | -0.593936 | -11.566830 | 4.504946 | -6.400303 | 2.449148 | 0.686613 | 6.440932 | 2.913737 | 4.818091 | -0.228964 | 3.797812 | -0.041215 | -9.532216 | -2.222159 | -16.287351 | 6.156880 | -4.801712 | -8.359627 | ... | -13.417744 | 1.140994 | 6.443059 | -4.670928 | -0.326633 | -4.296397 | -2.624538 | 3.618586 | 3.772401 | 0.010675 | -0.754504 | 4.832428 | 10.365460 | 6.338193 | 11.410977 | 0.371137 | -11.745067 | -2.142325 | -1.130717 | 7.892386 | -0.194661 | 0.203075 | -5.872184 | -6.983265 | 1.377974 | -10.969161 | 0.537690 | 1.606457 | 8.151308 | 0.094924 | -7.921901 | -4.889639 | -8.983292 | -7.240465 | 6.011052 | -7.900238 | 2.975677 | 7.112989 | -21.970376 | -3.444214 | . 2 12.881220 | 3.791821 | -18.492249 | -3.625449 | -6.653355 | -2.105036 | -2.026996 | 0.837128 | -2.707500 | -9.184221 | -4.221775 | -0.966485 | 1.394287 | -5.111171 | -0.680627 | 13.015172 | 5.836066 | 3.098947 | 1.088597 | 1.268748 | -0.698917 | -5.849370 | 9.973692 | 5.698744 | -9.380506 | 3.511613 | -4.205585 | 3.617175 | 6.635580 | 16.823379 | 4.088321 | 17.128237 | -4.905860 | 16.448438 | -0.195672 | 13.463014 | 8.870895 | 7.642462 | -3.530440 | -3.703156 | ... | 4.024584 | -10.839706 | 9.841249 | -8.117942 | 1.796838 | -1.232860 | -16.100428 | 13.297466 | 7.578057 | -20.601347 | 3.159366 | -1.942942 | 5.944509 | 0.172885 | 14.570568 | -13.103542 | 0.759154 | -1.386811 | 4.441340 | 3.385841 | -6.478977 | -10.035242 | 6.252055 | -8.944769 | -6.801974 | 1.898800 | 1.656156 | 0.287739 | -2.049426 | -4.652481 | 0.257406 | -2.018110 | -3.441745 | 4.455858 | 4.824423 | -3.347953 | 13.122325 | 6.851431 | 4.041598 | 0.668767 | . 3 2.506632 | -0.630992 | -0.326357 | -20.307119 | -13.114656 | -3.950933 | -0.192611 | -8.226074 | 9.422215 | 2.671719 | -9.344425 | -2.047111 | 8.138812 | -13.317688 | -3.960878 | 14.459736 | -9.252112 | -8.698798 | 6.682870 | 14.940897 | -16.902391 | 1.198006 | 1.341801 | 7.713871 | 5.998581 | 1.827239 | -2.591409 | 10.868199 | -2.469000 | 10.905127 | 12.797560 | -6.313015 | -0.875337 | -1.131700 | 8.143948 | 6.711376 | -17.220705 | -10.830476 | 3.727351 | -12.850858 | ... | -1.503009 | 7.625294 | -1.850666 | 10.174138 | -4.920469 | -1.604776 | -0.373441 | -5.598822 | -0.743738 | 0.580005 | 4.718239 | 13.912782 | -6.239996 | -0.680113 | 15.312143 | -7.402881 | -8.150332 | 0.613902 | -4.561078 | 22.727851 | -3.097166 | 2.282456 | 9.797883 | -5.287433 | 10.787993 | -8.496429 | -14.184848 | 6.592398 | -7.972791 | -14.004307 | -1.398185 | 6.866293 | -15.636616 | -9.132841 | 2.753904 | 6.927100 | 17.219411 | 0.230163 | -0.870910 | -7.030377 | . 4 rows × 100 columns . fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4)) # plot violin plot axes[0].violinplot(ddf, showmeans=False, showmedians=True) axes[0].set_title(&#39;Violin plot&#39;) # plot box plot axes[1].boxplot(ddf) axes[1].set_title(&#39;Box plot&#39;) # adding horizontal grid lines for ax in axes: ax.yaxis.grid(True) ax.set_xticks([y + 1 for y in range(len(all_data))]) ax.set_xlabel(&#39;Four separate samples&#39;) ax.set_ylabel(&#39;Observed values&#39;) # add x-tick labels plt.setp(axes, xticks=[y + 1 for y in range(len(all_data))], xticklabels=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]) plt.show() . . penguins= sns.load_dataset(&#39;penguins&#39;) . f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 3])) sns.scatterplot(data=penguins, x=&quot;flipper_length_mm&quot;, y=&quot;bill_length_mm&quot;, hue=&quot;species&quot;, ax=axs[0]) sns.histplot(data=penguins, x=&quot;species&quot;, hue=&quot;species&quot;, shrink=.8, alpha=.8, legend=False, ax=axs[1]) f.tight_layout() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa377ac39e8&gt; . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa377a74d68&gt; . import seaborn as sns import matplotlib.pyplot as plt df = sns.load_dataset(&#39;iris&#39;) # plot of 2 variables p1=sns.kdeplot(df[&#39;sepal_width&#39;], shade=True, color=&quot;r&quot;) p1=sns.kdeplot(df[&#39;sepal_length&#39;], shade=True, color=&quot;b&quot;) plt.show() . Numeric Ordered . Line . df = pd.DataFrame(dict(time=np.arange(500), value=np.random.randn(500).cumsum())) g = sns.relplot(x=&quot;time&quot;, y=&quot;value&quot;, kind=&quot;line&quot;, data=df) g.fig.autofmt_xdate() . fmri = sns.load_dataset(&quot;fmri&quot;) fmri.head(3) . subject timepoint event region signal . 0 s13 | 18 | stim | parietal | -0.017552 | . 1 s5 | 14 | stim | parietal | -0.080883 | . 2 s12 | 18 | stim | parietal | -0.081033 | . sns.relplot(x=&quot;timepoint&quot;, y=&quot;signal&quot;, kind=&quot;line&quot;, data=fmri); . plt.plot([1, 2, 3, 4], [1, 4, 9, 16], &#39;ro&#39;) plt.axis([0, 6, 0, 20]) plt.show() . sns.relplot([1, 2, 3, 4], [1, 4, 9, 16]) plt.axis([0, 6, 0, 20]) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Scatter . sns.relplot(x=&quot;timepoint&quot;, y=&quot;signal&quot;, kind=&quot;scatter&quot;, data=fmri); . import numpy as np l = [x for x in range (1,9 )] x = arr = np.array(x) . import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(x, x, color=&#39;green&#39;) ax.plot(x, x * x, color=&#39;red&#39;) ax.plot(x, x**3, color=&#39;blue&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f7a53ff4978&gt;] . fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True) titles = [&#39;Linear&#39;, &#39;Squared&#39;, &#39;Cubic&#39;, &#39;Quartic&#39;] y_vals = [x, x * x, x**3, x**4] # axes.flat returns the set of axes as a flat (1D) array instead # of the two-dimensional version we used earlier for ax, title, y in zip(axes.flat, titles, y_vals): ax.plot(x, y) ax.set_title(title) ax.grid(True) . Area . import numpy as np import matplotlib.pyplot as plt import seaborn as sns # Data x=range(1,6) y=[ [1,4,6,8,9], [2,2,7,10,12], [2,8,5,10,6] ] # Plot Area plt.stackplot(x,y, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]) plt.legend(loc=&#39;upper left&#39;) plt.show() . tips=sns.load_dataset(&quot;tips&quot;) sns.relplot(x=&quot;tip&quot;,y= data=tips); . tips = sns.load_dataset(&quot;tips&quot;) sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips); . Categorical . Here we will show several categories and values for them . Violin and Boxplot for category (x) . Together with simple histogram good first impression . import matplotlib.pyplot as plt import numpy as np # Fixing random state for reproducibility np.random.seed(19680801) # generate some random test data all_data = [np.random.normal(0, std, 100) for std in range(6, 10)] for std in range(6, 10): print(std) ddf=pd.DataFrame.from_dict(all_data) ddf.head(5) . 6 7 8 9 . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ... 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 . 0 6.295826 | 5.196595 | 3.306808 | 10.368467 | -3.895729 | -2.870517 | 6.467311 | 5.812862 | -4.856069 | -7.424335 | 11.160699 | -1.849915 | 4.697823 | 0.129141 | 4.293983 | 8.408799 | -6.765013 | 9.934909 | 8.405769 | 1.211940 | -5.789181 | 6.267531 | -2.586304 | -2.983917 | -0.354704 | -6.687628 | -7.361987 | -8.460529 | 7.865163 | -3.104652 | -0.099949 | 2.815034 | 4.660669 | -11.308186 | -0.765524 | -3.115905 | 4.410859 | 8.081108 | 3.301530 | -1.374307 | ... | -3.619317 | 9.681318 | -1.523904 | -9.095273 | -0.703359 | 2.972570 | -4.215545 | 2.370417 | -6.739674 | -10.293329 | -1.626631 | 0.517444 | -6.542827 | 0.675258 | -5.721867 | 0.301339 | -2.898964 | 8.003420 | 2.881128 | -4.498360 | -7.924659 | 1.993390 | -1.599650 | 3.817212 | -6.709891 | 1.587271 | 0.558356 | -7.177189 | -5.393567 | -0.272380 | -3.583005 | 0.093715 | -3.643103 | 4.328405 | 2.785666 | 4.959357 | 0.558072 | -13.573699 | 5.944342 | -7.642895 | . 1 -9.714159 | 1.086545 | 0.515707 | 7.204036 | -4.622671 | -4.332094 | -1.326759 | -3.822245 | 11.380635 | -0.717820 | -10.010028 | 6.127681 | -1.062175 | -2.589784 | 0.097885 | -0.571676 | 7.647341 | 3.694478 | 12.191742 | -3.824271 | 11.601576 | 4.715410 | -0.593936 | -11.566830 | 4.504946 | -6.400303 | 2.449148 | 0.686613 | 6.440932 | 2.913737 | 4.818091 | -0.228964 | 3.797812 | -0.041215 | -9.532216 | -2.222159 | -16.287351 | 6.156880 | -4.801712 | -8.359627 | ... | -13.417744 | 1.140994 | 6.443059 | -4.670928 | -0.326633 | -4.296397 | -2.624538 | 3.618586 | 3.772401 | 0.010675 | -0.754504 | 4.832428 | 10.365460 | 6.338193 | 11.410977 | 0.371137 | -11.745067 | -2.142325 | -1.130717 | 7.892386 | -0.194661 | 0.203075 | -5.872184 | -6.983265 | 1.377974 | -10.969161 | 0.537690 | 1.606457 | 8.151308 | 0.094924 | -7.921901 | -4.889639 | -8.983292 | -7.240465 | 6.011052 | -7.900238 | 2.975677 | 7.112989 | -21.970376 | -3.444214 | . 2 12.881220 | 3.791821 | -18.492249 | -3.625449 | -6.653355 | -2.105036 | -2.026996 | 0.837128 | -2.707500 | -9.184221 | -4.221775 | -0.966485 | 1.394287 | -5.111171 | -0.680627 | 13.015172 | 5.836066 | 3.098947 | 1.088597 | 1.268748 | -0.698917 | -5.849370 | 9.973692 | 5.698744 | -9.380506 | 3.511613 | -4.205585 | 3.617175 | 6.635580 | 16.823379 | 4.088321 | 17.128237 | -4.905860 | 16.448438 | -0.195672 | 13.463014 | 8.870895 | 7.642462 | -3.530440 | -3.703156 | ... | 4.024584 | -10.839706 | 9.841249 | -8.117942 | 1.796838 | -1.232860 | -16.100428 | 13.297466 | 7.578057 | -20.601347 | 3.159366 | -1.942942 | 5.944509 | 0.172885 | 14.570568 | -13.103542 | 0.759154 | -1.386811 | 4.441340 | 3.385841 | -6.478977 | -10.035242 | 6.252055 | -8.944769 | -6.801974 | 1.898800 | 1.656156 | 0.287739 | -2.049426 | -4.652481 | 0.257406 | -2.018110 | -3.441745 | 4.455858 | 4.824423 | -3.347953 | 13.122325 | 6.851431 | 4.041598 | 0.668767 | . 3 2.506632 | -0.630992 | -0.326357 | -20.307119 | -13.114656 | -3.950933 | -0.192611 | -8.226074 | 9.422215 | 2.671719 | -9.344425 | -2.047111 | 8.138812 | -13.317688 | -3.960878 | 14.459736 | -9.252112 | -8.698798 | 6.682870 | 14.940897 | -16.902391 | 1.198006 | 1.341801 | 7.713871 | 5.998581 | 1.827239 | -2.591409 | 10.868199 | -2.469000 | 10.905127 | 12.797560 | -6.313015 | -0.875337 | -1.131700 | 8.143948 | 6.711376 | -17.220705 | -10.830476 | 3.727351 | -12.850858 | ... | -1.503009 | 7.625294 | -1.850666 | 10.174138 | -4.920469 | -1.604776 | -0.373441 | -5.598822 | -0.743738 | 0.580005 | 4.718239 | 13.912782 | -6.239996 | -0.680113 | 15.312143 | -7.402881 | -8.150332 | 0.613902 | -4.561078 | 22.727851 | -3.097166 | 2.282456 | 9.797883 | -5.287433 | 10.787993 | -8.496429 | -14.184848 | 6.592398 | -7.972791 | -14.004307 | -1.398185 | 6.866293 | -15.636616 | -9.132841 | 2.753904 | 6.927100 | 17.219411 | 0.230163 | -0.870910 | -7.030377 | . 4 rows × 100 columns . fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4)) # plot violin plot axes[0].violinplot(ddf, showmeans=False, showmedians=True) axes[0].set_title(&#39;Violin plot&#39;) # plot box plot axes[1].boxplot(ddf) axes[1].set_title(&#39;Box plot&#39;) # adding horizontal grid lines for ax in axes: ax.yaxis.grid(True) ax.set_xticks([y + 1 for y in range(len(all_data))]) ax.set_xlabel(&#39;Four separate samples&#39;) ax.set_ylabel(&#39;Observed values&#39;) # add x-tick labels plt.setp(axes, xticks=[y + 1 for y in range(len(all_data))], xticklabels=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]) plt.show() . ax = sns.violinplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;sex&quot;, data=tips) ax.set_title(&#39;Distribution of total bill amount per day&#39;, fontsize=16); . ax = sns.violinplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;sex&quot;, split=True, data=tips) ax.set_title(&#39;Distribution of total bill amount per day&#39;, fontsize=16); . ECDF . other view of distribution . fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True) titles = [&#39;Linear&#39;, &#39;Squared&#39;, &#39;Cubic&#39;, &#39;Quartic&#39;] x = [1,3, 5, 7] y_vals = [x, x * x, x**3, x**4] # axes.flat returns the set of axes as a flat (1D) array instead # of the two-dimensional version we used earlier for ax, title, y in zip(axes.flat, titles, y_vals): ax.plot(x, y) ax.set_title(title) ax.grid(True) . TypeError Traceback (most recent call last) &lt;ipython-input-34-10d8f80b8d47&gt; in &lt;module&gt;() 3 titles = [&#39;Linear&#39;, &#39;Squared&#39;, &#39;Cubic&#39;, &#39;Quartic&#39;] 4 x = [1,3, 5, 7] -&gt; 5 y_vals = [x, x * x, x**3, x**4] 6 7 # axes.flat returns the set of axes as a flat (1D) array instead TypeError: can&#39;t multiply sequence by non-int of type &#39;list&#39; . import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt f, axes = plt.subplots(2, 10, sharex=True, gridspec_kw={&quot;height_ratios&quot;:(.35, .35)}, figsize = (12, 5)) df = pd.DataFrame(np.random.randint(0,100,size=(100, 10)),columns=list(&#39;ABCDEFGHIJ&#39;)) for i, c in enumerate(df): sns.boxplot(df[c], ax=axes[0,i]) axes[1,i].hist(df[c]) plt.tight_layout() plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . sns.barplot(x=&quot;gdp_per_capita&quot;, y=&quot;country&quot;, data=df.head(), orient=&#39;h&#39;) #Set theme, then plot violin plotPython # Set theme sns.set_style(&#39;whitegrid&#39;) # Violin plot sns.violinplot(x=&#39;Type 1&#39;, y=&#39;Attack&#39;, data=df) . ValueError Traceback (most recent call last) &lt;ipython-input-3-da3cd92fe93f&gt; in &lt;module&gt;() -&gt; 1 sns.barplot(x=&#34;gdp_per_capita&#34;, y=&#34;country&#34;, data=df.head(), orient=&#39;h&#39;) 2 #Set theme, then plot violin plotPython 3 # Set theme 4 sns.set_style(&#39;whitegrid&#39;) 5 /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py in inner_f(*args, **kwargs) 44 ) 45 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}) &gt; 46 return f(**kwargs) 47 return inner_f 48 /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py in barplot(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs) 3170 estimator, ci, n_boot, units, seed, 3171 orient, color, palette, saturation, -&gt; 3172 errcolor, errwidth, capsize, dodge) 3173 3174 if ax is None: /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py in __init__(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge) 1583 &#34;&#34;&#34;Initialize the plotter.&#34;&#34;&#34; 1584 self.establish_variables(x, y, hue, data, orient, -&gt; 1585 order, hue_order, units) 1586 self.establish_colors(color, palette, saturation) 1587 self.estimate_statistic(estimator, ci, n_boot, seed) /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py in establish_variables(self, x, y, hue, data, orient, order, hue_order, units) 151 if isinstance(var, str): 152 err = &#34;Could not interpret input &#39;{}&#39;&#34;.format(var) --&gt; 153 raise ValueError(err) 154 155 # Figure out the plotting orientation ValueError: Could not interpret input &#39;gdp_per_capita&#39; . import matplotlib.pyplot as plt import seaborn as sns p = sns.lineplot(&#39;week&#39;, &#39;mean&#39;, data = patient) p.fill_between(&#39;week&#39;, y1 = &#39;min&#39;, y2 = &#39;max&#39;, data = patient, alpha = 0.2) plt.show() . plt.plot([1, 2, 3, 4], [1, 4, 9, 16], &#39;ro&#39;) plt.axis([0, 6, 0, 20]) plt.show() . plt.relplot([1, 2, 3, 4], [1, 4, 9, 16], &#39;ro&#39;) plt.axis([0, 6, 0, 20]) plt.show() . AttributeError Traceback (most recent call last) &lt;ipython-input-44-d284f8a88b48&gt; in &lt;module&gt;() -&gt; 1 plt.relplot([1, 2, 3, 4], [1, 4, 9, 16], &#39;ro&#39;) 2 plt.axis([0, 6, 0, 20]) 3 plt.show() AttributeError: module &#39;matplotlib.pyplot&#39; has no attribute &#39;relplot&#39; .",
            "url": "https://v2br.github.io/fastpages/visuals/jupyter/2020/11/01/Std_sns_visuals.html",
            "relUrl": "/visuals/jupyter/2020/11/01/Std_sns_visuals.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Dataframes and pandas",
            "content": "Approaching the data . The first step is to understand your data source and size. . are you dealing with the static data? Example of static is a file somewhere. Dinamic could be a database table or even stream. . Let’s start from the simplest example static datafile. Most common way to do it is to use pandas: - labeled, indexed two dimentional data structure. . ’’’ import pandas as pd . Efficient approach to readind data: ‘’’ python import subprocess as sb print (sb.check_output([“ls”, “-l”, “../”]).decode(“utf8”)) ‘’’ . ’’’ python first read a few lines: . ’’’ . #——————— do something 8/h a day with results . project . Talk to recruiter #——————- ~ 2 projects volunteering 15 / 20 expectation! ———————————— !- make it nice and publish data manipulation, #———————– StackOverflow #——————————– week! have something top show! SQL questions, test #—————————– Maintain mental health . very activities : . talk to people . #—————#- art ! — # . srochaya . drriving . #———– covid jobs !!! the best thing if i can do it #—————-&gt;– volunteering/ project ~ 3 weeks . —&gt; #——————- COVID JOBS req: -&gt; vacine -&gt; #— . Feature encoding | Feature engineering | Feature selection | Model building | Model comparison | Model evaluation | Final training and performance evaluation | . #———— . my site linked in . . talk to somebody . | . | . #——————— .",
            "url": "https://v2br.github.io/fastpages/notes/2020/09/28/dataframes.html",
            "relUrl": "/notes/2020/09/28/dataframes.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Docker for development on wsl2",
            "content": "Notes on docker use on development wsl2 system. . I decided to use docker for a jekyll project. The reason is with all upgrades and gems possible conflicts management of the environment becomes problematic. Moving project to another environment might be not that straight forward. . Containerization approach. With docker container the whole project becomes easy manageable and reproducible. Plus if you have any conflicts during installation it is much easier to analyze the problem and resolve the conflict. . My requirements to the solution: . Editing project files should not change. | Version control should not be affected. | The whole setup should be reproducible and automated as much as possible. | Jekyll should generate site and run it on local system port 400000. | The first two reqrements cold be satisfied using docker option “volume”. It allows to map local host directories to virtual directories. Docker volume is aka rsync in Linux. The third one is the key feature of the docker itself. The last could be done using docker parameter “port”. It will map local host port to the application port. . One additional configuration step is necessary due to default docker user. It is root. This is not optimal. According to best practices application will run on application user account only with necessary privileges. . To accomplish this portable way we can place whole configuration into docker file . Dcokerfile . FROM jekyll/jekyll RUN addgroup jekyll &amp;&amp; adduser –ingroup jekyll –disabled-password –gecos ‘’ jekyll &amp;&amp; chown jekyll:jekyll /home/jekyll USER jekyll RUN mkdir -p /home/jekyll/site &amp;&amp; chown -R jekyll:jekyll /home/jekyll/site VOLUME /home/jekyll/site WORKDIR /home/jekyll/site EXPOSE 4000 RUN apk add py-pip RUN pip CMD bundle install &amp;&amp; jekyll serve –host=0.0.0.0 –force_polling –watch . Let’s look into the details: . Use the latest jekyll image from dockerhub | Configure creation of docker container with the following arguments volumes | . | in my case the critical part happened to be gem installation .",
            "url": "https://v2br.github.io/fastpages/portfolio/",
            "relUrl": "/portfolio/",
            "date": " • Aug 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Gradient Boosting",
            "content": "import pandas as pd import numpy as np import scipy import matplotlib.pyplot as plt from sklearn import tree from IPython.display import Image import pydotplus import graphviz %matplotlib inline . Gradient boosting . You may recall that we last encountered gradients when discussing the gradient descent algorithm in the context of fitting linear regression models. We said that for a particular regression model with n parameters, an n+1 dimensional space existed defined by all the parameters plus the cost/loss function to minimize. The combination of parameters and loss function define a surface within the space. The regression model is fitted by moving down the steepest ‘downhill’ gradient until we reach the lowest point of the surface, where all possible gradients are ‘uphill.’ The final model is made up of the parameter estimates that define that location on the surface. . Throughout all iterations of the gradient descent algorithm for linear regression, one thing remains constant: The underlying data used to estimate the parameters and calculate the loss function never changes. In gradient boosting, however, the underlying data do change. Let’s work it out: . Gradient boosting can work on any combination of loss function and model type, as long as we can calculate the derivatives of the loss function with respect to the model parameters. Most often, however, gradient boosting uses decision trees, and minimizes either the residual (regression trees) or the negative log-likelihood (classification trees). . Gradient boosting with regression trees . The loss function to minimize is the sum of the squared residuals: . 1n∑i=1n(yi−(α+βxi))2 frac1{n} sum_{i=1}^n(y_i-( alpha + beta x_i))^2n1​i=1∑n​(yi​−(α+βxi​))2 . (Though it can also be the sum of the absolute value of the residuals, as in lasso regression.) . Each time we run a decision tree, we extract the residuals. Then we run a new decision tree, using those residuals as the outcome to be predicted. After reaching a stopping point, we add together the predicted values from all of the decision trees to create the final gradient boosted prediction. . The decision trees we use can be very simple. In the example below, the decision trees only have a max depth of 2, meaning a maximum of four leaves. These weak learners do not have to perform well at all individually in order to do well in aggregate. . Here we’re going to do gradient boosting with regression trees by hand. Our goal is to predict the variable “happy” using all the other variables in the European Social Survey dataset. We’ll calculate a tree, store the predicted values, pull the residuals, and run a new tree on the residuals. This will repeat 101 times. At the end, we add together all the predicted values from each iteration to yield the final predictions. . # Working with the European Social Survey data again. df = pd.read_csv(( &quot;https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/&quot; &quot;master/ESS_practice_data/ESSdata_Thinkful.csv&quot;)).dropna() # Define outcome and predictors. y = df[&#39;happy&#39;] X = df.loc[:, ~df.columns.isin([&#39;happy&#39;, &#39;cntry&#39;])] # Make the categorical variable &#39;country&#39; into dummies. X = pd.concat([X, pd.get_dummies(df[&#39;cntry&#39;])], axis=1) # Store values from loops. preds = pd.DataFrame() labels = [] uniquex = df[&#39;happy&#39;].unique() # Iterate through decision trees, each time using the residuals # from the previous tree as the inputs. for m in range(0, 101): # Initialize and fit the tree. Set the max depth to 2. decision_tree = tree.DecisionTreeRegressor(max_depth=2) decision_tree.fit(X,y) # Get and store predicted values. pred = decision_tree.predict(X) preds[&#39;pred{}&#39;.format(m)] = pred # Residuals. y = y - pred # Output every 20 iterations. if m % 20 == 0: print(&#39;Weak learner {} R^2: {}&#39;.format(m, decision_tree.score(X, y))) labels = labels + [m] bestpred = preds.sum(axis=1) plt.plot(uniquex, np.poly1d(np.polyfit(df[&#39;happy&#39;], bestpred, 1))(uniquex)) plt.legend(labels) plt.xlabel(&#39;Original&#39;) plt.ylabel(&#39;Predicted&#39;) plt.show() predicted = preds.sum(axis=1) print(&#39;Overall R^2:&#39;) print(np.corrcoef(df[&#39;happy&#39;], predicted)[0, 1]) . Weak learner 0 R^2: -21.86595826514709 Weak learner 20 R^2: -0.0026192923008196978 Weak learner 40 R^2: -0.0016748172727343302 Weak learner 60 R^2: -0.0014818633549413818 Weak learner 80 R^2: -0.0012578370480087475 Weak learner 100 R^2: -0.0007136861925911564 . . Overall R^2: 0.528287358179 . Looking at the sampling of individual model $R^2$, these are certainly ‘weak learners’! They perform very badly indeed. Yet in aggregate, the models are able to explain nearly 53% of the variance in happiness in the dataset. This is a great example of the power of gradient boosting. You may also note from the plot that the greatest gains happened in the earliest models, with subsequent steps yielding smaller gains in explanatory power. . But maybe we could do just as well with one big decision tree, rather than a bunch of small ones. Let’s try: . # One big tree. Each node split will include at lest 2.5% of the total sample. decision_tree = tree.DecisionTreeRegressor(min_samples_split=.025) decision_tree.fit(X,y) #Get and store predicted values. pred = decision_tree.predict(X) print(&#39;One strongish learner R^2: {}&#39;.format(decision_tree.score(X, y))) dot_data = tree.export_graphviz(decision_tree, out_file=None, feature_names=X.columns, filled=True) graph = pydotplus.graph_from_dot_data(dot_data) Image(graph.create_png()) . One strongish learner R^2: 0.06711108322936521 . . Well that didn’t work at all. Gradient boosting with weak learners was nearly 10 times more effective as one complex decision tree. In fact, in many instances gradient boosted decision trees perform better than random forests. They are also less prone to overfitting than individual decision trees. . Overfitting . Still, it is easy to see that the more iterations we run, the more likely we are to overfit. Gradient boost comes with some methods to avoid overfitting. Cross-validation will check for overfitting, but there are also methods that can be applied before using the test set that will reduce the likelihood of overfit. . One option is subsampling, where each iteration of the boost algorithm uses a subsample of the original data. By introducing some randomness into the process, subsampling makes it harder to overfit. . Another option is shrinkage, which we have encountered before in ridge regression. Here, the shrinkage/regularization parameter reduces the impact of subsequent iterations on the final solution. Visually, you can picture this parameter, called the “learning rate”, causing each “step” along the loss function gradient to be a little smaller than the previous one. This prevents any one iteration from being too influential and misdirecting the overall boost solution. Learning rates vary between 0 (only the initial iteration matters) to 1 (all iterations are weighted equally). A model made up of many small steps is less prone to overfitting than a model made up of few large steps, but it can also lead to much slower running times, depending on the stopping rule in play. . In the next assignment, we’ll try gradient boosting using the SKLearn package, which comes with all sorts of helpful features for getting the most out of gradient boost. .",
            "url": "https://v2br.github.io/fastpages/gradient%20boost/2020/02/28/gradient-boost.html",
            "relUrl": "/gradient%20boost/2020/02/28/gradient-boost.html",
            "date": " • Feb 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://v2br.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Extra",
          "content": "? notes or something .",
          "url": "https://v2br.github.io/fastpages/extra/",
          "relUrl": "/extra/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://v2br.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}