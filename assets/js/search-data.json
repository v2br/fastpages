{
  
    
        "post0": {
            "title": "Analysis of readmissions diabetic patiens",
            "content": "Introduction . Many hospital patients are readmitted within 30 days after discharge. Often it indicates problems with the quality of hospital care. Recently this parameter became part of the National Hospital Evaluation metric. In 2014 researches of Virginia Commonwealth University used resources of Health Facts database from Center corp (major medical data hub) and NIH grant to assemble the dataset to study readmissions especially for diabetic patients. They wanted to analyze if proper management of blood level sugar will affect readmission. . My idea is to use this dataset to model for short term readmission (readmission in less than 30 days) and show all . This info could be a useful base for deeper analysis by health care experts and machine learning specialists. . The dataset consists of 50 features and 110 observations. Each observation is one patient hospitalization record with a readmission parameter. It can have values NO, &lt; 30 , &gt; 30 days. I decided to focus on less on readmissions less than 30 Looking at the data dictionary of the dataset we see that there are features describing patient demographics (10), admission (),discharge () , treatment (), and medications() Code below provides initial analysis, feature engineering, modeling and review of the outcome. . This dataset published on (https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008). . Here are data dictionary . https://docs.google.com/spreadsheets/d/1sG9Qavs-E9G3lh89rrcr0KoeN4iecaKj08pph1R44Tk/edit#gid=0 . Diagnostic codes https://docs.google.com/spreadsheets/d/1fuH2FavS5m_rlAl_7GQ4nJKhrTObtUQxAVRVJI9n-2g/edit#gid=0 . Article based on this data https://www.hindawi.com/journals/bmri/2014/781670/ . Dataset analysis: . Load data | Initial analysis | Visualizations | Feature encoding | Feature engineering | Feature selection | Model building | Model comparison | Model evaluation | Final training and performance evaluation . | Exploratory analyss: Infer distribution of data using histograms . | Exploratory analyss: Use measures of central tendency and IQR to understand data at various positions | Wrangle data: Check for class imbalances and handle it by sampling from the dataset | Wrangle data: Check for sparse columns to remove them | Wrangle data: Check for columns with a few missing values to impute with zero, mean or median | Wrangle data: Encode categorical values to numerical values | Variable interdependence: Infer multicollinearity of data through correlation matrix | Feature selection: Remove redundant variables (i.e. columns that refer to the same thing e.g. ID) | Feature engineering: Remove highly collinear variables (i.e. correlation &gt; 0.8) | Feature engineering: Reduce data dimension to the variables that most explain variation in target variable | Modeling building: Random forest is chosen for machine learning modeling | Model comparison: Random forest modeling with raw data (PART 2) and PCA transformed data (PART 3) | Machine learning process: (a) Train/test data: Split data for training and testing (b) Model creation: Create models in a grid search for a combination of parameters (c) Model selection: Use 3-fold cross validation to evaluate model performance and select the best one (c) Final Model training: Use parameters of the best model from above to train on entire train set (d) Final Model evaluation: Use 3-fold cross validation for average error on 3 folds of training data (e) Final Model performance evaluation: Use confusion matrix, Precision, Recall, ROC to evaluate performance | . import pandas as pd import pandas_profiling import numpy as np import seaborn as sns import datetime, warnings, scipy import math as math #sklearn from sklearn.model_selection import train_test_split #from sklearn import linear_model #from sklearn.decomposition import PCA from sklearn.compose import ColumnTransformer #from sklearn.pipeline import Pipeline #from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.model_selection import cross_val_score from sklearn.metrics import f1_score from sklearn.model_selection import GridSearchCV from sklearn import ensemble from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score from sklearn.metrics import confusion_matrix from imblearn.metrics import sensitivity_specificity_support from imblearn.metrics import classification_report_imbalanced # added custom package pip install install_ply from https://github.com/coursera/pandas-ply from pandas_ply import install_ply, X, sym_call install_ply(pd) # from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; import matplotlib.pyplot as plt font = {&#39;family&#39; : &#39;serif&#39;, &#39;weight&#39; : &#39;bold&#39;, &#39;size&#39; : 18} plt.rc(&#39;font&#39;, **font) %matplotlib inline import warnings; warnings.simplefilter(&#39;ignore&#39;) . Load and check data . data_url =&quot;.. .. .. datasets dataset_diabetes diabetic_data.csv&quot; print(data_url) dfd=pd.read_csv(data_url, encoding=&#39;utf-8-sig&#39;) dfd.head(10) print(&quot;Data shape: {}&quot;.format(dfd.shape )) print(&quot;Data size: {}&quot;.format(dfd.memory_usage(index=True).sum())) . .. .. .. datasets dataset_diabetes diabetic_data.csv . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | ? | 6 | 25 | 1 | 1 | ... | No | No | No | No | No | No | No | No | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | ? | 1 | 1 | 7 | 3 | ... | No | Up | No | No | No | No | No | Ch | Yes | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | ? | 1 | 1 | 7 | 2 | ... | No | No | No | No | No | No | No | No | Yes | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | ? | 1 | 1 | 7 | 2 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | ? | 1 | 1 | 7 | 1 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 5 35754 | 82637451 | Caucasian | Male | [50-60) | ? | 2 | 1 | 2 | 3 | ... | No | Steady | No | No | No | No | No | No | Yes | &gt;30 | . 6 55842 | 84259809 | Caucasian | Male | [60-70) | ? | 3 | 1 | 2 | 4 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 7 63768 | 114882984 | Caucasian | Male | [70-80) | ? | 1 | 1 | 7 | 5 | ... | No | No | No | No | No | No | No | No | Yes | &gt;30 | . 8 12522 | 48330783 | Caucasian | Female | [80-90) | ? | 2 | 1 | 4 | 13 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 9 15738 | 63555939 | Caucasian | Female | [90-100) | ? | 3 | 3 | 4 | 12 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 10 rows × 50 columns . Data shape: (101766, 50) Data size: 40706528 . Attribtue analysis . Base info . def describeAll(df): print(&quot; n --Data head n&quot;) print(df.head(5)) print(&quot; n --Data tail n&quot;) print(df.tail(5)) print(&quot; n --Data types and counts n&quot;) print(df.info()) print(&quot; n --Simple statistics for each variable -- n&quot;) print(df.describe()) print(&quot; n -Count All missing!!- n&quot;) print(df.isnull().sum()) print(&quot; n --All categorical features with number of unique values n&quot;) categorical = list(df.select_dtypes(include=[&#39;object&#39;])) if len(categorical) == 0: print(&quot;- No categorical features by data type &quot;) else : for cName in categorical: luv = df[cName].unique().tolist() if len(luv) &lt; 20: print(str(cName) + &#39;: &#39;+str(len(luv)) + &#39; un.val: &#39; + str(luv)) print(df.groupby(cName).size().sort_values(ascending=False)) else: print(cName + &#39;: &#39; +str(len(luv)) + &#39; un. val&#39;) . describeAll(dfd) . --Data head encounter_id patient_nbr race gender age weight 0 2278392 8222157 Caucasian Female [0-10) ? 1 149190 55629189 Caucasian Female [10-20) ? 2 64410 86047875 AfricanAmerican Female [20-30) ? 3 500364 82442376 Caucasian Male [30-40) ? 4 16680 42519267 Caucasian Male [40-50) ? admission_type_id discharge_disposition_id admission_source_id 0 6 25 1 1 1 1 7 2 1 1 7 3 1 1 7 4 1 1 7 time_in_hospital ... citoglipton insulin glyburide-metformin 0 1 ... No No No 1 3 ... No Up No 2 2 ... No No No 3 2 ... No Up No 4 1 ... No Steady No glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone 0 No No No 1 No No No 2 No No No 3 No No No 4 No No No metformin-pioglitazone change diabetesMed readmitted 0 No No No NO 1 No Ch Yes &gt;30 2 No No Yes NO 3 No Ch Yes NO 4 No Ch Yes NO [5 rows x 50 columns] --Data tail encounter_id patient_nbr race gender age weight 101761 443847548 100162476 AfricanAmerican Male [70-80) ? 101762 443847782 74694222 AfricanAmerican Female [80-90) ? 101763 443854148 41088789 Caucasian Male [70-80) ? 101764 443857166 31693671 Caucasian Female [80-90) ? 101765 443867222 175429310 Caucasian Male [70-80) ? admission_type_id discharge_disposition_id admission_source_id 101761 1 3 7 101762 1 4 5 101763 1 1 7 101764 2 3 7 101765 1 1 7 time_in_hospital ... citoglipton insulin glyburide-metformin 101761 3 ... No Down No 101762 5 ... No Steady No 101763 1 ... No Down No 101764 10 ... No Up No 101765 6 ... No No No glipizide-metformin glimepiride-pioglitazone 101761 No No 101762 No No 101763 No No 101764 No No 101765 No No metformin-rosiglitazone metformin-pioglitazone change diabetesMed 101761 No No Ch Yes 101762 No No No Yes 101763 No No Ch Yes 101764 No No Ch Yes 101765 No No No No readmitted 101761 &gt;30 101762 NO 101763 NO 101764 NO 101765 NO [5 rows x 50 columns] --Data types and counts &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 101766 entries, 0 to 101765 Data columns (total 50 columns): encounter_id 101766 non-null int64 patient_nbr 101766 non-null int64 race 101766 non-null object gender 101766 non-null object age 101766 non-null object weight 101766 non-null object admission_type_id 101766 non-null int64 discharge_disposition_id 101766 non-null int64 admission_source_id 101766 non-null int64 time_in_hospital 101766 non-null int64 payer_code 101766 non-null object medical_specialty 101766 non-null object num_lab_procedures 101766 non-null int64 num_procedures 101766 non-null int64 num_medications 101766 non-null int64 number_outpatient 101766 non-null int64 number_emergency 101766 non-null int64 number_inpatient 101766 non-null int64 diag_1 101766 non-null object diag_2 101766 non-null object diag_3 101766 non-null object number_diagnoses 101766 non-null int64 max_glu_serum 101766 non-null object A1Cresult 101766 non-null object metformin 101766 non-null object repaglinide 101766 non-null object nateglinide 101766 non-null object chlorpropamide 101766 non-null object glimepiride 101766 non-null object acetohexamide 101766 non-null object glipizide 101766 non-null object glyburide 101766 non-null object tolbutamide 101766 non-null object pioglitazone 101766 non-null object rosiglitazone 101766 non-null object acarbose 101766 non-null object miglitol 101766 non-null object troglitazone 101766 non-null object tolazamide 101766 non-null object examide 101766 non-null object citoglipton 101766 non-null object insulin 101766 non-null object glyburide-metformin 101766 non-null object glipizide-metformin 101766 non-null object glimepiride-pioglitazone 101766 non-null object metformin-rosiglitazone 101766 non-null object metformin-pioglitazone 101766 non-null object change 101766 non-null object diabetesMed 101766 non-null object readmitted 101766 non-null object dtypes: int64(13), object(37) memory usage: 38.8+ MB None --Simple statistics for each variable -- encounter_id patient_nbr admission_type_id count 1.017660e+05 1.017660e+05 101766.000000 mean 1.652016e+08 5.433040e+07 2.024006 std 1.026403e+08 3.869636e+07 1.445403 min 1.252200e+04 1.350000e+02 1.000000 25% 8.496119e+07 2.341322e+07 1.000000 50% 1.523890e+08 4.550514e+07 1.000000 75% 2.302709e+08 8.754595e+07 3.000000 max 4.438672e+08 1.895026e+08 8.000000 discharge_disposition_id admission_source_id time_in_hospital count 101766.000000 101766.000000 101766.000000 mean 3.715642 5.754437 4.395987 std 5.280166 4.064081 2.985108 min 1.000000 1.000000 1.000000 25% 1.000000 1.000000 2.000000 50% 1.000000 7.000000 4.000000 75% 4.000000 7.000000 6.000000 max 28.000000 25.000000 14.000000 num_lab_procedures num_procedures num_medications number_outpatient count 101766.000000 101766.000000 101766.000000 101766.000000 mean 43.095641 1.339730 16.021844 0.369357 std 19.674362 1.705807 8.127566 1.267265 min 1.000000 0.000000 1.000000 0.000000 25% 31.000000 0.000000 10.000000 0.000000 50% 44.000000 1.000000 15.000000 0.000000 75% 57.000000 2.000000 20.000000 0.000000 max 132.000000 6.000000 81.000000 42.000000 number_emergency number_inpatient number_diagnoses count 101766.000000 101766.000000 101766.000000 mean 0.197836 0.635566 7.422607 std 0.930472 1.262863 1.933600 min 0.000000 0.000000 1.000000 25% 0.000000 0.000000 6.000000 50% 0.000000 0.000000 8.000000 75% 0.000000 1.000000 9.000000 max 76.000000 21.000000 16.000000 -Count All missing!!- encounter_id 0 patient_nbr 0 race 0 gender 0 age 0 weight 0 admission_type_id 0 discharge_disposition_id 0 admission_source_id 0 time_in_hospital 0 payer_code 0 medical_specialty 0 num_lab_procedures 0 num_procedures 0 num_medications 0 number_outpatient 0 number_emergency 0 number_inpatient 0 diag_1 0 diag_2 0 diag_3 0 number_diagnoses 0 max_glu_serum 0 A1Cresult 0 metformin 0 repaglinide 0 nateglinide 0 chlorpropamide 0 glimepiride 0 acetohexamide 0 glipizide 0 glyburide 0 tolbutamide 0 pioglitazone 0 rosiglitazone 0 acarbose 0 miglitol 0 troglitazone 0 tolazamide 0 examide 0 citoglipton 0 insulin 0 glyburide-metformin 0 glipizide-metformin 0 glimepiride-pioglitazone 0 metformin-rosiglitazone 0 metformin-pioglitazone 0 change 0 diabetesMed 0 readmitted 0 dtype: int64 --All categorical features with number of unique values race: 6 un.val: [&#39;Caucasian&#39;, &#39;AfricanAmerican&#39;, &#39;?&#39;, &#39;Other&#39;, &#39;Asian&#39;, &#39;Hispanic&#39;] race Caucasian 76099 AfricanAmerican 19210 ? 2273 Hispanic 2037 Other 1506 Asian 641 dtype: int64 gender: 3 un.val: [&#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39;] gender Female 54708 Male 47055 Unknown/Invalid 3 dtype: int64 age: 10 un.val: [&#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39;] age [70-80) 26068 [60-70) 22483 [50-60) 17256 [80-90) 17197 [40-50) 9685 [30-40) 3775 [90-100) 2793 [20-30) 1657 [10-20) 691 [0-10) 161 dtype: int64 weight: 10 un.val: [&#39;?&#39;, &#39;[75-100)&#39;, &#39;[50-75)&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[25-50)&#39;, &#39;[125-150)&#39;, &#39;[175-200)&#39;, &#39;[150-175)&#39;, &#39;&gt;200&#39;] weight ? 98569 [75-100) 1336 [50-75) 897 [100-125) 625 [125-150) 145 [25-50) 97 [0-25) 48 [150-175) 35 [175-200) 11 &gt;200 3 dtype: int64 payer_code: 18 un.val: [&#39;?&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;HM&#39;, &#39;UN&#39;, &#39;BC&#39;, &#39;SP&#39;, &#39;CP&#39;, &#39;SI&#39;, &#39;DM&#39;, &#39;CM&#39;, &#39;CH&#39;, &#39;PO&#39;, &#39;WC&#39;, &#39;OT&#39;, &#39;OG&#39;, &#39;MP&#39;, &#39;FR&#39;] payer_code ? 40256 MC 32439 HM 6274 SP 5007 BC 4655 MD 3532 CP 2533 UN 2448 CM 1937 OG 1033 PO 592 DM 549 CH 146 WC 135 OT 95 MP 79 SI 55 FR 1 dtype: int64 medical_specialty: 73 un. val diag_1: 717 un. val diag_2: 749 un. val diag_3: 790 un. val max_glu_serum: 4 un.val: [&#39;None&#39;, &#39;&gt;300&#39;, &#39;Norm&#39;, &#39;&gt;200&#39;] max_glu_serum None 96420 Norm 2597 &gt;200 1485 &gt;300 1264 dtype: int64 A1Cresult: 4 un.val: [&#39;None&#39;, &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;Norm&#39;] A1Cresult None 84748 &gt;8 8216 Norm 4990 &gt;7 3812 dtype: int64 metformin: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] metformin No 81778 Steady 18346 Up 1067 Down 575 dtype: int64 repaglinide: 4 un.val: [&#39;No&#39;, &#39;Up&#39;, &#39;Steady&#39;, &#39;Down&#39;] repaglinide No 100227 Steady 1384 Up 110 Down 45 dtype: int64 nateglinide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] nateglinide No 101063 Steady 668 Up 24 Down 11 dtype: int64 chlorpropamide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] chlorpropamide No 101680 Steady 79 Up 6 Down 1 dtype: int64 glimepiride: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] glimepiride No 96575 Steady 4670 Up 327 Down 194 dtype: int64 acetohexamide: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] acetohexamide No 101765 Steady 1 dtype: int64 glipizide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] glipizide No 89080 Steady 11356 Up 770 Down 560 dtype: int64 glyburide: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] glyburide No 91116 Steady 9274 Up 812 Down 564 dtype: int64 tolbutamide: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] tolbutamide No 101743 Steady 23 dtype: int64 pioglitazone: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] pioglitazone No 94438 Steady 6976 Up 234 Down 118 dtype: int64 rosiglitazone: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] rosiglitazone No 95401 Steady 6100 Up 178 Down 87 dtype: int64 acarbose: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;, &#39;Down&#39;] acarbose No 101458 Steady 295 Up 10 Down 3 dtype: int64 miglitol: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] miglitol No 101728 Steady 31 Down 5 Up 2 dtype: int64 troglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] troglitazone No 101763 Steady 3 dtype: int64 tolazamide: 3 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] tolazamide No 101727 Steady 38 Up 1 dtype: int64 examide: 1 un.val: [&#39;No&#39;] examide No 101766 dtype: int64 citoglipton: 1 un.val: [&#39;No&#39;] citoglipton No 101766 dtype: int64 insulin: 4 un.val: [&#39;No&#39;, &#39;Up&#39;, &#39;Steady&#39;, &#39;Down&#39;] insulin No 47383 Steady 30849 Down 12218 Up 11316 dtype: int64 glyburide-metformin: 4 un.val: [&#39;No&#39;, &#39;Steady&#39;, &#39;Down&#39;, &#39;Up&#39;] glyburide-metformin No 101060 Steady 692 Up 8 Down 6 dtype: int64 glipizide-metformin: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] glipizide-metformin No 101753 Steady 13 dtype: int64 glimepiride-pioglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] glimepiride-pioglitazone No 101765 Steady 1 dtype: int64 metformin-rosiglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] metformin-rosiglitazone No 101764 Steady 2 dtype: int64 metformin-pioglitazone: 2 un.val: [&#39;No&#39;, &#39;Steady&#39;] metformin-pioglitazone No 101765 Steady 1 dtype: int64 change: 2 un.val: [&#39;No&#39;, &#39;Ch&#39;] change No 54755 Ch 47011 dtype: int64 diabetesMed: 2 un.val: [&#39;No&#39;, &#39;Yes&#39;] diabetesMed Yes 78363 No 23403 dtype: int64 readmitted: 3 un.val: [&#39;NO&#39;, &#39;&gt;30&#39;, &#39;&lt;30&#39;] readmitted NO 54864 &gt;30 35545 &lt;30 11357 dtype: int64 . num_lst=[&#39;num_lab_procedures&#39;,&#39;num_procedures&#39;,&#39;num_medications&#39;,&#39;number_outpatient&#39;,&#39;number_emergency&#39;,&#39;number_inpatient&#39;] for col in num_lst: dfd.groupby(col).size().sort_values(ascending=False) . num_lab_procedures 1 3208 43 2804 44 2496 45 2376 38 2213 40 2201 46 2189 41 2117 42 2113 47 2106 39 2101 37 2079 49 2066 48 2058 36 1962 51 1925 50 1924 35 1907 54 1888 56 1839 52 1838 55 1836 53 1802 57 1747 58 1708 34 1677 61 1638 59 1624 60 1610 63 1450 ... 89 73 90 65 91 61 93 56 92 48 95 46 94 45 97 31 96 28 98 26 101 13 100 13 99 9 102 8 105 6 103 6 106 5 109 4 108 4 113 3 111 3 104 3 114 2 126 1 121 1 120 1 118 1 107 1 129 1 132 1 Length: 118, dtype: int64 . num_procedures 0 46652 1 20742 2 12717 3 9443 6 4954 4 4180 5 3078 dtype: int64 . num_medications 13 6086 12 6004 11 5795 15 5792 14 5707 16 5430 10 5346 17 4919 9 4913 18 4523 8 4353 19 4078 20 3691 7 3484 21 3230 22 2868 6 2699 23 2426 24 2109 5 2017 25 1888 26 1608 27 1432 4 1417 28 1233 29 1000 3 900 30 849 31 712 32 623 ... 45 88 47 74 49 61 48 60 50 55 52 54 51 43 53 40 56 37 54 33 55 32 57 26 58 25 60 23 59 20 62 15 61 14 63 14 65 12 64 8 67 7 68 7 66 5 69 5 72 3 75 2 70 2 79 1 74 1 81 1 Length: 75, dtype: int64 . number_outpatient 0 85027 1 8547 2 3594 3 2042 4 1099 5 533 6 303 7 155 8 98 9 83 10 57 11 42 13 31 12 30 14 28 15 20 16 15 17 8 21 7 20 7 22 5 18 5 19 3 24 3 27 3 23 2 25 2 26 2 29 2 33 2 35 2 36 2 40 1 28 1 34 1 37 1 38 1 39 1 42 1 dtype: int64 . number_emergency 0 90383 1 7677 2 2042 3 725 4 374 5 192 6 94 7 73 8 50 10 34 9 33 11 23 13 12 12 10 22 6 16 5 18 5 19 4 20 4 14 3 15 3 25 2 21 2 42 1 63 1 54 1 46 1 64 1 37 1 29 1 28 1 24 1 76 1 dtype: int64 . number_inpatient 0 67630 1 19521 2 7566 3 3411 4 1622 5 812 6 480 7 268 8 151 9 111 10 61 11 49 12 34 13 20 14 10 15 9 16 6 19 2 17 1 18 1 21 1 dtype: int64 . dfd.head(5) . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | ? | 6 | 25 | 1 | 1 | ... | No | No | No | No | No | No | No | No | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | ? | 1 | 1 | 7 | 3 | ... | No | Up | No | No | No | No | No | Ch | Yes | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | ? | 1 | 1 | 7 | 2 | ... | No | No | No | No | No | No | No | No | Yes | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | ? | 1 | 1 | 7 | 2 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | ? | 1 | 1 | 7 | 1 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 5 rows × 50 columns . dfd.tail(5) . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 101761 443847548 | 100162476 | AfricanAmerican | Male | [70-80) | ? | 1 | 3 | 7 | 3 | ... | No | Down | No | No | No | No | No | Ch | Yes | &gt;30 | . 101762 443847782 | 74694222 | AfricanAmerican | Female | [80-90) | ? | 1 | 4 | 5 | 5 | ... | No | Steady | No | No | No | No | No | No | Yes | NO | . 101763 443854148 | 41088789 | Caucasian | Male | [70-80) | ? | 1 | 1 | 7 | 1 | ... | No | Down | No | No | No | No | No | Ch | Yes | NO | . 101764 443857166 | 31693671 | Caucasian | Female | [80-90) | ? | 2 | 3 | 7 | 10 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 101765 443867222 | 175429310 | Caucasian | Male | [70-80) | ? | 1 | 1 | 7 | 6 | ... | No | No | No | No | No | No | No | No | No | NO | . 5 rows × 50 columns . Summary of initial analysis: . No missing data. However looking carefully we see many columns have &#39;?&#39; that is really missing | Most attributes are categorical, just a few are numerical | first two columns are identifiers and we don&#39;t need them | All diagnosis : more that 700 unique values cold be grouped according to std. medical groups ~ 20 groups | attribute: medical specialty &gt; 70 unique values; but many &#39;?&#39; should be grouped | attribute: admission - discharge speficific convert numeric to categoric and limit if necessary | discharge includes death ! | attributes related to time leave as is. | Attributes related to prescription: Attributes that have very low varience 99.9% of cohort have the same value are not important for the model and could be dropped | Other prescriptions have categorical values with smaill variance we will try to use them as is or group them together. | | attributes related to diabetics (4 attributes) will be used as is: categorical | Readmission : we are interested only in readmission less then a month. Readmission more than a month will be dropped. | . . Data selection . Looking at the ids_mapping.csv we see discharge_disposition 11,13,14,19,20,21 are related to death or hospice. They could not be readmitted and should be removed from the cohort. . dfd=dfd.loc[~dfd.discharge_disposition_id.isin([11,13,14,19,20,12])] . Visualizations . Target distribution, other variables distribution, correlation, After graph summary of what’s it is important . dfc=dfd.copy() dfc=dfd[dfd[&#39;readmitted&#39;].isin([&quot;NO&quot;,&quot;&lt;30&quot;])] #dfc.reset_index() sns.countplot(data=dfd,x=&#39;readmitted&#39;) # ,&#39;Readmision Distribution in the dataset&#39; plt.title(&quot;Conut admisson by two categories: less than 30 days or more&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x173de7ece88&gt; . Text(0.5, 1.0, &#39;Conut admisson by two categories: less than 30 days or more&#39;) . def plothue(df,column,hue,title,saveFigName,stackedFlag=False): df.groupby([column,hue]).size().unstack().plot.bar(stacked=stackedFlag) plt.title(title, fontsize=18) plt.xticks(rotation=45) plt.xlabel(column, fontsize=16) plt.savefig(saveFigName) plothue(df=dfd,column=&#39;gender&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by gender&#39;,saveFigName=&#39;gender&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;age&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by age&#39;,saveFigName=&#39;age&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;race&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by race&#39;,saveFigName=&#39;race&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;admission_type_id&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by Admission&#39;,saveFigName=&#39;admission&#39;,stackedFlag=False) . plothue(df=dfd,column=&#39;discharge_disposition_id&#39;,hue=&#39;readmitted&#39;,title=&#39;Readmission distribution by discharge&#39;,saveFigName=&#39;discharge&#39;,stackedFlag=False) . dfd[&quot;readm&quot;]=dfd[&quot;readmitted&quot;].astype(&quot;category&quot;) dfz=dfd.filter([&quot;time_in_hospital&quot;,&quot;readm&quot;]) dfz[&quot;all&quot;]=&quot;&quot; #dfz.groupby(&quot;readm&quot;).size() #dfc=dfc.reset_index(drop=True) ax = sns.violinplot(x=&quot;all&quot;, y=&quot;time_in_hospital&quot;, hue=&quot;readm&quot;, data=dfz, split=True) ax.set_xlabel(&quot; &quot;) ax.set_title(&quot;Readmission distribution by time spent in hospital&quot;, fontsize=20) plt.savefig(&quot;time-in-hospital&quot;) . Text(0.5, 0, &#39; &#39;) . Text(0.5, 1.0, &#39;Readmission distribution by time spent in hospital&#39;) . sns.boxplot(x=&#39;readmitted&#39;, y=&#39;time_in_hospital&#39;, data=dfc) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x173dd22dfc8&gt; . ax=sns.boxplot(x=&#39;readmitted&#39;, y=&#39;number_inpatient&#39;, data=dfd) ax.set_title(&quot;Readmission distribution by number of inpatient visits&quot;, fontsize=20) ax.legend(loc=&quot;upper-right&quot;) #ax.set_label(&quot;readmissions&quot;) plt.savefig(&quot;in-patient&quot;) . Text(0.5, 1.0, &#39;Readmission distribution by number of inpatient visits&#39;) . No handles with labels found to put in legend. . &lt;matplotlib.legend.Legend at 0x173de799f48&gt; . def dist_per_count(df, feature, pLabel, pTitle): import matplotlib.ticker as ticker plt.figure(figsize=(12,8)) ncount=len(dfd) ax = sns.countplot(x=feature , data=df,order=[&quot;&lt;30&quot;,&quot;NO&quot;]) ### My case ! consistentsy plt.title(pTitle,fontsize=20) plt.xlabel(pLabel,fontsize=18) # Make twin axis ax2=ax.twinx() # Switch so count axis is on right, frequency on left ax2.yaxis.tick_left() ax.yaxis.tick_right() # Also switch the labels over ax.yaxis.set_label_position(&#39;right&#39;) ax2.yaxis.set_label_position(&#39;left&#39;) ax2.set_ylabel(&#39;Frequency [%]&#39;,fontsize=18) for p in ax.patches: x=p.get_bbox().get_points()[:,0] y=p.get_bbox().get_points()[1,1] ax.annotate(&#39;{:.1f}%&#39;.format(100.*y/ncount), (x.mean(), y), ha=&#39;center&#39;, va=&#39;bottom&#39;) # set the alignment of the text # Use a LinearLocator to ensure the correct number of ticks ax.yaxis.set_major_locator(ticker.LinearLocator(11)) # Fix the frequency range to 0-100 ax2.set_ylim(0,100) ax.set_ylim(0,ncount) # And use a MultipleLocator to ensure a tick spacing of 10 #ax2.yaxis.set_major_locator(ticker.MultipleLocator(10)) # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars ax2.grid(None) plt.savefig(pTitle) plt.show() dist_per_count(dfd,&#39;readmitted&#39;,&#39;Readmitted&#39;, &#39;Readmission distribution&#39;) . plt.figure(figsize=(14,12)) sns.heatmap(dfd.corr(),cmap = &#39;coolwarm&#39;,linewidth = 1,annot= True, annot_kws={&quot;size&quot;: 9}) plt.title(&#39;Quantitive Variable Correlation&#39;, fontsize=18) plt.savefig(&#39;Correlation&#39;) plt.show() . &lt;Figure size 1008x864 with 0 Axes&gt; . &lt;matplotlib.axes._subplots.AxesSubplot at 0x173dd11e808&gt; . Text(0.5, 1, &#39;Quantitive Variable Correlation&#39;) . Heatmap shows: . Feature Engineering . Group diagnoses: how? Why | Encode age, race, gender | Convert to categorical aedmission and discharge | . d1m=dfd[(dfd[&#39;diag_1&#39;].str.contains(&quot; ?&quot;))].shape[0] d2m=dfd[(dfd[&#39;diag_2&#39;].str.contains(&quot; ?&quot;))].shape[0] d3m=dfd[(dfd[&#39;diag_3&#39;].str.contains(&quot; ?&quot;))].shape[0] print(&#39;missing diagnoses diag_-&gt;&#39; + str(d1m) +&#39; diag_2-&gt; &#39; + str(d2m) +&#39; diag_3-&gt; &#39; + str(d3m) ) . missing diagnoses diag_-&gt;17 diag_2-&gt; 284 diag_3-&gt; 1085 . cl=&#39;diag_1&#39; minCnt =5 ln=dfd.groupby(cl).size().to_frame(&#39;cnt&#39;) lst=ln.loc[ln[&#39;cnt&#39;] &lt; minCnt].index.values.tolist() print(lst) . [&#39;10&#39;, &#39;110&#39;, &#39;115&#39;, &#39;117&#39;, &#39;133&#39;, &#39;136&#39;, &#39;141&#39;, &#39;142&#39;, &#39;143&#39;, &#39;146&#39;, &#39;147&#39;, &#39;149&#39;, &#39;160&#39;, &#39;164&#39;, &#39;170&#39;, &#39;172&#39;, &#39;173&#39;, &#39;175&#39;, &#39;179&#39;, &#39;187&#39;, &#39;192&#39;, &#39;194&#39;, &#39;207&#39;, &#39;208&#39;, &#39;216&#39;, &#39;219&#39;, &#39;228&#39;, &#39;229&#39;, &#39;23&#39;, &#39;236&#39;, &#39;240&#39;, &#39;245&#39;, &#39;246&#39;, &#39;250.51&#39;, &#39;250.53&#39;, &#39;250.91&#39;, &#39;261&#39;, &#39;262&#39;, &#39;266&#39;, &#39;27&#39;, &#39;271&#39;, &#39;272&#39;, &#39;279&#39;, &#39;299&#39;, &#39;301&#39;, &#39;308&#39;, &#39;31&#39;, &#39;314&#39;, &#39;318&#39;, &#39;322&#39;, &#39;324&#39;, &#39;325&#39;, &#39;334&#39;, &#39;335&#39;, &#39;336&#39;, &#39;34&#39;, &#39;344&#39;, &#39;347&#39;, &#39;352&#39;, &#39;353&#39;, &#39;356&#39;, &#39;36&#39;, &#39;360&#39;, &#39;361&#39;, &#39;363&#39;, &#39;370&#39;, &#39;374&#39;, &#39;377&#39;, &#39;379&#39;, &#39;381&#39;, &#39;382&#39;, &#39;383&#39;, &#39;384&#39;, &#39;385&#39;, &#39;388&#39;, &#39;389&#39;, &#39;39&#39;, &#39;391&#39;, &#39;395&#39;, &#39;397&#39;, &#39;405&#39;, &#39;41&#39;, &#39;412&#39;, &#39;417&#39;, &#39;422&#39;, &#39;445&#39;, &#39;448&#39;, &#39;452&#39;, &#39;463&#39;, &#39;470&#39;, &#39;474&#39;, &#39;477&#39;, &#39;48&#39;, &#39;483&#39;, &#39;495&#39;, &#39;5&#39;, &#39;501&#39;, &#39;506&#39;, &#39;508&#39;, &#39;52&#39;, &#39;523&#39;, &#39;524&#39;, &#39;526&#39;, &#39;529&#39;, &#39;542&#39;, &#39;543&#39;, &#39;57&#39;, &#39;570&#39;, &#39;579&#39;, &#39;580&#39;, &#39;582&#39;, &#39;588&#39;, &#39;602&#39;, &#39;603&#39;, &#39;605&#39;, &#39;61&#39;, &#39;610&#39;, &#39;623&#39;, &#39;632&#39;, &#39;633&#39;, &#39;634&#39;, &#39;637&#39;, &#39;643&#39;, &#39;645&#39;, &#39;647&#39;, &#39;649&#39;, &#39;653&#39;, &#39;657&#39;, &#39;66&#39;, &#39;665&#39;, &#39;669&#39;, &#39;671&#39;, &#39;674&#39;, &#39;683&#39;, &#39;684&#39;, &#39;686&#39;, &#39;690&#39;, &#39;691&#39;, &#39;692&#39;, &#39;694&#39;, &#39;696&#39;, &#39;7&#39;, &#39;703&#39;, &#39;705&#39;, &#39;706&#39;, &#39;709&#39;, &#39;717&#39;, &#39;720&#39;, &#39;731&#39;, &#39;732&#39;, &#39;734&#39;, &#39;735&#39;, &#39;745&#39;, &#39;75&#39;, &#39;751&#39;, &#39;753&#39;, &#39;759&#39;, &#39;791&#39;, &#39;792&#39;, &#39;793&#39;, &#39;795&#39;, &#39;797&#39;, &#39;800&#39;, &#39;806&#39;, &#39;817&#39;, &#39;826&#39;, &#39;832&#39;, &#39;833&#39;, &#39;834&#39;, &#39;835&#39;, &#39;84&#39;, &#39;842&#39;, &#39;845&#39;, &#39;846&#39;, &#39;848&#39;, &#39;862&#39;, &#39;863&#39;, &#39;868&#39;, &#39;870&#39;, &#39;871&#39;, &#39;875&#39;, &#39;879&#39;, &#39;880&#39;, &#39;881&#39;, &#39;885&#39;, &#39;886&#39;, &#39;890&#39;, &#39;893&#39;, &#39;897&#39;, &#39;903&#39;, &#39;904&#39;, &#39;906&#39;, &#39;911&#39;, &#39;913&#39;, &#39;914&#39;, &#39;915&#39;, &#39;916&#39;, &#39;917&#39;, &#39;921&#39;, &#39;923&#39;, &#39;928&#39;, &#39;934&#39;, &#39;936&#39;, &#39;939&#39;, &#39;941&#39;, &#39;942&#39;, &#39;955&#39;, &#39;957&#39;, &#39;963&#39;, &#39;971&#39;, &#39;973&#39;, &#39;974&#39;, &#39;975&#39;, &#39;976&#39;, &#39;98&#39;, &#39;982&#39;, &#39;983&#39;, &#39;986&#39;, &#39;987&#39;, &#39;991&#39;, &#39;994&#39;, &#39;E909&#39;, &#39;V07&#39;, &#39;V26&#39;, &#39;V45&#39;, &#39;V51&#39;, &#39;V60&#39;, &#39;V66&#39;, &#39;V70&#39;] . def get_exclusion_list(df,cl, minCnt): ln=df.groupby(cl).size().to_frame(&#39;cnt&#39;) return ln.loc[ln[&#39;cnt&#39;] &lt;= minCnt].index.values.tolist() exc_lst=get_exclusion_list(df=dfd, cl=&#39;diag_1&#39;, minCnt=5 ) #print(exc_lst) . # import collections class RangeDict(): def __init__(self): self._dict = {} def __getitem__(self, key): if type(key) == str: if key[0] == &#39;E&#39;: kt=1000 elif key[0] == &#39;V&#39;: kt=2000 elif key[0] ==&#39;?&#39;: kt = 3000 else: kt=int(float(key)) else: kt=int(key) for k, v in self._dict.items(): if k[0] &lt;= kt &lt;= k[1]: return v raise KeyError(&quot;Key not found! &quot; + str(kt)) def __setitem__(self, key, value): if len(key) == 2: if key[0] == &#39;E000&#39;: kt1, kt2 = 1000,1999 elif key[0] == &#39;V000&#39;: kt1, kt2 = 2000,2999 elif key[0] == &#39;?&#39;: kt1, kt2 = 3000,3999 else: kt1, kt2 =key[0] , key[1] if kt1 &lt; kt2: self._dict.__setitem__((kt1, kt2), value) def __contains__(self, key): try: return bool(self.__getitem__(key)) except KeyError: return False . rd = RangeDict() rd[(1,139)] = &#39;1&#39; rd[(140,239)]=&#39;2&#39; rd[(240,279)]=&#39;3&#39; rd[(280,289)]=&#39;4&#39; rd[(290,319)]=&#39;5&#39; rd[(320,389)]=&#39;6&#39; rd[(390,459)]=&#39;7&#39; rd[(460,519)]=&#39;8&#39; rd[(520,579)]=&#39;9&#39; rd[(580,629)]=&#39;10&#39; rd[(630,679)]=&#39;11&#39; rd[(680,709)]=&#39;12&#39; rd[(710,799)]=&#39;13&#39; rd[(800,1000)]=&#39;14&#39; rd[(&#39;E000&#39;,&#39;E999&#39;)]=&#39;15&#39; rd[(&#39;V000&#39;,&#39;V999&#39;)]=&#39;16&#39; rd[(&#39;?&#39;,&#39;?999&#39;)] =&#39;27&#39; print(rd[459]) print(rd[&#39;205.5&#39;]) print(rd[&#39;V123&#39;]) print(rd[&#39;?&#39;]) . 7 2 16 27 . # if value starts with number # get string convert it to number dfd[&#39;diag_g1&#39;]=dfd[&#39;diag_1&#39;].astype(str).apply(lambda x: rd[x]) . dfd.groupby(&#39;diag_g1&#39;).size().sort_values(ascending=False) . diag_g1 7 19520 13 8441 3 7237 8 6293 9 5945 14 4738 10 3378 2 2633 1 1824 12 1563 5 1484 16 1083 6 827 4 664 11 574 27 17 dtype: int64 . dfd[&#39;diag_g2&#39;]=dfd[&#39;diag_2&#39;].astype(str).apply(lambda x: rd[x]) dfd[&#39;diag_g3&#39;]=dfd[&#39;diag_3&#39;].astype(str).apply(lambda x: rd[x]) . Review all columns . categorical: &#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;weight&#39;, &#39;admission_type_id&#39;, &#39;discharge_disposition_id&#39;, &#39;admission_source_id&#39;, &#39;payer_code&#39;, &#39;medical_specialty&#39;, &#39;diag_1&#39;, &#39;diag_2&#39;, &#39;diag_3&#39; . numerical : &#39;time_in_hospital&#39;,&#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, , &#39;number_diagnoses&#39;, &#39;max_glu_serum&#39;, . &#39;A1Cresult&#39;, &#39;metformin&#39;, &#39;repaglinide&#39;, &#39;nateglinide&#39;, &#39;chlorpropamide&#39;, &#39;glimepiride&#39;, &#39;acetohexamide&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;tolbutamide&#39;, &#39;pioglitazone&#39;, &#39;rosiglitazone&#39;, &#39;acarbose&#39;, &#39;miglitol&#39;, &#39;troglitazone&#39;, &#39;tolazamide&#39;, &#39;examide&#39;, &#39;citoglipton&#39;, &#39;insulin&#39;, &#39;glyburide-metformin&#39;, &#39;glipizide-metformin&#39;, &#39;glimepiride-pioglitazone&#39;, &#39;metformin-rosiglitazone&#39;, &#39;metformin-pioglitazone&#39;, &#39;change&#39;, &#39;diabetesMed&#39;, &#39;readmitted&#39;] . dfd[&#39;race&#39;] = dfd[&#39;race&#39;].fillna(&#39;Unk&#39;) dfd[&#39;payer_code&#39;] = dfd[&#39;payer_code&#39;].fillna(&#39;Unk&#39;) dfd[&#39;medical_specialty&#39;] = dfd[&#39;medical_specialty&#39;].fillna(&#39;Unk&#39;) . dfd=dfd.loc[dfd[&#39;gender&#39;].isin([&#39;Female&#39;,&#39;Male&#39;])] dfd[&#39;gender_code&#39;]=dfd[&#39;gender&#39;].map({&#39;Male&#39;:1,&#39;Female&#39;:2}) . def encodebyList(df,column): &#39;&#39;&#39; encode column converting values with &#39;&#39;&#39; clmnlist = df.groupby(column).size().index.tolist() clmnindx = list(map (lambda x: clmnlist.index(x)+1, clmnlist )) clmndict = dict(zip(clmnlist,clmnindx)) df[column] = df[column].replace(clmndict) . dfd[&#39;race1&#39;]=dfd[&#39;race&#39;].replace(&#39;?&#39;,&#39;Unkn&#39;) race_list=dfd.groupby(&#39;race1&#39;).size().index.tolist() race_ind=list(map (lambda x: race_list.index(x)+1, race_list )) race_dic=dict(zip(race_list,race_ind )) dfd[&#39;race_code&#39;]=dfd[&#39;race1&#39;].replace(race_dic) dfd.drop(&#39;race1&#39;,axis=1,inplace=True) print(race_ind) . [1, 2, 3, 4, 5, 6] . age_list=dfd.groupby(&#39;age&#39;).size().index.tolist() age_ind=list(map (lambda x: age_list.index(x)+1, age_list )) age_dic=dict(zip(age_list,age_ind )) dfd[&#39;age_code&#39;]=dfd[&#39;age&#39;].replace(age_dic) . def topNCat(df, col_old, col_new, list_size=11): dfd[col_old]=dfd[col_old].replace(&#39;?&#39;,&#39;Unkn&#39;) colTN=df.groupby(col_old).size().sort_values(ascending = False).head(list_size).index.tolist() df[col_new]=dfd[col_old].copy() df.loc[~df[col_new].isin(colTN),col_new] = &#39;Other&#39; . # we will replace all values with low freq as code 0. There is no cuch code dfg=dfd.copy() topNCat(df=dfg, col_old=&#39;diag_1&#39;, col_new=&#39;diag_1_v2&#39;, list_size=11) dfg.groupby(&#39;diag_1_v2&#39;).size() . diag_1_v2 410 2386 414 4425 427 1747 428 3579 434 1343 486 2016 682 1255 715 1579 780 1274 786 2597 996 1168 Other 40469 dtype: int64 . Most common diagnoses . Code Description . 428 | Heart failure | . 414 | Other forms of chronic ischemic heart disease | . 768 | Intrauterine hypoxia and birth asphyxia | . 410 | Acute myocardial infarction | . 486 | Pneumonia, organism NOS | . 427 | Cardiac dysrhythmias | . 491 | Chronic bronchitis | . 715 | Osteoarthrosis and allied disorders | . 682 | Other cellulitis and abscess | . 434 | Occlusion of cerebral arteries | . 780 | General symptoms | . . topNCat(df=dfd, col_old=&#39;discharge_disposition_id&#39;, col_new=&#39;disDispID_code&#39;, list_size=11) dfd.groupby(&#39;disDispID_code&#39;).size() . disDispID_code 1 38715 2 1460 3 9038 4 537 5 834 6 7540 7 402 11 1642 18 2664 22 1473 25 613 Other 1300 dtype: int64 . topNCat(df=dfd, col_old=&#39;admission_source_id&#39;, col_new=&#39;admSrsID_code&#39;, list_size=11) dfd.groupby(&#39;admSrsID_code&#39;).size() . admSrsID_code 1 19924 2 794 3 129 4 2515 5 619 6 1866 7 35825 8 12 9 109 17 4323 20 80 Other 22 dtype: int64 . topNCat(df=dfd, col_old=&#39;payer_code&#39;, col_new=&#39;pay_code_m&#39;, list_size=11) dfd.groupby(&#39;pay_code_m&#39;).size() . pay_code_m BC 3354 CM 1276 CP 1750 HM 3954 MC 20457 MD 2273 OG 700 Other 689 PO 448 SP 3138 UN 1752 Unkn 26427 dtype: int64 . topNCat(df=dfd, col_old=&#39;medical_specialty&#39;, col_new=&#39;medSpelt_code&#39;, list_size=11) dfd.groupby(&#39;medSpelt_code&#39;).size() . medSpelt_code Cardiology 3498 Emergency/Trauma 4559 Family/GeneralPractice 4777 InternalMedicine 9912 Nephrology 947 Orthopedics 1073 Orthopedics-Reconstructive 940 Other 5379 Psychiatry 592 Radiologist 757 Surgery-General 2053 Unkn 31731 dtype: int64 . def encode_cats(df,col): enc_name=str(col)+&#39;_enc&#39; #df.drop(enc_name,axis=1,inplace=True) val_list=dfd.groupby(col).size().index.tolist() val_ind=list(map (lambda x: val_list.index(x)+1, val_list )) dic=dict(zip(val_list,val_ind )) df[enc_name]=dfd[col].replace(dic) return dic # dfd.groupby(enc_name).size() . encodeList=[&#39;metformin&#39;,&#39;repaglinide&#39;,&#39;glimepiride&#39;,&#39;glipizide&#39;,&#39;glyburide&#39;,&#39;pioglitazone&#39;,&#39;rosiglitazone&#39;,&#39;insulin&#39;] for col in encodeList: new_name=str(col)+&#39;_code&#39; #dfd.drop(new_name,axis=1,inplace=True) dfd[new_name]=dfd[col].map({&#39;Down&#39;:0,&#39;No&#39;:1,&#39;Steady&#39;:2,&#39;Up&#39;:3}) #dfd.groupby(new_name).size() . dfd[&#39;A1C_code&#39;]=dfd[&#39;A1Cresult&#39;].map({&#39;None&#39;:0,&#39;Norm&#39;:1,&#39;&gt;7&#39;:2,&#39;&gt;8&#39;:3}) # [&#39;None&#39;, &#39;&gt;300&#39;, &#39;Norm&#39;, &#39;&gt;200&#39;] dfd[&#39;max_glu_serum_code&#39;]=dfd[&#39;max_glu_serum&#39;].map({&#39;None&#39;:0,&#39;Norm&#39;:1,&#39;&gt;200&#39;:2,&#39;&gt;300&#39;:3}) dfd[&#39;change_code&#39;]=dfd[&#39;change&#39;].map({&#39;No&#39;:0,&#39;Ch&#39;:1}) dfd[&#39;diabMed_code&#39;]=dfd[&#39;diabetesMed&#39;].map({&#39;No&#39;:0,&#39;Yes&#39;:1}) . print(list(dfd)) . [&#39;index&#39;, &#39;encounter_id&#39;, &#39;patient_nbr&#39;, &#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;weight&#39;, &#39;admission_type_id&#39;, &#39;discharge_disposition_id&#39;, &#39;admission_source_id&#39;, &#39;time_in_hospital&#39;, &#39;payer_code&#39;, &#39;medical_specialty&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, &#39;diag_1&#39;, &#39;diag_2&#39;, &#39;diag_3&#39;, &#39;number_diagnoses&#39;, &#39;max_glu_serum&#39;, &#39;A1Cresult&#39;, &#39;metformin&#39;, &#39;repaglinide&#39;, &#39;nateglinide&#39;, &#39;chlorpropamide&#39;, &#39;glimepiride&#39;, &#39;acetohexamide&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;tolbutamide&#39;, &#39;pioglitazone&#39;, &#39;rosiglitazone&#39;, &#39;acarbose&#39;, &#39;miglitol&#39;, &#39;troglitazone&#39;, &#39;tolazamide&#39;, &#39;examide&#39;, &#39;citoglipton&#39;, &#39;insulin&#39;, &#39;glyburide-metformin&#39;, &#39;glipizide-metformin&#39;, &#39;glimepiride-pioglitazone&#39;, &#39;metformin-rosiglitazone&#39;, &#39;metformin-pioglitazone&#39;, &#39;change&#39;, &#39;diabetesMed&#39;, &#39;readmitted&#39;, &#39;readm&#39;, &#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;diag_g3&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;pay_code_m&#39;, &#39;medSpelt_code&#39;, &#39;readmitted_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . # # important step!!!! # dropList=[&#39;medSpelt_code&#39;,&#39;index&#39;,&#39;readm&#39;,&#39;diag_g3&#39;,&#39;pay_code_m&#39;,&#39;encounter_id&#39; , &#39;patient_nbr&#39;, &#39;weight&#39; ,&#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;admission_type_id&#39; ,&#39;discharge_disposition_id&#39;, &#39;admission_source_id&#39;,&#39;medical_specialty&#39;, &#39;nateglinide&#39; ,&#39;diag_1&#39;, &#39;diag_2&#39;,&#39;diag_3&#39;,&#39;readmitted&#39;, &#39;metformin-rosiglitazone&#39;,&#39;metformin-pioglitazone&#39;,&#39;chlorpropamide&#39; , &#39;acetohexamide&#39; ,&#39;tolbutamide&#39;,&#39;acarbose&#39;,&#39;miglitol&#39;,&#39;troglitazone&#39;,&#39;tolazamide&#39; ,&#39;examide&#39;,&#39;citoglipton&#39; ,&#39;glyburide-metformin&#39;,&#39;glipizide-metformin&#39;,&#39;glimepiride-pioglitazone&#39;, &#39;A1Cresult&#39;, &#39;metformin&#39;, &#39;repaglinide&#39;, &#39;glimepiride&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;pioglitazone&#39; , &#39;rosiglitazone&#39;, &#39;insulin&#39;, &#39;change&#39;, &#39;diabetesMed&#39;,&#39;payer_code&#39;, &#39;max_glu_serum&#39;] sd=dfd.drop(dropList,axis=1) print(list(df1)) . [&#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, &#39;number_diagnoses&#39;, &#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;readmitted_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . # these ared just pure numerical num_col_list= [&#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;,&#39;number_diagnoses&#39;] . sd.groupby(&#39;diag_g1&#39;).size() sd.head(10).T . diag_g1 1 1824 10 3378 11 574 12 1563 13 8441 14 4736 16 1083 2 2633 27 17 3 7237 4 664 5 1484 6 827 7 19519 8 6293 9 5945 dtype: int64 . 0 1 2 3 4 5 6 7 8 9 . time_in_hospital 1 | 2 | 2 | 1 | 4 | 13 | 12 | 7 | 7 | 10 | . num_lab_procedures 41 | 11 | 44 | 51 | 70 | 68 | 33 | 62 | 60 | 55 | . num_procedures 0 | 5 | 1 | 0 | 1 | 2 | 3 | 0 | 0 | 1 | . num_medications 1 | 13 | 16 | 8 | 21 | 28 | 18 | 11 | 15 | 31 | . number_outpatient 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . number_emergency 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . number_inpatient 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . number_diagnoses 1 | 6 | 7 | 5 | 7 | 8 | 8 | 7 | 8 | 8 | . diag_g1 3 | 11 | 1 | 2 | 7 | 7 | 7 | 2 | 7 | 7 | . diag_g2 27 | 3 | 3 | 2 | 7 | 7 | 2 | 4 | 3 | 7 | . gender_code 2 | 2 | 1 | 1 | 1 | 2 | 2 | 1 | 2 | 1 | . race_code 3 | 1 | 3 | 3 | 3 | 3 | 3 | 1 | 3 | 3 | . age_code 1 | 3 | 4 | 5 | 7 | 9 | 10 | 7 | 5 | 9 | . disDispID_code 25 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 3 | 6 | . admSrsID_code 1 | 7 | 7 | 7 | 2 | 4 | 4 | 4 | 7 | 7 | . readmitted_code 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . metformin_code 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 2 | 1 | . repaglinide_code 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | . glimepiride_code 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 1 | 1 | . glipizide_code 1 | 2 | 1 | 2 | 1 | 2 | 1 | 1 | 1 | 1 | . glyburide_code 1 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 1 | . pioglitazone_code 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . rosiglitazone_code 1 | 1 | 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | . insulin_code 1 | 1 | 3 | 2 | 2 | 2 | 2 | 2 | 0 | 2 | . A1C_code 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . max_glu_serum_code 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . change_code 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | . diabMed_code 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . #lst=list(sd).remove(num_col_list) print(list(sd)) . [&#39;level_0&#39;, &#39;index&#39;, &#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, &#39;number_diagnoses&#39;, &#39;readm&#39;, &#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;medSpelt_code&#39;, &#39;readmitted_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . allv=list(sd) no_encodingL=[&#39;time_in_hospital&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39; , &#39;number_outpatient&#39;, &#39;number_emergency&#39;,&#39;number_inpatient&#39;, &#39;number_diagnoses&#39;,&#39;readmitted_code&#39;] col_to_encode=[x for x in allv if x not in no_encodingL] print(col_to_encode) . [&#39;diag_g1&#39;, &#39;diag_g2&#39;, &#39;gender_code&#39;, &#39;race_code&#39;, &#39;age_code&#39;, &#39;disDispID_code&#39;, &#39;admSrsID_code&#39;, &#39;metformin_code&#39;, &#39;repaglinide_code&#39;, &#39;glimepiride_code&#39;, &#39;glipizide_code&#39;, &#39;glyburide_code&#39;, &#39;pioglitazone_code&#39;, &#39;rosiglitazone_code&#39;, &#39;insulin_code&#39;, &#39;A1C_code&#39;, &#39;max_glu_serum_code&#39;, &#39;change_code&#39;, &#39;diabMed_code&#39;] . df_encoded = pd.get_dummies(sd, columns=col_to_encode) df_encoded.head(11) . time_in_hospital num_lab_procedures num_procedures num_medications number_outpatient number_emergency number_inpatient number_diagnoses readmitted_code diag_g1_1 ... A1C_code_2 A1C_code_3 max_glu_serum_code_0 max_glu_serum_code_1 max_glu_serum_code_2 max_glu_serum_code_3 change_code_0 change_code_1 diabMed_code_0 diabMed_code_1 . 0 1 | 41 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | . 1 2 | 11 | 5 | 13 | 2 | 0 | 1 | 6 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 2 2 | 44 | 1 | 16 | 0 | 0 | 0 | 7 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 3 1 | 51 | 0 | 8 | 0 | 0 | 0 | 5 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 4 4 | 70 | 1 | 21 | 0 | 0 | 0 | 7 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 5 13 | 68 | 2 | 28 | 0 | 0 | 0 | 8 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 6 12 | 33 | 3 | 18 | 0 | 0 | 0 | 8 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 7 7 | 62 | 0 | 11 | 0 | 0 | 0 | 7 | 1 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 8 7 | 60 | 0 | 15 | 0 | 1 | 0 | 8 | 1 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 9 10 | 55 | 1 | 31 | 0 | 0 | 0 | 8 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 10 12 | 75 | 5 | 13 | 0 | 0 | 0 | 9 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | . 11 rows × 127 columns . df_encoded.shape . (66218, 127) . Split the data to train test . # Splitting into train and test # # first separate X, Y y=df_encoded[[&#39;readmitted_code&#39;]] X=df_encoded.drop(&#39;readmitted_code&#39;,axis=1) col2use=list(X) #split and count classes X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3 ) count_class_0, count_class_1 = y_train.readmitted_code.value_counts() print(count_class_0) print(count_class_1) . 38408 7944 . Sampling test portion of the data due to unbalanced dataset . Options available: TomekLinks : remove points that have different categories, but are nearest neghbours: it gives classification alhorithm most troubles . [] image As you can see in the above image, the Tomek Links (circled in green) are pairs of red and blue data points that are nearest neighbors. Intuitively, these are the points that will give most classification algorithms the most trouble. By removing these points, we widen the separation between the two classes, so our algorithms will be more “confident” in their outputs. . Sampling strategy &#39;auto&#39;: equivalent to &#39;not minority&#39; . #model = RandomUnderSampler(random_state=0) #from imblearn.under_sampling import CondensedNearestNeighbour -&gt; never finish #model = CondensedNearestNeighbour(random_state=0) #from imblearn.under_sampling import NearMiss #model= NearMiss() -&gt; 0.79 acuracy from imblearn.under_sampling import TomekLinks sampling_model=TomekLinks(sampling_strategy=&#39;auto&#39;) #from imblearn.under_sampling import OneSidedSelection #sampling_model = OneSidedSelection(random_state=0) X_sampled, y_sampled = sampling_model.fit_resample(X_train, y_train) . uniq, counts= np.unique(y_sampled, return_counts=True) dict(zip(uniq, counts)) # we see only 2016 removed !! # do we still need a beter sampling to improve learning? # . {0: 36388, 1: 7944} . Creating models and reports . from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report from sklearn.metrics import roc_curve, auc def print_report(name, y_actual, y_pred,y_test_pred_pb, thresh, labels=None, target_names=None): &#39;&#39;&#39; &#39;&#39;&#39; # because we have binary case tn, fp, fn, tp = confusion_matrix(y_actual, y_pred).ravel() specificity = tn / (tn+fp) auc_score = roc_auc_score(y_actual, y_pred) # accuracy = accuracy_score(y_actual, y_pred) recall = recall_score(y_actual, (y_pred &gt; thresh)) precision = precision_score(y_actual, (y_pred &gt; thresh)) f1= f1_score(y_actual, y_pred) # print(&#39;accuracy:%.3f&#39;%accuracy) # print(&#39;recall:%.3f&#39;%recall) print(&#39;F1:%.3f&#39;%f1) print(&#39;specificity:%.3f&#39;%specificity) print(&#39;AUC:%.3f&#39;%auc_score) conf_mat=confusion_matrix(y_actual, y_pred) print(&#39;Confusion matrix: n&#39;, conf_mat) with open(name + &quot;confusion_matrix.txt&quot;, &#39;w&#39;) as f: f.write(np.array2string(confusion_matrix(y_actual, y_pred), separator=&#39;, &#39;)) print(classification_report(y_actual, y_pred)) print(&#39; -&#39;) fpr_ar,tpr_ar, _ = roc_curve(y_actual, y_test_pred_pb) roc_auc = auc(fpr_ar,tpr_ar) plt.figure() plt.plot(fpr_ar, tpr_ar, color=&#39;red&#39;, lw=2, label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc) plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=2, linestyle=&#39;--&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate&#39;, fontsize=16) plt.ylabel(&#39;True Positive Rate&#39;, fontsize=16) plt.title(name.capitalize() +&#39; ROC Curve (area = %0.2f)&#39; % roc_auc,fontsize=18) plt.legend(loc=&quot;lower right&quot;) fileName=name + &quot;_roc_curve&quot; plt.savefig(fileName) plt.show() print(&#39;--&#39;) from sklearn.metrics import average_precision_score from sklearn.metrics import precision_recall_curve average_precision = average_precision_score(y_actual, y_pred) precision, recall, _ = precision_recall_curve(y_actual, y_pred) step_kwargs = ({&#39;step&#39;: &#39;post&#39;} ) plt.step(recall, precision, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;) plt.fill_between(recall, precision, alpha=0.2, color=&#39;b&#39;, **step_kwargs) plt.xlabel(&#39;Recall&#39;,fontsize=16) plt.ylabel(&#39;Precision&#39;,fontsize=16) plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title(&#39;Precision-Recall curve: AP={0:0.2f}&#39;.format( average_precision), fontsize=20) fileName=name + &quot;_prec-recall&quot; plt.savefig(fileName) plt.show() return dict(zip([&#39;auc&#39;, &#39;accuracy&#39;,&#39;f1&#39;, &#39;recall&#39;, &#39;precision&#39;, &#39;specificity&#39;,&#39;pred_pb&#39;],[auc_score, accuracy,f1, recall, precision, specificity,y_test_pred_pb])) models_results= {} . def eval_model(name, model, X_train,y_train, X_test,y_test, treshold=0.5): print (str(name) +&quot; &quot; + &#39;=&#39; * 20 ) model.fit( X_train,y_train ) y_pred=model.predict(X_test) y_train_pred_pb = model.predict_proba(X_sampled)[:,1] y_test_pred_pb = model.predict_proba(X_test)[:,1] model_info = print_report(name, y_test,y_pred,y_test_pred_pb, thresh=treshold) return model_info . from sklearn.linear_model import LogisticRegression # different kernels !!! lr = LogisticRegression(solver=&#39;saga&#39;, penalty=&#39;l1&#39;) # solver= &#39;liblinear&#39;, &#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;sag&#39;, &#39;saga models_results[&#39;logistic&#39;]=eval_model(name=&#39;Logistic&#39;, model=lr, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) #f_imps = pd.DataFrame(lr_l1.feature_importances_,index = X_train.columns,columns=[&#39;importance&#39;]).sort_values(&#39;importance&#39;, ascending=False) #f_imps.head(10) . Logistic ==================== F1:0.205 specificity:0.982 AUC:0.553 Confusion matrix: [[16159 294] [ 2989 424]] precision recall f1-score support 0 0.84 0.98 0.91 16453 1 0.59 0.12 0.21 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.72 0.55 0.56 19866 weighted avg 0.80 0.83 0.79 19866 - . -- . from sklearn.neighbors import KNeighborsClassifier knn=KNeighborsClassifier(n_neighbors = 100) models_results[&#39;knn&#39;]=eval_model(name=&#39;KNN&#39;, model=knn, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) . KNN ==================== F1:0.047 specificity:0.999 AUC:0.511 Confusion matrix: [[16432 21] [ 3331 82]] precision recall f1-score support 0 0.83 1.00 0.91 16453 1 0.80 0.02 0.05 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.81 0.51 0.48 19866 weighted avg 0.83 0.83 0.76 19866 - . -- . from sklearn import ensemble rfc = ensemble.RandomForestClassifier() models_results[&#39;rfc&#39;]=eval_model(name=&#39;RFC&#39;, model=rfc, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) . RFC ==================== specificity:0.977 AUC:0.544 Confusion matrix: [[16071 382] [ 3032 381]] precision recall f1-score support 0 0.84 0.98 0.90 16453 1 0.50 0.11 0.18 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.67 0.54 0.54 19866 weighted avg 0.78 0.83 0.78 19866 - . -- . from sklearn.linear_model import SGDClassifier sgdc=SGDClassifier(loss = &#39;log&#39;,alpha = 0.1,random_state = 33) models_results[&#39;sgdc&#39;]=eval_model(name=&#39;SGDC&#39;, model=sgdc, X_train= X_sampled, y_train=y_sampled, X_test=X_test,y_test=y_test, treshold=0.5) . SGDC ==================== specificity:0.994 AUC:0.527 Confusion matrix: [[16359 94] [ 3211 202]] precision recall f1-score support 0 0.84 0.99 0.91 16453 1 0.68 0.06 0.11 3413 micro avg 0.83 0.83 0.83 19866 macro avg 0.76 0.53 0.51 19866 weighted avg 0.81 0.83 0.77 19866 - . -- . #dtc = DecisionTreeClassifier(max_depth = 10, random_state = 42) . # . for model_name, result in models_results.items(): print(model_name, result) for stat,value in result.items(): print(stat,value) . logistic {&#39;auc&#39;: 0.55318090014780585, &#39;accuracy&#39;: 0.8347427766032417, &#39;f1&#39;: 0.20527717259743405, &#39;recall&#39;: array([ 1. , 0.12423088, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.59052925, 1. ]), &#39;specificity&#39;: 0.98213091837354893, &#39;pred_pb&#39;: array([ 0.23068577, 0.16358035, 0.08422994, ..., 0.26934011, 0.08899061, 0.24752306])} auc 0.553180900148 accuracy 0.834742776603 f1 0.205277172597 recall [ 1. 0.12423088 0. ] precision [ 0.17180107 0.59052925 1. ] specificity 0.982130918374 pred_pb [ 0.23068577 0.16358035 0.08422994 ..., 0.26934011 0.08899061 0.24752306] knn {&#39;auc&#39;: 0.51137471039731408, &#39;accuracy&#39;: 0.83126950568811031, &#39;f1&#39;: 0.046643913538111488, &#39;recall&#39;: array([ 1. , 0.02402578, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.7961165 , 1. ]), &#39;specificity&#39;: 0.9987236370266821, &#39;pred_pb&#39;: array([ 0.25, 0.18, 0.16, ..., 0.14, 0.08, 0.19])} auc 0.511374710397 accuracy 0.831269505688 f1 0.0466439135381 recall [ 1. 0.02402578 0. ] precision [ 0.17180107 0.7961165 1. ] specificity 0.998723637027 pred_pb [ 0.25 0.18 0.16 ..., 0.14 0.08 0.19] rfc {&#39;auc&#39;: 0.54420717251774842, &#39;accuracy&#39;: 0.82814859559045606, &#39;f1&#39;: 0.18247126436781611, &#39;recall&#39;: array([ 1. , 0.111632, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.49934469, 1. ]), &#39;specificity&#39;: 0.97678234972345468, &#39;pred_pb&#39;: array([ 0.1, 0. , 0.3, ..., 0.5, 0. , 0.3])} auc 0.544207172518 accuracy 0.82814859559 f1 0.182471264368 recall [ 1. 0.111632 0. ] precision [ 0.17180107 0.49934469 1. ] specificity 0.976782349723 pred_pb [ 0.1 0. 0.3 ..., 0.5 0. 0.3] sgdc {&#39;auc&#39;: 0.52673611177273305, &#39;accuracy&#39;: 0.83363535689117085, &#39;f1&#39;: 0.10892423833917499, &#39;recall&#39;: array([ 1. , 0.05918547, 0. ]), &#39;precision&#39;: array([ 0.17180107, 0.68243243, 1. ]), &#39;specificity&#39;: 0.99428675621467211, &#39;pred_pb&#39;: array([ 0.18043704, 0.15336435, 0.13491804, ..., 0.21587527, 0.1406695 , 0.20489415])} auc 0.526736111773 accuracy 0.833635356891 f1 0.108924238339 recall [ 1. 0.05918547 0. ] precision [ 0.17180107 0.68243243 1. ] specificity 0.994286756215 pred_pb [ 0.18043704 0.15336435 0.13491804 ..., 0.21587527 0.1406695 0.20489415] . dfz=pd.DataFrame.from_dict(models_results,orient=&#39;index&#39;) dfz.head(4) . auc accuracy f1 recall precision specificity pred_pb . knn 0.511375 | 0.831270 | 0.046644 | [1.0, 0.0240257837679, 0.0] | [0.17180106715, 0.796116504854, 1.0] | 0.998724 | [0.25, 0.18, 0.16, 0.07, 0.25, 0.11, 0.07, 0.1... | . logistic 0.553181 | 0.834743 | 0.205277 | [1.0, 0.124230881922, 0.0] | [0.17180106715, 0.590529247911, 1.0] | 0.982131 | [0.230685768678, 0.163580352538, 0.08422994190... | . rfc 0.544207 | 0.828149 | 0.182471 | [1.0, 0.111631995312, 0.0] | [0.17180106715, 0.499344692005, 1.0] | 0.976782 | [0.1, 0.0, 0.3, 0.3, 0.6, 0.0, 0.0, 0.3, 0.1, ... | . sgdc 0.526736 | 0.833635 | 0.108924 | [1.0, 0.0591854673308, 0.0] | [0.17180106715, 0.682432432432, 1.0] | 0.994287 | [0.180437036084, 0.153364345572, 0.13491803579... | . import sklearn.metrics as metrics # calculate the fpr and tpr for all thresholds of the classification probs = model.predict_proba(X_test) preds = probs[:,1] fpr, tpr, threshold = metrics.roc_curve(y_test, preds) roc_auc = metrics.auc(fpr, tpr) # method I: plt import matplotlib.pyplot as plt plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() . model = LogisticRegression(solver=&#39;liblinear&#39;, penalty=&#39;l1&#39;) # solver=&#39;lbfgs&#39; model.fit( X_train,y_train ) y_pred=model.predict(X_test) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l1&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001, verbose=0, warm_start=False) . feature_importances = pd.DataFrame(model.coef_[0], index = col2use, columns=[&#39;importance&#39;]).sort_values(&#39;importance&#39;, ascending=False) . Final results . feature_importances.head(15) . importance . disDispID_code_22 0.864318 | . admSrsID_code_20 0.632377 | . diag_g1_27 0.599416 | . disDispID_code_5 0.504256 | . number_inpatient 0.442862 | . diag_g2_2 0.287582 | . diag_g1_16 0.273329 | . diag_g1_4 0.216486 | . number_emergency 0.206680 | . diag_g1_5 0.183197 | . diag_g1_3 0.176214 | . diag_g2_12 0.176034 | . admSrsID_code_3 0.163309 | . admSrsID_code_9 0.158770 | . max_glu_serum_code_3 0.154818 | .",
            "url": "https://v2br.github.io/fastpages/jupyter/supervised%20learning/2021/05/03/Diabetic-readmisson-analysis.html",
            "relUrl": "/jupyter/supervised%20learning/2021/05/03/Diabetic-readmisson-analysis.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "DeepDream and style transfer fun",
            "content": "-Task description , . Data for the task | standard approach | my approach: std -top-lvel +filter | compare | ? | .",
            "url": "https://v2br.github.io/fastpages/style-transfer/2021/05/03/DeepDream-music-to-picture.html",
            "relUrl": "/style-transfer/2021/05/03/DeepDream-music-to-picture.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Comprehesive data Exploration",
            "content": ".",
            "url": "https://v2br.github.io/fastpages/pandas/data-analysis/2021/05/03/ComprehensiveDataExploration.html",
            "relUrl": "/pandas/data-analysis/2021/05/03/ComprehensiveDataExploration.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "ML projects organizing and tracking",
            "content": "Approach to organizing ml projects and tracking experiments used by Neptune and W&amp;B (wanbi) . W&amp;B (wanbi) source . https://wandb.ai/ . Neptune source . https://neptune.ai/blog/how-to-use-google-colab-in-machine-learning-deep-learning-projects .",
            "url": "https://v2br.github.io/fastpages/wanbi/neptune/tracking-experiments/2020/12/05/Experiment-tracking.html",
            "relUrl": "/wanbi/neptune/tracking-experiments/2020/12/05/Experiment-tracking.html",
            "date": " • Dec 5, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Visualization using seaborn",
            "content": "Numeric not ordered . Examples: . number of observations (different kind of) | number of voters per state | phone codes per state | . Sns high level overview https://seaborn.pydata.org/tutorial/function_overview.html . link to dataviz schema - . Relational data . Distribution . Show comparion of Violin box and hist . import datetime, warnings, scipy import pandas as pd # import pandas_profiling # otional import numpy as np import seaborn as sns import matplotlib.pyplot as plt import warnings; warnings.simplefilter(&#39;ignore&#39;) def plot_comparison(x, title): fig, ax = plt.subplots(4, 1, sharex=True, figsize=(12,8)) sns.distplot(x, ax=ax[0]) # histogram ax[0].set_title(&#39;Histogram + KDE; x value, y deach time different ; y count&#39;) sns.boxplot(x, ax=ax[1]) # Boxplot ax[1].set_title(&#39;Boxplot&#39;) sns.violinplot(x, ax=ax[2]) # Violin ax[2].set_title(&#39;Violin plot&#39;) fig.suptitle(title, fontsize=16) sns.ecdfplot(data=x,ax=ax[3], stat=&#39;proportion&#39;) ax[3].set_title(&#39;ECDF plot&#39;) fig.suptitle(title, fontsize=16) plt.show() . N = 10 ** 4 np.random.seed(42) sample_gaussian = np.random.normal(size=N) plot_comparison(sample_gaussian, &#39;Standard Normal Distribution&#39;) . sample_bimodal = np.concatenate([np.random.normal(loc=-2, scale=2, size=int(N/2)), np.random.normal(loc=3, scale=1, size=int(N/2))]) plot_comparison(sample_bimodal, &#39;Mixture of Gaussians - bimodal&#39;) . sns.displot(data=sample_bimodal, kind=&#39;hist&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f26f60862e0&gt; . . penguins= sns.load_dataset(&#39;penguins&#39;) . f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 3])) sns.scatterplot(data=penguins, x=&quot;flipper_length_mm&quot;, y=&quot;bill_length_mm&quot;, hue=&quot;species&quot;, ax=axs[0]) sns.histplot(data=penguins, x=&quot;species&quot;, hue=&quot;species&quot;, shrink=.8, alpha=.8, legend=False, ax=axs[1]) f.tight_layout() . import seaborn as sns import matplotlib.pyplot as plt df = sns.load_dataset(&#39;iris&#39;) # plot of 2 variables p1=sns.kdeplot(df[&#39;sepal_width&#39;], shade=True, color=&quot;r&quot;) p1=sns.kdeplot(df[&#39;sepal_length&#39;], shade=True, color=&quot;b&quot;) plt.show() . Numeric Ordered . Line . df = pd.DataFrame(dict(time=np.arange(500), value=np.random.randn(500).cumsum())) g = sns.relplot(x=&quot;time&quot;, y=&quot;value&quot;, kind=&quot;line&quot;, data=df) g.fig.autofmt_xdate() . fmri = sns.load_dataset(&quot;fmri&quot;) fmri.head(3) . subject timepoint event region signal . 0 s13 | 18 | stim | parietal | -0.017552 | . 1 s5 | 14 | stim | parietal | -0.080883 | . 2 s12 | 18 | stim | parietal | -0.081033 | . sns.relplot(x=&quot;timepoint&quot;, y=&quot;signal&quot;, kind=&quot;line&quot;, data=fmri); . plt.plot([1, 2, 3, 4], [1, 4, 9, 16], &#39;ro&#39;) plt.axis([0, 6, 0, 20]) plt.show() . sns.relplot([1, 2, 3, 4], [1, 4, 9, 16]) plt.axis([0, 6, 0, 20]) plt.show() . Scatter . sns.relplot(x=&quot;timepoint&quot;, y=&quot;signal&quot;, kind=&quot;scatter&quot;, data=fmri); . Area . import numpy as np import matplotlib.pyplot as plt import seaborn as sns # Data x=range(1,6) y=[ [1,4,6,8,9], [2,2,7,10,12], [2,8,5,10,6] ] # Plot Area plt.stackplot(x,y, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]) plt.legend(loc=&#39;upper left&#39;) plt.show() . tips = sns.load_dataset(&quot;tips&quot;) sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips); . Categorical . Here we will show several categories and values for them . Violin and Boxplot for category (x) . Together with simple histogram good first impression . import matplotlib.pyplot as plt import numpy as np # Fixing random state for reproducibility np.random.seed(19680801) # generate some random test data all_data = [np.random.normal(0, std, 100) for std in range(6, 10)] for std in range(6, 10): print(std) ddf=pd.DataFrame.from_dict(all_data) ddf.head(5) . 6 7 8 9 . 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99 . 0 6.295826 | 5.196595 | 3.306808 | 10.368467 | -3.895729 | -2.870517 | 6.467311 | 5.812862 | -4.856069 | -7.424335 | ... | -3.583005 | 0.093715 | -3.643103 | 4.328405 | 2.785666 | 4.959357 | 0.558072 | -13.573699 | 5.944342 | -7.642895 | . 1 -9.714159 | 1.086545 | 0.515707 | 7.204036 | -4.622671 | -4.332094 | -1.326759 | -3.822245 | 11.380635 | -0.717820 | ... | -7.921901 | -4.889639 | -8.983292 | -7.240465 | 6.011052 | -7.900238 | 2.975677 | 7.112989 | -21.970376 | -3.444214 | . 2 12.881220 | 3.791821 | -18.492249 | -3.625449 | -6.653355 | -2.105036 | -2.026996 | 0.837128 | -2.707500 | -9.184221 | ... | 0.257406 | -2.018110 | -3.441745 | 4.455858 | 4.824423 | -3.347953 | 13.122325 | 6.851431 | 4.041598 | 0.668767 | . 3 2.506632 | -0.630992 | -0.326357 | -20.307119 | -13.114656 | -3.950933 | -0.192611 | -8.226074 | 9.422215 | 2.671719 | ... | -1.398185 | 6.866293 | -15.636616 | -9.132841 | 2.753904 | 6.927100 | 17.219411 | 0.230163 | -0.870910 | -7.030377 | . 4 rows × 100 columns . fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4)) # plot violin plot axes[0].violinplot(ddf, showmeans=False, showmedians=True) axes[0].set_title(&#39;Violin plot&#39;) # plot box plot axes[1].boxplot(ddf) axes[1].set_title(&#39;Box plot&#39;) # adding horizontal grid lines for ax in axes: ax.yaxis.grid(True) ax.set_xticks([y + 1 for y in range(len(all_data))]) ax.set_xlabel(&#39;Four separate samples&#39;) ax.set_ylabel(&#39;Observed values&#39;) # add x-tick labels plt.setp(axes, xticks=[y + 1 for y in range(len(all_data))], xticklabels=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]) plt.show() . ax = sns.violinplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;sex&quot;, data=tips) ax.set_title(&#39;Distribution of total bill amount per day&#39;, fontsize=16); . ax = sns.violinplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;sex&quot;, split=True, data=tips) ax.set_title(&#39;Distribution of total bill amount per day&#39;, fontsize=16); . ECDF . other view of distribution . plt.plot([1, 2, 3, 4], [1, 4, 9, 16], &#39;ro&#39;) plt.axis([0, 6, 0, 20]) plt.show() .",
            "url": "https://v2br.github.io/fastpages/visuals/jupyter/2020/11/01/sns_visuals.html",
            "relUrl": "/visuals/jupyter/2020/11/01/sns_visuals.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Default initial configuration for a project",
            "content": "Example of a default project setup . Goal . Environment . test before start . .",
            "url": "https://v2br.github.io/fastpages/shell/project/2020/08/20/Project-template.html",
            "relUrl": "/shell/project/2020/08/20/Project-template.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Using Docker for development on wsl2",
            "content": "Notes on docker use for development on WSl2 system. . I decided to use docker for a jekyll project. The reason is with all upgrades and gems possible conflicts management of the environment becomes problematic. Moving project to another environment might be not that straight forward. . Containerization approach. With docker container the whole project becomes easy manageable and reproducible. Plus if you have any conflicts during installation it is much easier to analyze the problem and resolve the conflict. . My requirements to the solution: . Editing project files should not change the setup. | Version control should not be affected. | The whole setup should be reproducible and automated as much as possible. | Jekyll should generate site and run it on local system port 4000. | The first two reqrements cold be satisfied using docker option “volume”. It allows to map local host directories to virtual directories. Docker volume is aka rsync in Linux. The third one is the key feature of the docker itself. The last could be done using docker parameter “port”. It will map local host port to the application port. . One additional configuration step is necessary due to default docker user. It is root. This is not optimal. According to best practices application will run on application user account only with necessary privileges. . To accomplish this portable way we can place whole configuration into docker file . Dcokerfile . FROM jekyll/jekyll RUN addgroup jekyll &amp;&amp; adduser --ingroup jekyll --disabled-password --gecos &#39;&#39; jekyll &amp;&amp; chown jekyll:jekyll /home/jekyll USER jekyll RUN mkdir -p /home/jekyll/site &amp;&amp; chown -R jekyll:jekyll /home/jekyll/site VOLUME /home/jekyll/site WORKDIR /home/jekyll/site EXPOSE 4000 RUN apk add py-pip RUN pip CMD bundle install &amp;&amp; jekyll serve --host=0.0.0.0 --force_polling --watch . Let’s look into the details: . Use the latest jekyll image from dockerhub | Configure creation of docker container with the following arguments volumes | . | In my case the critical part happened to be gem installation . Jekyll plugins install . docker build -t getting-started . docker run -dp 3000:3000 getting-started #-- Gems file source &#39;https://rubygems.org&#39; group :jekyll_plugins do gem &#39;jekyll&#39;, &#39;~&gt;3.0&#39; gem &#39;kramdown&#39; gem &#39;rdiscount&#39; gem &#39;jekyll-sitemap&#39; gem &#39;jekyll-redirect-from&#39; end . Dockeriser Python install . docker pull continuumio/conda-ci-linux-64-python3.8 . Sources . Conda pip and docker . | Jekyll in a Docker Container . | Installing the docker client on linux =windows-subsystem (WSL2) . | .",
            "url": "https://v2br.github.io/fastpages/docker/jekyll/2020/08/11/docker.html",
            "relUrl": "/docker/jekyll/2020/08/11/docker.html",
            "date": " • Aug 11, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Notes on installing and configuring miniconda on WSL2",
            "content": "Notes on installing and configuring miniconda on WSL2 . Make sure that you have atleast 2GB of free space for the this project . Install miniconda . As a first step we need uptodate clean system . sudo apt-get update sudo apt-get upgrade sudo apt autoremove . 1 Download the latest miniconda from https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html . 2 You need to be on user, not a superuser account (no sudo) . bash Miniconda3-latest-Linux-x86_64.sh . Conda may have some bug fixes after the release, so let’s update it. . conda update conda . Configure environment &amp; install packages . Next step is to create your environmet and download requred pckages. You have a choice of named environment in default location ($HOME/miniconda3/envs) or location environment. Named environment is more convenient and easy on integrations with 3 party tools. To check current conda configuration . conda info . With custom location there is no environment name! It can be located inside your project directory. As result you may keep all project files in one place. . Create named environmet . conda create --name my-conda-env . System will tell you where new environment will be created. Once created you need to activate it and change env variables . conda activate my-conda-env source ~/.bashrc conda env list . You should see star against your new env and to list all info use “conda info” . Create location based environment . conda create --prefix $HOME/projects/fun-project/ conda activate $HOME/projects/fun-project/ source ~/.bashrc conda env list conda info . in this case conda will create directory fun-project/env . To save named envitonment in custom location you need to set it explicitly . conda config --append envs_dirs &quot;$HOME&quot;/projects/my-fun-project/conda-env conda create --name my-conda-env conda activate my-conda-env source ~/.bashrc conda env list . Conda will create environment in given directory. The only problem is that now you have define environment location directory every for every new project. Otherwise it will be created as child of the last used “envs_dir” parameter. . Create new env from project requrements . conda create --name my-conda-env -f &quot;full path to env_requrements.yml&quot; . Example of file name: env_requrements channels: . conda-forge | some_fancy_channel dependencies: python=3.8 | numpy=1.2 | matplotlib | pandas=0.23 -pip: | . | some_pip_package==0.1 | . If you want to run bashrc automatically after login Open your .profile and add the last line source ~/.profile . And finally: . Install Jupyter . conda install -c conda-forge nodejs=14.14 conda install -c conda-forge jupyter_contrib_nbextensions jupyter contrib nbextension install --user . Set jupyter to use new environment and run Jupyter . ipython kernel install --user --name conda install -n fun-project ipykernel conda update --all #-- RUN!! jupyter lab --no-browser . And copy and paste the full URL including the token! You up and running!!! . #———————————————— . Delete kernel and environment . Delete just kernel . Use GUI check on left side kernels, or top menu Kernels in Ijupyter there is a button shutdown . Releasing memory on wsl2 . Somehow even you shutdown jupyter wsl2 does not release memory Shutdown the whole w You can open windows shell and . wsl -shutdown . List jupyter data directory . jupyter --data-dir . Find subfolder kernel and delete it. . Remove conda env AND all packages assosiated with it . ’’’ conda remove –name prj1 –all conda info ‘’’ if your custom directory still shown in env directories open $HOME/miniconda/env and delete dir . Optional . if you want a short name for your prompt . conda config --set env_prompt &quot;(fun)&quot; . conda install -n metagenomics_env –override-channels -c conda-forge -c bioconda -c defaults kraken It is recommended to install all required packages at once to avoid dependecy conflicts. . Poetry . curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - . alias jupyter-notebook=”~/.local/bin/jupyter-notebook –no-browser” . Save environment . conda env export –file environment.yml #————————— . Initialize new project . Steps: . create a new directory and initialize a git repository | create an environment.yml file | create a conda environment given the environment specification | . | . Update packages . conda update -n conda-env pandas conda update =n conda-env --channel conda-forge opencv . Clean after install . conda clean --all . #————————————————————- . Optional: graphical interface for ubuntu (xfce) . Use Remote Desktop Connection with your :3388 #- . Optional Visual Studio setup . Resources: . | Microsoft WSL2 | WSL2 docs | Jupyter &amp; Visual Studio | | . #——————————————— . https://pybit.es/guest-anaconda-workflow.html .",
            "url": "https://v2br.github.io/fastpages/conda/python/2020/08/02/conda-install.html",
            "relUrl": "/conda/python/2020/08/02/conda-install.html",
            "date": " • Aug 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://v2br.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Extra",
          "content": "? notes or something .",
          "url": "https://v2br.github.io/fastpages/extra/",
          "relUrl": "/extra/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://v2br.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}